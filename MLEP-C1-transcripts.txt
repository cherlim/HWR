MLEP C1 transcripts
WEBVTT


00:00:00.240 --> 00:00:04.650
Hi and welcome to machine learning
engineering for production.


00:00:04.650 --> 00:00:07.268
A lot of learners have asked, Hey Andrew,


00:00:07.268 --> 00:00:11.940
I've learned to train a machine
learning model now, what do I do?


00:00:11.940 --> 00:00:14.017
Machine learning models are great,


00:00:14.017 --> 00:00:16.966
but unless you know how to
put them into production,


00:00:16.966 --> 00:00:20.933
it's hard to get them to create
the maximum amount of possible value.


00:00:20.933 --> 00:00:22.905
Or for
those of you that may be looking for


00:00:22.905 --> 00:00:25.650
a position in machine learning
many interview as well.


00:00:25.650 --> 00:00:29.643
Have you ever deployed a machine
learning algorithm production.


00:00:29.643 --> 00:00:33.636
In this four course specialization,
the first course taught by me, the 2nd,


00:00:33.636 --> 00:00:35.611
3rd and 4th causes taught by Robert.


00:00:35.611 --> 00:00:37.940
Crow is an expert at this from google.


00:00:37.940 --> 00:00:41.991
We hope to share with you to practical
hands on skills and techniques.


00:00:41.991 --> 00:00:45.430
You need to not just build
a machine learning model, but


00:00:45.430 --> 00:00:47.740
also to put them into production.


00:00:47.740 --> 00:00:51.915
And so by the end of this first course and
by the end of this specialization,


00:00:51.915 --> 00:00:56.450
I hope you have a good sense of the entire
life cycle of machine learning project.


00:00:56.450 --> 00:00:59.051
From training model to
put into production and


00:00:59.051 --> 00:01:02.400
really how to manage the entire
machine learning project.


00:01:02.400 --> 00:01:03.840
Let's jump in.


00:01:03.840 --> 00:01:08.834
Let's start with an example, let's say
you're using computer vision to inspect


00:01:08.834 --> 00:01:13.350
phones coming off a manufacturing line
to see if there are defects on them.


00:01:13.350 --> 00:01:16.941
So this phone shown on the left
doesn't have any stretches on it.


00:01:16.941 --> 00:01:21.869
But if there was a stretch of crack or
something, a computer vision


00:01:21.869 --> 00:01:27.503
algorithm would hopefully be able to
find this type of stretch, or defect.


00:01:27.503 --> 00:01:33.340
And maybe put the bounding box around
it as part of quality control.


00:01:33.340 --> 00:01:37.519
If you get a data set of scratched
phones you can train a computer vision


00:01:37.519 --> 00:01:41.650
algorithm maybe in your network
to detect these types of defects.


00:01:41.650 --> 00:01:47.240
But what do you now need to do in order
to put this into production deployment?


00:01:47.240 --> 00:01:52.100
This would be an example of how you
could deploy a system like this.


00:01:52.100 --> 00:01:54.524
You might have an edge device.


00:01:54.524 --> 00:01:59.111
By edge device, I mean a device that is
living inside the factory that is


00:01:59.111 --> 00:02:01.620
manufacturing these smartphones.


00:02:01.620 --> 00:02:06.084
And that edge device would have a piece
of inspection software whose job it


00:02:06.084 --> 00:02:09.828
is to take a picture of the phone,
see if there's a stretch and


00:02:09.828 --> 00:02:13.880
then make a decision on whether
this phone is acceptable, or not.


00:02:13.880 --> 00:02:18.520
This is actually commonly done in
factories is called automated visual


00:02:18.520 --> 00:02:19.930
defect inspection.


00:02:19.930 --> 00:02:24.101
What the inspection software does is
it will control camera that will take


00:02:24.101 --> 00:02:27.948
a picture of the smartphone as it
rolls off the manufacturing line.


00:02:27.948 --> 00:02:35.480
And it then has to make an API call to
pass this picture to a prediction server.


00:02:35.480 --> 00:02:40.224
And the job of the prediction server
is to accept these API calls,


00:02:40.224 --> 00:02:43.845
receive an image,
make a decision as to whether or


00:02:43.845 --> 00:02:48.005
not this phone is defective and
return this prediction.


00:02:48.005 --> 00:02:51.714
And then the inspection software it can
make the appropriate control decision


00:02:51.714 --> 00:02:54.479
whether to let it still move
on in the manufacturing line.


00:02:54.479 --> 00:02:59.740
Or whether to shove it to a side, because
it was defective and not acceptable.


00:02:59.740 --> 00:03:03.366
So after you have trained
a learning algorithm,


00:03:03.366 --> 00:03:08.237
maybe train the neural network to take
as input X pictures of phones.


00:03:08.237 --> 00:03:13.910
And map down to y predictions about
whether the phone is defective or not.


00:03:13.910 --> 00:03:19.040
You still have to take this
machine learning model.


00:03:19.040 --> 00:03:22.054
Put it in a production server,
setup API interfaces and


00:03:22.054 --> 00:03:24.501
really write all of
the rest of the software.


00:03:24.501 --> 00:03:29.940
In order to deploy this learning
algorithm into production.


00:03:29.940 --> 00:03:33.615
This prediction server is
sometimes in the cloud and


00:03:33.615 --> 00:03:38.230
sometimes the prediction server
is actually at the edge as well.


00:03:38.230 --> 00:03:42.867
In fact in manufacturing we use edge
deployments a lot, because you can't have


00:03:42.867 --> 00:03:46.680
your factory go down every time
your internet access goes down.


00:03:46.680 --> 00:03:51.192
But cloud deployments with prediction
server, is a server in the cloud,


00:03:51.192 --> 00:03:53.840
is also used for many applications.


00:03:53.840 --> 00:03:56.000
Let's say you write all the software.


00:03:56.000 --> 00:03:58.640
What could possibly go wrong?


00:03:58.640 --> 00:04:03.093
It turns out that just because
you've trained a learning algorithm


00:04:03.093 --> 00:04:07.011
that does well on your test set,
which is to be celebrated.


00:04:07.011 --> 00:04:10.140
It's great when you do well
when you hold a test set.


00:04:10.140 --> 00:04:14.120
Unfortunately reaching that
milestone doesn't mean you're done.


00:04:14.120 --> 00:04:17.103
There can still be quite a lot of work and


00:04:17.103 --> 00:04:23.240
challenges ahead to get a valuable
production deployment running.


00:04:23.240 --> 00:04:29.140
For example, let's say your training
sets has images that look like this.


00:04:29.140 --> 00:04:32.746
There's a good phone on the left,
the one in the middle,


00:04:32.746 --> 00:04:37.555
it has a big scratch across it and
you've trained your learning algorithm to


00:04:37.555 --> 00:04:40.955
recognize that things like
this on the left are okay.


00:04:40.955 --> 00:04:46.721
Meaning that no defects and maybe draw
bounding boxes around scratches or


00:04:46.721 --> 00:04:49.604
other defects that finds and films.


00:04:49.604 --> 00:04:53.871
When you deploy it in the factory,
you may find that the real life


00:04:53.871 --> 00:04:58.786
production deployment gives you back
images like this much darker ones.


00:04:58.786 --> 00:05:02.875
Because the lighting factory, because the
lighting conditions in the factory have


00:05:02.875 --> 00:05:07.340
changed for some reason compared to the
time when the training set was collected.


00:05:07.340 --> 00:05:11.480
This problem is sometimes called
concept drift or data drift.


00:05:11.480 --> 00:05:15.477
You learn more about these
terms later in this week.


00:05:15.477 --> 00:05:20.155
But this is one example of the many
practical problems that we,


00:05:20.155 --> 00:05:25.009
as machine learning engineers should
step up to solve if we want to


00:05:25.009 --> 00:05:29.534
make sure that we don't just do
well on the holdout test set.


00:05:29.534 --> 00:05:34.226
But that our systems actually
create value in a practical


00:05:34.226 --> 00:05:37.840
production deployment environment.


00:05:37.840 --> 00:05:42.370
I've worked on quite a few projects
where my machine learning team and


00:05:42.370 --> 00:05:45.304
I would successfully
new a proof of concept.


00:05:45.304 --> 00:05:48.661
And by that I mean we train
a model in Jupiter notebook and


00:05:48.661 --> 00:05:51.456
it will work great and
we will celebrate that.


00:05:51.456 --> 00:05:54.147
You should celebrate it
when you have a learning,


00:05:54.147 --> 00:05:58.340
algorithm worked well in Jupiter
notebook in a development environment.


00:05:58.340 --> 00:06:03.424
But it turns out that sometimes I'll
see many projects where that success,


00:06:03.424 --> 00:06:07.078
which is a great success to
the practical deployment is


00:06:07.078 --> 00:06:09.791
still maybe another six months of work.


00:06:09.791 --> 00:06:15.623
And this is just one of many of the
practical things that a machine learning


00:06:15.623 --> 00:06:21.860
team has to watch out for and handle in
order to actually deploy these systems.


00:06:21.860 --> 00:06:25.530
Some machine learning engineers will say
is not a machine learning problem to


00:06:25.530 --> 00:06:26.790
address these problems.


00:06:26.790 --> 00:06:28.400
The dataset changes.


00:06:28.400 --> 00:06:32.440
Some machine engineers think well,
is that the machine learning problem?


00:06:32.440 --> 00:06:37.040
My point of view is that our job
is to make these things work.


00:06:37.040 --> 00:06:41.723
And so if the data set has changed is
I think of it as my responsibility


00:06:41.723 --> 00:06:45.110
when I work on a project to step in and
do what I can to


00:06:45.110 --> 00:06:49.440
access the data distribution as it
is rather than as I wish it is.


00:06:49.440 --> 00:06:53.436
So this specialization will teach you
about a lot of these important practical


00:06:53.436 --> 00:06:57.310
things for building machine learning
systems that work not just in the lab,


00:06:57.310 --> 00:07:02.040
not just in the Jupiter notebook, but
in a production deployment environment.


00:07:02.040 --> 00:07:05.721
A second challenge of deploying
machine learning models and


00:07:05.721 --> 00:07:09.780
production is that it takes a lot
more than machine learning code.


00:07:09.780 --> 00:07:14.671
Over the last decade there's been a lot
of attention on machine learning models.


00:07:14.671 --> 00:07:18.810
So your neural network or
other algorithm that learns


00:07:18.810 --> 00:07:22.970
a function mapping from
some input to some output.


00:07:22.970 --> 00:07:28.240
And that's been amazing progress
in machine learning models.


00:07:28.240 --> 00:07:32.991
But it turns out that if you look at
a machine learning system in production,


00:07:32.991 --> 00:07:37.451
if this little orange rectangle
represents the machine learning code,


00:07:37.451 --> 00:07:39.587
the machine learning model code.


00:07:39.587 --> 00:07:45.040
Then this is all the codes you need for
the entire machine learning project.


00:07:45.040 --> 00:07:48.668
I feel like for
many machine learning projects,


00:07:48.668 --> 00:07:52.670
maybe only 5-10%,
maybe even less of the code.


00:07:52.670 --> 00:07:54.130
Machine learning code.


00:07:54.130 --> 00:07:58.139
And I think this is one of
the reasons why when you have


00:07:58.139 --> 00:08:02.619
a proof of concept model
working maybe Jupiter notebook.


00:08:02.619 --> 00:08:07.011
It can still be a lot of work
to go from that initial proof of


00:08:07.011 --> 00:08:10.140
concept to the production deployment.


00:08:10.140 --> 00:08:13.430
So sometimes people refer to the POC.


00:08:13.430 --> 00:08:17.240
Or the proof of concept to production gap.


00:08:17.240 --> 00:08:23.722
And a lot of that gap is sometimes just
the sheer amount of work it is to also


00:08:23.722 --> 00:08:30.880
write all of this code out here beyond
the initial machine learning model code.


00:08:30.880 --> 00:08:33.970
So what is all this other stuff?


00:08:33.970 --> 00:08:40.867
This is a diagram that have adapted
from a paper by D Scully and others.


00:08:40.867 --> 00:08:45.541
Beyond the machine learning codes
there are also many components,


00:08:45.541 --> 00:08:48.821
especially components for
managing the data,


00:08:48.821 --> 00:08:53.663
such as data collection,
data verification, feature extraction.


00:08:53.663 --> 00:08:55.566
And after you are serving it,


00:08:55.566 --> 00:09:00.840
how to monitor the system will monitor
the data comes back, help you analyze it.


00:09:00.840 --> 00:09:05.752
But there are often many other components
that need to be built to enable a working


00:09:05.752 --> 00:09:07.360
production deployment.


00:09:07.360 --> 00:09:11.759
So in this course you learn what
are all of these other pieces of


00:09:11.759 --> 00:09:15.840
software needed for
a valuable production deployment.


00:09:15.840 --> 00:09:20.016
But rather than looking at all of these
complex piece is one of the most useful


00:09:20.016 --> 00:09:22.181
frameworks are found for organizing.


00:09:22.181 --> 00:09:26.641
The workflow of a machine learning project
is to systematically plan out the life


00:09:26.641 --> 00:09:29.140
cycle of a machine learning project.


00:09:29.140 --> 00:09:33.765
Let's go to the next video to dive into
what is the full life cycle of a machine


00:09:33.765 --> 00:09:35.020
learning project.


00:09:35.020 --> 00:09:37.672
And I hope this framework
will be very useful for


00:09:37.672 --> 00:09:41.810
all of your machine learning projects
that you plan to deploy in the future.


00:09:41.810 --> 00:09:43.061
Let's go to the next video.
WEBVTT


00:00:00.000 --> 00:00:01.620
When I'm building a machine


00:00:01.620 --> 00:00:03.645
learning system, I've found that,


00:00:03.645 --> 00:00:06.090
thinking through the
Machine Learning project


00:00:06.090 --> 00:00:07.770
lifecycle is


00:00:07.770 --> 00:00:09.915
a effective way
for me to plan out


00:00:09.915 --> 00:00:12.825
all the steps that
I need to work on.


00:00:12.825 --> 00:00:15.435
When you are working on
Machine Learning system,


00:00:15.435 --> 00:00:17.970
I think you'll find too
that this framework allows


00:00:17.970 --> 00:00:20.460
you to plan out all the
important things you


00:00:20.460 --> 00:00:22.950
need to do in order
to get the system to


00:00:22.950 --> 00:00:25.965
work and also to
minimize surprises.


00:00:25.965 --> 00:00:28.140
Let's dive in. These are


00:00:28.140 --> 00:00:30.795
the major steps of a
Machine Learning project.


00:00:30.795 --> 00:00:33.420
First is scoping,
in which you have


00:00:33.420 --> 00:00:36.310
to define the project or
decide what to work on.


00:00:36.310 --> 00:00:39.050
What exactly do you want to
apply Machine Learning to,


00:00:39.050 --> 00:00:42.930
and what is X and what is Y.


00:00:42.930 --> 00:00:45.980
After having chosen the project,


00:00:45.980 --> 00:00:49.220
you then have to collect data or


00:00:49.220 --> 00:00:52.885
acquire the data you
need for your algorithm.


00:00:52.885 --> 00:00:55.490
This includes defining the data


00:00:55.490 --> 00:00:57.214
and establishing a baseline,


00:00:57.214 --> 00:01:00.605
and then also labeling
and organizing the data.


00:01:00.605 --> 00:01:02.240
There are some best practices for


00:01:02.240 --> 00:01:03.710
this that are non-intuitive


00:01:03.710 --> 00:01:07.205
that you learn more about
later in this week.


00:01:07.205 --> 00:01:08.945
After you have your data,


00:01:08.945 --> 00:01:11.540
you then have to train the model.


00:01:11.540 --> 00:01:12.875
During the model phase,


00:01:12.875 --> 00:01:14.420
you have to select and train


00:01:14.420 --> 00:01:18.420
the model and also
perform error analysis.


00:01:18.420 --> 00:01:20.540
You might know that
Machine Learning


00:01:20.540 --> 00:01:23.135
is often a highly iterative task.


00:01:23.135 --> 00:01:25.235
During the process
of error analysis,


00:01:25.235 --> 00:01:27.650
you may go back and
update the model,


00:01:27.650 --> 00:01:30.185
or you may also go back
to the earlier phase


00:01:30.185 --> 00:01:33.205
and decide you need to
collect more data as well.


00:01:33.205 --> 00:01:35.495
As part of error analysis


00:01:35.495 --> 00:01:37.624
before taking a system
to deployments,


00:01:37.624 --> 00:01:41.330
I'll often also carry
out a final check,


00:01:41.330 --> 00:01:42.830
maybe a final audit,


00:01:42.830 --> 00:01:45.830
to make sure that the system's
performance is good enough


00:01:45.830 --> 00:01:49.675
and that it's sufficiently
reliable for the application.


00:01:49.675 --> 00:01:51.890
Sometimes, an engineer
thinks that when you


00:01:51.890 --> 00:01:54.220
deploy a system, you're done.


00:01:54.220 --> 00:01:55.985
I now tell most people,


00:01:55.985 --> 00:01:58.145
when you deploy a system
for the first time,


00:01:58.145 --> 00:02:00.830
you are maybe about halfway
to the finish line,


00:02:00.830 --> 00:02:03.140
because it's often only after you


00:02:03.140 --> 00:02:05.840
turn on live traffic
that you then


00:02:05.840 --> 00:02:08.300
learn the second half of
the important lessons


00:02:08.300 --> 00:02:11.720
needed in order to get the
system to perform well.


00:02:11.720 --> 00:02:13.685
To carry out the deployment step,


00:02:13.685 --> 00:02:15.880
you have to deploy
it in production,


00:02:15.880 --> 00:02:18.655
write the software needed to
put into production,


00:02:18.655 --> 00:02:20.975
Then also monitor the system,


00:02:20.975 --> 00:02:23.255
track the data that
continues to come in,


00:02:23.255 --> 00:02:25.775
and maintain the system.


00:02:25.775 --> 00:02:28.805
For example, if the data
distribution changes,


00:02:28.805 --> 00:02:32.090
you may need to update the model.


00:02:32.090 --> 00:02:34.805
After the initial deployment,


00:02:34.805 --> 00:02:38.120
maintenance will often
mean going back to


00:02:38.120 --> 00:02:41.420
perform more error analysis
and maybe retrain the model,


00:02:41.420 --> 00:02:45.200
or it might mean taking
the data you get back.


00:02:45.200 --> 00:02:46.580
Now that the system is deployed


00:02:46.580 --> 00:02:48.290
and is running on live data,


00:02:48.290 --> 00:02:50.090
and feeding that back into


00:02:50.090 --> 00:02:54.935
your dataset to then
potentially update your data,


00:02:54.935 --> 00:02:57.980
retrain the model, and
so on until you can put


00:02:57.980 --> 00:03:01.375
an updated model into deployment.


00:03:01.375 --> 00:03:03.980
I found this framework useful for


00:03:03.980 --> 00:03:07.055
a very large variety of
Machine Learning projects.


00:03:07.055 --> 00:03:09.755
From computer vision,
to audio data,


00:03:09.755 --> 00:03:13.515
to structure data, to
many other applications.


00:03:13.515 --> 00:03:15.260
Feel free to take a screenshot of


00:03:15.260 --> 00:03:17.150
this image and use it with


00:03:17.150 --> 00:03:19.160
your friends or by yourself to


00:03:19.160 --> 00:03:21.910
plan out your Machine
Learning project as well.


00:03:21.910 --> 00:03:23.755
Thanks also to learning AIs,


00:03:23.755 --> 00:03:25.930
Steven Layett and
Daniel Pipryata,


00:03:25.930 --> 00:03:28.690
who were instrumental to
developing this diagram.


00:03:28.690 --> 00:03:31.685
In this video, we
quickly went through


00:03:31.685 --> 00:03:35.120
the machine learning
project lifecycle in order


00:03:35.120 --> 00:03:36.770
to deepen our understanding of


00:03:36.770 --> 00:03:38.750
this project lifecycle or be


00:03:38.750 --> 00:03:41.900
useful to walk through
a concrete example.


00:03:41.900 --> 00:03:43.505
In the next video,


00:03:43.505 --> 00:03:46.070
let's step through what
these different steps of


00:03:46.070 --> 00:03:48.890
machine learning project
lifecycle look like in


00:03:48.890 --> 00:03:52.190
the context of a speech
recognition application.


00:03:52.190 --> 00:03:54.720
Let's go on to the next video.
WEBVTT


00:00:00.340 --> 00:00:04.320
One of the successes of deep learning
has been speech recognition.


00:00:04.320 --> 00:00:08.566
Deep learning has made speech recognition
much more accurate than maybe


00:00:08.566 --> 00:00:09.420
a decade ago.


00:00:09.420 --> 00:00:13.963
And this is allowing many of us to
use speech recognition in our smart


00:00:13.963 --> 00:00:18.370
speakers on our smartphones for
voice search and in other context.


00:00:18.370 --> 00:00:21.855
You may have heard occasionally
about the research


00:00:21.855 --> 00:00:25.440
work that goes into building
better speech models.


00:00:25.440 --> 00:00:30.216
But what else is needed to actually
build a valuable production deployment


00:00:30.216 --> 00:00:32.130
speech recognition system.


00:00:32.130 --> 00:00:36.417
Let's use the machine learning project
life cycle to set through a speech


00:00:36.417 --> 00:00:41.327
recognition example so you can understand
all the steps needed to actually build and


00:00:41.327 --> 00:00:43.240
deploy such a system.


00:00:43.240 --> 00:00:48.332
I've worked on speech recognition systems
in a commercial context before and


00:00:48.332 --> 00:00:52.957
so the first step of that was scoping
have to first define the project and


00:00:52.957 --> 00:00:56.349
just make a decision to
work on speech recognition,


00:00:56.349 --> 00:00:59.761
say for voice search as part
of defining the project.


00:00:59.761 --> 00:01:02.887
That also encourage you
to try to estimate or


00:01:02.887 --> 00:01:06.140
maybe at least estimate the key metrics.


00:01:06.140 --> 00:01:08.460
This will be very problem dependence.


00:01:08.460 --> 00:01:12.760
Almost every application will have his
own unique set of goals and metrics.


00:01:12.760 --> 00:01:17.261
But the case of speech recognition,
some things I cared about where how


00:01:17.261 --> 00:01:20.280
accurate is the speech
system was the latency?


00:01:20.280 --> 00:01:23.340
How long does the system take
to transcribe speech and


00:01:23.340 --> 00:01:24.780
what is the throughput?


00:01:24.780 --> 00:01:27.740
How many queries per second we handle.


00:01:27.740 --> 00:01:33.110
And then if possible, you might also
try to estimate the resources needed.


00:01:33.110 --> 00:01:37.451
So how much time, how much compute
how much budget as well as timeline.


00:01:37.451 --> 00:01:40.440
How long will it take to
carry out this project?


00:01:40.440 --> 00:01:45.660
I'll have a lot more to say on
scoping in week three of this course.


00:01:45.660 --> 00:01:51.340
So we'll come back to this topic and
describe this in greater detail as well.


00:01:51.340 --> 00:01:55.214
The next step is the data stage where
you have to define the data and


00:01:55.214 --> 00:01:59.140
establish a baseline and
also label and organize the data.


00:01:59.140 --> 00:02:00.650
What's hard about this?


00:02:00.650 --> 00:02:05.328
One of the challenges of practical
speech recognition systems is is


00:02:05.328 --> 00:02:10.251
the data label consistently,
here's an audio clip of a fairly typical


00:02:10.251 --> 00:02:15.011
recording you might get if you're
working on speech recognition for


00:02:15.011 --> 00:02:20.040
voice search, let me play this audio clip,
"Um, today's weather"


00:02:20.040 --> 00:02:24.031
And the question is given this
audio clip that you just heard,


00:02:24.031 --> 00:02:28.340
would you want
to transcribe it like that?


00:02:28.340 --> 00:02:30.781
Which if you have
transcriptionist label the data,


00:02:30.781 --> 00:02:33.350
this would be a perfectly
reasonable transcription.


00:02:33.350 --> 00:02:36.590
Or would you want to
transcribe it like that?


00:02:36.590 --> 00:02:39.938
Which is also a completely
reasonable transcription or


00:02:39.938 --> 00:02:44.777
should the transcriptionist say, well
there's often a lot of noise and audio,


00:02:44.777 --> 00:02:49.119
you know, maybe there's a sound of
a conclave, something fell down and


00:02:49.119 --> 00:02:51.421
you don't want to transcribe noise.


00:02:51.421 --> 00:02:56.030
So maybe it's just noise and
you should transcribe it like that.


00:02:56.030 --> 00:03:01.600
It turns out that any of these three ways
of transcribing the audio is just fine.


00:03:01.600 --> 00:03:04.525
I would probably prefer either
the first or the second, not the third.


00:03:04.525 --> 00:03:08.526
But what what hurts your learning
algorithm's performance is if one third of


00:03:08.526 --> 00:03:12.070
the transcription is used the first,
one third, the second and


00:03:12.070 --> 00:03:14.183
one third third way of trans driving.


00:03:14.183 --> 00:03:18.644
Because then your data is inconsistent and
confusing for the learning


00:03:18.644 --> 00:03:23.710
algorithm because how is the learning
algorithm supposed to guess which one of these


00:03:23.710 --> 00:03:29.040
conventions specific transcription has
happened to use for an audio clip.


00:03:29.040 --> 00:03:32.640
So Spotting correcting
consistencies like that.


00:03:32.640 --> 00:03:37.489
Maybe just asking everyone to standardize
on this first convention that


00:03:37.489 --> 00:03:42.120
can have a significant impact on
your learning algorithm's performance.


00:03:42.120 --> 00:03:46.800
So we'll come back later in this course
to dive into some best practices for


00:03:46.800 --> 00:03:50.940
how to spot inconsistencies and
how to address them.


00:03:50.940 --> 00:03:54.861
Other examples of data
definition questions for


00:03:54.861 --> 00:04:00.978
an audio clip like today's whether,
how much silence do you want before and


00:04:00.978 --> 00:04:05.391
after each clip after a speaker
has stopped speaking.


00:04:05.391 --> 00:04:09.370
Do you want to include another 100
milliseconds of silence after that?


00:04:09.370 --> 00:04:15.540
Or 300 milliseconds or
500 milliseconds, half a second?


00:04:15.540 --> 00:04:19.740
Or how do you perform
volume normalization?


00:04:19.740 --> 00:04:24.550
Some speakers speak loudly, some are less
loud and then there's actually a tricky


00:04:24.550 --> 00:04:28.534
case of if you have a single audio
clip with some really loud volume and


00:04:28.534 --> 00:04:31.930
some really soft volume,
all within the same audio clip.


00:04:31.930 --> 00:04:36.647
So how do you perform volume
normalization questions


00:04:36.647 --> 00:04:41.440
like all of these are data
definition questions.


00:04:41.440 --> 00:04:43.760
A lot of progress in machine learning.


00:04:43.760 --> 00:04:45.271
That is a lot of machine learning.


00:04:45.271 --> 00:04:50.109
Research was driven by researchers
working to improve performance on


00:04:50.109 --> 00:04:51.600
benchmark data set.


00:04:51.600 --> 00:04:56.122
In that model, researchers
might download the data set and


00:04:56.122 --> 00:04:58.740
just work on that fixed data set.


00:04:58.740 --> 00:05:03.325
And this mindset has led to tremendous
progress in machine learning so


00:05:03.325 --> 00:05:07.598
no complaints at all about this mindset,
but if you are working on


00:05:07.598 --> 00:05:11.900
a production system then you don't
have to keep the data set fix.


00:05:11.900 --> 00:05:16.850
I often edit the training set or even at
the test set if that's what's needed in


00:05:16.850 --> 00:05:22.140
order to improve the data quality to
get a production system to work better.


00:05:22.140 --> 00:05:27.013
So what are practical ways to do this
effectively not an ad hoc way, but


00:05:27.013 --> 00:05:31.740
systematic frameworks for
making sure you have high quality data.


00:05:31.740 --> 00:05:34.213
You learn more about this
later in this course and


00:05:34.213 --> 00:05:36.151
later in the specialization as well.


00:05:37.240 --> 00:05:42.266
After you've collected your data set,
the next step is modeling,


00:05:42.266 --> 00:05:47.840
in which you have to select and train
the model and perform error analysis.


00:05:47.840 --> 00:05:54.140
The three key inputs that go into training
a machine learning model are the code that


00:05:54.140 --> 00:06:00.740
is the algorithm or the neural network
model architecture that you might choose.


00:06:00.740 --> 00:06:05.226
You also have to pick hyper parameters and
then there's the data and


00:06:05.226 --> 00:06:09.100
running the code with your
hyper parameters on your data.


00:06:09.100 --> 00:06:14.873
Gives you the machine learning model the
celebrate, a machine learning model for


00:06:14.873 --> 00:06:18.910
learning from, say,
audio clips to text transcripts.


00:06:18.910 --> 00:06:22.356
I found that in a lot of research work or


00:06:22.356 --> 00:06:28.174
academic work you tend to hold
the data fixed and vary the code and


00:06:28.174 --> 00:06:34.661
may be vary the hyper parameters in
order to try to get good performance.


00:06:35.740 --> 00:06:40.837
In contrast, I found that for
a lot of product teams, if your main


00:06:40.837 --> 00:06:46.862
goal is to just build and deploy a working
valuable machine learning system,


00:06:46.862 --> 00:06:51.961
I found that it can be even more
effective to hold the code fixed and


00:06:51.961 --> 00:06:57.810
to instead focus on optimizing the data
and maybe the hyper parameters.


00:06:57.810 --> 00:07:01.891
In order to get a high performing model,


00:07:01.891 --> 00:07:06.933
A machine learning system
includes both codes and


00:07:06.933 --> 00:07:11.734
data and
also hyper parameters that there maybe


00:07:11.734 --> 00:07:16.440
a bit easier to optimize than the code or
data.


00:07:16.440 --> 00:07:21.400
And I found that rather than taking
a model centric view of trying to


00:07:21.400 --> 00:07:25.829
optimize the code to your fixed
data set for many problems,


00:07:25.829 --> 00:07:31.856
you can use an open source implementation
of something you download of GIT hub and


00:07:31.856 --> 00:07:35.160
instead just focus on optimizing the data.


00:07:35.160 --> 00:07:40.160
So during modeling, do you have to select
and train some model architecture?


00:07:40.160 --> 00:07:45.073
Maybe some neural network architecture
error analysis can then tell


00:07:45.073 --> 00:07:48.540
you where your model still falls short.


00:07:48.540 --> 00:07:53.440
And if you can use that error analysis
to tell you how to systematically


00:07:53.440 --> 00:07:57.700
improve your data,
maybe improve the code too. That's okay.


00:07:57.700 --> 00:08:03.560
But often if error analysis can tell you
how to systematically improve the data,


00:08:03.560 --> 00:08:08.740
that can be a very efficient way for
you to get to a high accuracy model.


00:08:08.740 --> 00:08:13.497
And part of the trick is you don't want
to just feel like you need to collect


00:08:13.497 --> 00:08:17.290
more data all the time because
we can always use more data.


00:08:17.290 --> 00:08:20.293
But rather than just
trying to collect more and


00:08:20.293 --> 00:08:25.145
more and more data, which is helpful but
can be expensive if their analysis


00:08:25.145 --> 00:08:29.842
can help you be more targeted in exactly
what data to collect that can help


00:08:29.842 --> 00:08:33.880
you'd be much more efficient
in building an accurate model.


00:08:33.880 --> 00:08:36.911
Finally, when you have
trained the model and


00:08:36.911 --> 00:08:40.899
when error analysis seems to
suggest is working well enough,


00:08:40.899 --> 00:08:45.710
you're then ready to go into deployment,
Check speech recognition.


00:08:45.710 --> 00:08:50.340
This is how you might
deploy a speech system.


00:08:50.340 --> 00:08:52.130
You have a mobile phone.


00:08:52.130 --> 00:08:57.720
This would be an edge device with software
running locally on your phone.


00:08:57.720 --> 00:09:02.310
That software taps into the microphone
to record what someone is saying.


00:09:02.310 --> 00:09:06.811
Maybe for voice search and
in a typical implementation of


00:09:06.811 --> 00:09:11.440
speech recognition,
you would use a V A D module.


00:09:11.440 --> 00:09:14.461
V D stands for a voice activity detection.


00:09:15.540 --> 00:09:16.640
Yeah.


00:09:16.640 --> 00:09:20.460
And it's usually a relatively
simple algorithm.


00:09:20.460 --> 00:09:25.991
Maybe a learning algorithm and
the job of the V A D allows the smartphone


00:09:25.991 --> 00:09:31.713
to select out just the audio that
contains hopefully someone speaking so


00:09:31.713 --> 00:09:36.910
that you can send only that audio
clip to your prediction server.


00:09:36.910 --> 00:09:41.370
And in this case maybe the prediction
server lives into cloud.


00:09:41.370 --> 00:09:43.740
This would be a common deployment pattern.


00:09:43.740 --> 00:09:48.544
The prediction server then
returns both the transcript so


00:09:48.544 --> 00:09:53.440
the user so you can see what
the system thinks you said.


00:09:53.440 --> 00:09:55.960
And it also returns to search results.


00:09:55.960 --> 00:09:59.928
If you're doing voice search and
the transcript and search results are then


00:09:59.928 --> 00:10:04.140
displayed in the front and
code running on your mobile phone.


00:10:04.140 --> 00:10:09.029
So implementing this type of
system would be the work needed to


00:10:09.029 --> 00:10:13.825
deploy a speech model in
production even after it's running


00:10:13.825 --> 00:10:19.340
though you still have to monitor and
maintain the system.


00:10:19.340 --> 00:10:24.029
So here's something that happened to
me once my team had built a speech


00:10:24.029 --> 00:10:28.740
recognition system and
it was trained mainly on adult voices.


00:10:28.740 --> 00:10:31.764
We pushed into production,
random production and


00:10:31.764 --> 00:10:36.334
we found that over time more and
more young individuals, kind of teenagers,


00:10:36.334 --> 00:10:41.328
you know, sometimes even younger seem to
be using our speech recognition system and


00:10:41.328 --> 00:10:45.030
the voices are very young
individuals just sound different.


00:10:45.030 --> 00:10:48.781
And so my speech systems
performance started to degrade.


00:10:48.781 --> 00:10:54.770
We just were not that good at recognizing
speech as spoken by younger voices.


00:10:54.770 --> 00:10:56.202
And so he had to go back and


00:10:56.202 --> 00:10:59.828
find a way to collect more data
are the things in order to fix it.


00:10:59.828 --> 00:11:06.729
So one of the key challenges when it
comes to deployment is concept drift or


00:11:06.729 --> 00:11:11.181
data drift,
which is what happens when the data


00:11:11.181 --> 00:11:15.857
distribution changes,
such as there are more and


00:11:15.857 --> 00:11:21.791
more young voices being fed to
the speech recognition system.


00:11:21.791 --> 00:11:26.584
And knowing how to put in place
appropriate monitors to spot such


00:11:26.584 --> 00:11:31.374
problems and then also how to fix
them in a timely way is a key skill


00:11:31.374 --> 00:11:36.281
needed to make sure your production
deployment creates a value.


00:11:36.281 --> 00:11:40.321
You hope it will to recap in this video.


00:11:40.321 --> 00:11:45.320
You saw the full life cycle of a machine
learning project using speech recognition


00:11:45.320 --> 00:11:47.240
as the running example.


00:11:47.240 --> 00:11:52.940
So from scoping to data to
modeling to deployment.


00:11:52.940 --> 00:11:57.333
Next I want to briefly share
review the major concepts and


00:11:57.333 --> 00:12:00.740
sequencing you learn about in this course.


00:12:00.740 --> 00:12:02.961
So come with me to the next video.
WEBVTT


00:00:00.000 --> 00:00:03.630
You've seen the machine
learning project life cycle.


00:00:03.630 --> 00:00:05.550
Let's briefly go over what


00:00:05.550 --> 00:00:07.935
you'll learn in the
rest of this course.


00:00:07.935 --> 00:00:09.720
Even though I presented


00:00:09.720 --> 00:00:12.585
the life cycle going
from left to right,


00:00:12.585 --> 00:00:15.480
I found that for learning
these materials,


00:00:15.480 --> 00:00:17.340
it will be more efficient for


00:00:17.340 --> 00:00:19.215
you to start at the end goal,


00:00:19.215 --> 00:00:20.790
and start from deployment,


00:00:20.790 --> 00:00:22.740
and then work backwards to


00:00:22.740 --> 00:00:25.050
modeling data and then scoping.


00:00:25.050 --> 00:00:26.715
In the rest of this week,


00:00:26.715 --> 00:00:28.095
starting with the next video,


00:00:28.095 --> 00:00:30.585
you'll learn about the
most important ideas


00:00:30.585 --> 00:00:32.190
in Deployment.


00:00:32.190 --> 00:00:34.125
Next week, in Week 2,


00:00:34.125 --> 00:00:36.285
you'll learn about modeling.


00:00:36.285 --> 00:00:38.430
You may have learned
about how to train


00:00:38.430 --> 00:00:40.970
a machine learning model
from other courses.


00:00:40.970 --> 00:00:42.875
In this video, I'll share


00:00:42.875 --> 00:00:46.280
some new ideas that you may
not have heard before of


00:00:46.280 --> 00:00:49.595
how to systematically use
a data-centric approach


00:00:49.595 --> 00:00:51.380
to be more efficient in how you


00:00:51.380 --> 00:00:53.500
improve the performance
of your model.


00:00:53.500 --> 00:00:55.230
Then in the third,


00:00:55.230 --> 00:00:57.195
and final week of this course,


00:00:57.195 --> 00:00:59.135
you'll learn about data.


00:00:59.135 --> 00:01:02.375
How to define data and
establish a baseline,


00:01:02.375 --> 00:01:04.190
and how to label and organize


00:01:04.190 --> 00:01:07.245
your data in a way
that is systematic.


00:01:07.245 --> 00:01:09.540
Not Ad hoc, not hacking around in


00:01:09.540 --> 00:01:10.940
the Jupyter notebook in the hope


00:01:10.940 --> 00:01:12.500
that you stumble onto
the right insights,


00:01:12.500 --> 00:01:14.135
but in a more systematic way


00:01:14.135 --> 00:01:16.160
that helps you be
more efficient in


00:01:16.160 --> 00:01:18.230
defining the data that will help


00:01:18.230 --> 00:01:20.920
the modeling to help
you get to deployment.


00:01:20.920 --> 00:01:23.300
Then finally, in Week 3 we'll


00:01:23.300 --> 00:01:26.225
also have an optional
section on scoping.


00:01:26.225 --> 00:01:29.795
In which I hope to share with
you some tips I've learned


00:01:29.795 --> 00:01:33.695
on how to define effective
machine learning projects.


00:01:33.695 --> 00:01:36.875
Throughout this course, you'll
also learn about MLOps,


00:01:36.875 --> 00:01:39.110
or machine learning operations.


00:01:39.110 --> 00:01:40.865
Which is an emerging discipline


00:01:40.865 --> 00:01:43.070
that comprises a set of tools and


00:01:43.070 --> 00:01:45.170
principles to support progress


00:01:45.170 --> 00:01:47.615
through the machine ML
project life cycle,


00:01:47.615 --> 00:01:50.705
but especially these three steps.


00:01:50.705 --> 00:01:53.885
For example, at Landing
AI, where I'm CEO,


00:01:53.885 --> 00:01:55.970
we used to do a lot of
these steps manually,


00:01:55.970 --> 00:01:57.725
which is okay, but slow.


00:01:57.725 --> 00:02:00.290
But after building
an MLOps tool called


00:02:00.290 --> 00:02:02.855
LandingLens for computer
vision applications,


00:02:02.855 --> 00:02:04.895
all these steps
became much quicker.


00:02:04.895 --> 00:02:07.190
The key idea in MLOps is


00:02:07.190 --> 00:02:10.235
that systematic ways to
think about scoping, data,


00:02:10.235 --> 00:02:12.560
modeling, and
deployment, and also


00:02:12.560 --> 00:02:16.210
software tools to support
the best practices.


00:02:16.210 --> 00:02:18.260
That's it. In this course,


00:02:18.260 --> 00:02:20.000
we're going to start
at the end goal,


00:02:20.000 --> 00:02:23.855
start from deployment, and
then work our way backwards.


00:02:23.855 --> 00:02:25.455
As you already know,


00:02:25.455 --> 00:02:28.940
being able to deploy system
is one of the most important,


00:02:28.940 --> 00:02:31.460
and valuable skills in
Machine Learning today.


00:02:31.460 --> 00:02:35.195
Let's go on to the next video
where we'll dive deep into


00:02:35.195 --> 00:02:37.880
the most important ideas


00:02:37.880 --> 00:02:40.655
needed to deploy machine
learning systems.


00:02:40.655 --> 00:02:43.260
I will see you in the next video.
WEBVTT


00:00:00.150 --> 00:00:03.220
One of the most
exciting moments of


00:00:03.220 --> 00:00:05.200
any machine learning
project is when you


00:00:05.200 --> 00:00:07.610
get to deploy your model,


00:00:07.610 --> 00:00:09.825
but what makes deployment hard?


00:00:09.825 --> 00:00:12.670
I think there are
two major categories


00:00:12.670 --> 00:00:15.250
of challenges in deploying
a machine learning model.


00:00:15.250 --> 00:00:17.380
First, are the machine learning


00:00:17.380 --> 00:00:19.030
or the statistical issues,


00:00:19.030 --> 00:00:22.135
and second, are the
software engine issues.


00:00:22.135 --> 00:00:23.770
Let's start with both of
these so that you can


00:00:23.770 --> 00:00:25.330
understand what you need to do to


00:00:25.330 --> 00:00:26.530
make sure that you have


00:00:26.530 --> 00:00:29.020
a successful deployment
of your system.


00:00:29.020 --> 00:00:33.370
One of the challenges of
a lot of deployments is,


00:00:33.370 --> 00:00:36.220
concept drift and, data drift.


00:00:36.220 --> 00:00:38.110
Loosely, this means what if


00:00:38.110 --> 00:00:39.610
your data changes after


00:00:39.610 --> 00:00:41.550
your system has
already been deployed?


00:00:41.550 --> 00:00:43.900
I had previously given an example


00:00:43.900 --> 00:00:46.060
from manufacturing
where you might have


00:00:46.060 --> 00:00:48.670
trained a learning
algorithm to detect


00:00:48.670 --> 00:00:50.560
scratches on smartphones under


00:00:50.560 --> 00:00:52.284
one set of lighting conditions,


00:00:52.284 --> 00:00:55.680
and then maybe the lighting
in the factory changes.


00:00:55.680 --> 00:00:59.810
That's one example of the
distribution of data changers.


00:00:59.810 --> 00:01:02.050
Let's walk through
a second example


00:01:02.050 --> 00:01:03.680
using speech recognition.


00:01:03.680 --> 00:01:05.950
I train a few speech
recognition systems,


00:01:05.950 --> 00:01:08.710
and when I built speech systems,


00:01:08.710 --> 00:01:12.030
quite often I would have
some purchased data.


00:01:12.030 --> 00:01:14.200
This would be some
purchased or licensed data,


00:01:14.200 --> 00:01:16.900
which includes both the input x,


00:01:16.900 --> 00:01:19.479
the audio, as well
as the transcript


00:01:19.479 --> 00:01:23.195
y that the speech system
supports it's output.


00:01:23.195 --> 00:01:25.620
In addition to data


00:01:25.620 --> 00:01:27.495
that you might purchase
from a vendor,


00:01:27.495 --> 00:01:28.800
you might also have


00:01:28.800 --> 00:01:32.370
historical user data
of user speaking to


00:01:32.370 --> 00:01:35.130
your application together with


00:01:35.130 --> 00:01:38.055
transcripts of that
raw user data.


00:01:38.055 --> 00:01:39.630
Such user data, of course,


00:01:39.630 --> 00:01:40.650
should be collected with


00:01:40.650 --> 00:01:42.840
very clear user opt-in


00:01:42.840 --> 00:01:46.725
permission and clear
safeguards for user privacy.


00:01:46.725 --> 00:01:49.860
After you've trained your
speech recognition system


00:01:49.860 --> 00:01:51.360
on a data set like this,


00:01:51.360 --> 00:01:54.060
you might then evaluate
it on a test set,


00:01:54.060 --> 00:01:56.520
but because speech data
does change over time,


00:01:56.520 --> 00:01:58.230
when I build speech
recognition systems,


00:01:58.230 --> 00:01:59.490
sometimes I would collect


00:01:59.490 --> 00:02:02.670
a dev set or hold
out validation set


00:02:02.670 --> 00:02:04.485
as well as test set


00:02:04.485 --> 00:02:07.460
comprising data from just
the last few months.


00:02:07.460 --> 00:02:08.850
You can test it on


00:02:08.850 --> 00:02:11.460
fairly recent data to make
sure your system works,


00:02:11.460 --> 00:02:13.800
even on relatively recent data.


00:02:13.800 --> 00:02:15.660
After you push the system to


00:02:15.660 --> 00:02:17.595
deployments, the question is,


00:02:17.595 --> 00:02:19.680
will the data change
or after you've


00:02:19.680 --> 00:02:22.425
run it for a few weeks
or a few months,


00:02:22.425 --> 00:02:25.650
has the data changed yet again?


00:02:25.650 --> 00:02:27.880
The data has changed,


00:02:27.880 --> 00:02:32.170
such as the language
changes or maybe people


00:02:32.170 --> 00:02:33.940
are using a brand new model of


00:02:33.940 --> 00:02:36.420
smartphone which has a
different microphone,


00:02:36.420 --> 00:02:37.995
so the audio sounds different,


00:02:37.995 --> 00:02:39.130
then the performance of


00:02:39.130 --> 00:02:41.560
a speech recognition
system can degrade.


00:02:41.560 --> 00:02:43.465
It's important for you to


00:02:43.465 --> 00:02:46.420
recognize how the
data has changed,


00:02:46.420 --> 00:02:48.280
and if you need to update


00:02:48.280 --> 00:02:50.680
your learning
algorithm as a result.


00:02:50.680 --> 00:02:56.135
When data changes, sometimes
it is a gradual change,


00:02:56.135 --> 00:02:59.560
such as the English
language which does change,


00:02:59.560 --> 00:03:02.465
but changes very slowly
with new vocabulary


00:03:02.465 --> 00:03:06.145
introduced at a
relatively slow rate.


00:03:06.145 --> 00:03:11.085
Sometimes data changes very


00:03:11.085 --> 00:03:15.015
suddenly where there's a
sudden shock to a system.


00:03:15.015 --> 00:03:19.445
For example, when COVID-19
the pandemic hit,


00:03:19.445 --> 00:03:23.715
a lot of credit card fraud
systems started to not work


00:03:23.715 --> 00:03:26.190
because the purchase patterns


00:03:26.190 --> 00:03:28.950
of individuals suddenly changed.


00:03:28.950 --> 00:03:31.890
Many people that did relatively
little online shopping


00:03:31.890 --> 00:03:36.240
suddenly started to use
much more online shopping.


00:03:36.240 --> 00:03:38.460
The way that people were using


00:03:38.460 --> 00:03:40.980
credit cards changed
very suddenly,


00:03:40.980 --> 00:03:45.120
and his actually tripped up
a lot of anti fraud systems.


00:03:45.120 --> 00:03:47.310
This very sudden shift to


00:03:47.310 --> 00:03:49.710
the data distribution meant


00:03:49.710 --> 00:03:52.170
that many machine learning
teams were scrambling


00:03:52.170 --> 00:03:53.400
a little bit at the start of


00:03:53.400 --> 00:03:56.160
COVID to collect new
data and retrain


00:03:56.160 --> 00:03:58.020
systems in order to make them


00:03:58.020 --> 00:04:01.455
adapt to this very new
data distribution.


00:04:01.455 --> 00:04:04.739
Sometimes the terminology
of how to describe


00:04:04.739 --> 00:04:09.195
these data changes is not
used completely consistently,


00:04:09.195 --> 00:04:12.750
but sometimes the term
data drift is used to


00:04:12.750 --> 00:04:16.890
describe if the input
distribution x changes,


00:04:16.890 --> 00:04:19.980
such as if a new politician or


00:04:19.980 --> 00:04:22.350
celebrity suddenly becomes well


00:04:22.350 --> 00:04:24.915
known and he's mentioned
much more than before.


00:04:24.915 --> 00:04:31.720
The term concept drift refers
to if the desired mapping.


00:04:31.720 --> 00:04:38.445
From x to y changes such
as if, before COVID-19.


00:04:38.445 --> 00:04:40.695
Perhaps for a given user,


00:04:40.695 --> 00:04:44.135
a lot of surprising
online purchases,


00:04:44.135 --> 00:04:47.645
should have flagged
that account for fraud.


00:04:47.645 --> 00:04:51.045
After the start of COVID-19,


00:04:51.045 --> 00:04:52.775
maybe those same purchases,


00:04:52.775 --> 00:04:55.925
would not have really
been any cause for alarm,


00:04:55.925 --> 00:04:57.320
in terms of flagging.


00:04:57.320 --> 00:05:00.215
That the credit card
may have been stolen.


00:05:00.215 --> 00:05:02.465
Another example of Concept drift,


00:05:02.465 --> 00:05:05.175
let's say that x is
the size of a house,


00:05:05.175 --> 00:05:07.130
and y is the price of a house,


00:05:07.130 --> 00:05:10.110
because you're trying to
estimate housing prices.


00:05:10.110 --> 00:05:13.025
If because of inflation
or changes in the market,


00:05:13.025 --> 00:05:16.220
houses may become more
expensive over time.


00:05:16.220 --> 00:05:17.930
The same size house,


00:05:17.930 --> 00:05:20.660
will end up with a higher price.


00:05:20.660 --> 00:05:22.415
That would be Concept drift.


00:05:22.415 --> 00:05:24.950
Maybe the size of
houses haven't changed,


00:05:24.950 --> 00:05:27.875
but the price of a
given house changes.


00:05:27.875 --> 00:05:29.765
Whereas data drift would be if,


00:05:29.765 --> 00:05:32.175
say, people start
building larger houses,


00:05:32.175 --> 00:05:34.610
or start building
smaller houses and


00:05:34.610 --> 00:05:36.710
thus the input distribution of


00:05:36.710 --> 00:05:39.435
the sizes of houses
actually changes over time.


00:05:39.435 --> 00:05:42.275
When you deploy a
machine learning system,


00:05:42.275 --> 00:05:44.310
one of the most important tasks,


00:05:44.310 --> 00:05:46.820
will often be to
make sure you can


00:05:46.820 --> 00:05:49.755
detect and manage any changes.


00:05:49.755 --> 00:05:51.545
Including both Concept drift,


00:05:51.545 --> 00:05:53.870
which is when the definition of


00:05:53.870 --> 00:05:56.430
what is y given x changes.


00:05:56.430 --> 00:05:57.990
As well as Data drift,


00:05:57.990 --> 00:06:01.100
which is if the
distribution of x changes,


00:06:01.100 --> 00:06:04.610
even if the mapping from
x or y does not change.


00:06:04.610 --> 00:06:07.984
In addition to managing
these changes to the data,


00:06:07.984 --> 00:06:09.860
a second set of issues,


00:06:09.860 --> 00:06:11.240
that you will have to


00:06:11.240 --> 00:06:14.010
manage to deploy a
system successfully,


00:06:14.010 --> 00:06:17.940
are Software engineering issues.


00:06:17.980 --> 00:06:21.575
You are implementing
a prediction service


00:06:21.575 --> 00:06:23.430
whose job it is to


00:06:23.430 --> 00:06:28.970
take queries x and
output prediction y,


00:06:28.970 --> 00:06:31.935
you have a lot of
design choices as


00:06:31.935 --> 00:06:34.820
to how to implement
this piece of software.


00:06:34.820 --> 00:06:36.320
Here's a checklist of questions,


00:06:36.320 --> 00:06:37.740
that might help you with making


00:06:37.740 --> 00:06:39.130
the appropriate decisions for


00:06:39.130 --> 00:06:42.120
managing the software
engineering issues.


00:06:42.120 --> 00:06:45.055
One decision you have to make
for your application is,


00:06:45.055 --> 00:06:46.985
do you need Real time predictions


00:06:46.985 --> 00:06:49.310
or are Batch predictions?


00:06:49.310 --> 00:06:51.265
For example, if you are


00:06:51.265 --> 00:06:53.275
building a speech
recognition system,


00:06:53.275 --> 00:06:54.730
where the user speaks


00:06:54.730 --> 00:06:56.545
and you need to get
a response back,


00:06:56.545 --> 00:06:58.090
in half a second,


00:06:58.090 --> 00:07:01.025
then clearly you need
real time predictions.


00:07:01.025 --> 00:07:04.405
In contrast, I have
also built systems,


00:07:04.405 --> 00:07:08.260
for hospitals that
take patient records.


00:07:08.260 --> 00:07:10.730
Take electronic health
records and run


00:07:10.730 --> 00:07:13.310
an overnight batch
process to see if there's


00:07:13.310 --> 00:07:17.060
something associated with the
patients, that we can spot.


00:07:17.060 --> 00:07:18.625
In that type of system,


00:07:18.625 --> 00:07:20.120
it was fine if we just ran it,


00:07:20.120 --> 00:07:24.740
in a batch of patient
records once per night.


00:07:24.740 --> 00:07:27.615
Whether you need to write
real time software,


00:07:27.615 --> 00:07:31.250
they can respond within
hundreds of milliseconds or


00:07:31.250 --> 00:07:33.020
whether you can
write software that


00:07:33.020 --> 00:07:35.265
just does a lot of
computation overnight,


00:07:35.265 --> 00:07:39.530
that will affect how you
implement your software.


00:07:39.530 --> 00:07:42.260
By the way, later this week,


00:07:42.260 --> 00:07:44.180
you also get to step


00:07:44.180 --> 00:07:46.850
through an optional
programming exercise,


00:07:46.850 --> 00:07:48.829
where you get to implement


00:07:48.829 --> 00:07:50.870
a real time prediction service,


00:07:50.870 --> 00:07:52.925
on your own computer.


00:07:52.925 --> 00:07:54.620
You see that at the optional


00:07:54.620 --> 00:07:56.205
exercise at the end of this week.


00:07:56.205 --> 00:07:58.375
Second question you
need to ask is,


00:07:58.375 --> 00:08:01.700
does your prediction
service run into clouds or


00:08:01.700 --> 00:08:05.375
does it run at the edge or
maybe even in a Web browser?


00:08:05.375 --> 00:08:07.790
Today there are many
speech recognition systems


00:08:07.790 --> 00:08:08.850
that run in the cloud,


00:08:08.850 --> 00:08:12.765
because having the compute
resources of the cloud,


00:08:12.765 --> 00:08:15.950
allows for more accurate
speech recognition.


00:08:15.950 --> 00:08:19.460
There are also some speech
systems, for example,


00:08:19.460 --> 00:08:21.710
a lot of speech
systems within cars,


00:08:21.710 --> 00:08:23.675
actually run at the edge.


00:08:23.675 --> 00:08:25.310
There are also some mobile speech


00:08:25.310 --> 00:08:26.705
recognition systems that work,


00:08:26.705 --> 00:08:30.060
even if your Wi-Fi is turned off.


00:08:30.060 --> 00:08:32.210
Those would be examples of


00:08:32.210 --> 00:08:34.340
speech systems that
run at the edge.


00:08:34.340 --> 00:08:35.704
When I am deploying


00:08:35.704 --> 00:08:37.970
visual inspection
systems in factories,


00:08:37.970 --> 00:08:42.945
I pretty much almost always
run that at the edge as well.


00:08:42.945 --> 00:08:45.135
Because sometimes unavoidably,


00:08:45.135 --> 00:08:47.494
the Internet connection
between the factory,


00:08:47.494 --> 00:08:49.790
and the rest of the
Internet may go down.


00:08:49.790 --> 00:08:52.115
You just can't afford to
shut down the factory,


00:08:52.115 --> 00:08:55.035
whenever its Internet
connection goes down,


00:08:55.035 --> 00:08:56.480
which happens very rarely


00:08:56.480 --> 00:08:58.200
but maybe sometimes does happen.


00:08:58.200 --> 00:09:00.420
With the rise of
modern Web browsers,


00:09:00.420 --> 00:09:01.875
there are better tools,


00:09:01.875 --> 00:09:04.444
for deploying
learning algorithms,


00:09:04.444 --> 00:09:07.800
right there within a
Web browser as well.


00:09:07.800 --> 00:09:10.275
When building a
prediction service,


00:09:10.275 --> 00:09:13.110
it's also useful to
take into account,


00:09:13.110 --> 00:09:16.295
how much computer
resources you have.


00:09:16.295 --> 00:09:18.630
There have been quite a few
times where I trained a


00:09:18.630 --> 00:09:21.620
neural network on a
very powerful GPU,


00:09:21.620 --> 00:09:24.510
only to realize that
I couldn't afford


00:09:24.510 --> 00:09:28.170
an equally powerful set
of GPUs for deployments,


00:09:28.170 --> 00:09:30.370
and wound up having to
do something else to


00:09:30.370 --> 00:09:33.055
compress or reduce
the model complexity.


00:09:33.055 --> 00:09:36.310
If you know how much CPU or


00:09:36.310 --> 00:09:38.160
GPU resources and maybe also


00:09:38.160 --> 00:09:39.580
how much memory resources you


00:09:39.580 --> 00:09:41.744
have for your prediction service,


00:09:41.744 --> 00:09:43.540
then that could help you choose


00:09:43.540 --> 00:09:46.490
the right software architecture.


00:09:46.490 --> 00:09:48.845
Depending on your application


00:09:48.845 --> 00:09:51.110
especially if it's
real-time application,


00:09:51.110 --> 00:09:55.745
latency and throughputs such
as measured in terms of QPS,


00:09:55.745 --> 00:09:57.770
queries per second, will be


00:09:57.770 --> 00:10:00.380
other software engineering metrics
you may need to hit.


00:10:00.380 --> 00:10:03.180
In speech recognition
is not uncommon to


00:10:03.180 --> 00:10:06.120
want to get an answer
back to the user,


00:10:06.120 --> 00:10:08.945
within half a second
or 500 milliseconds.


00:10:08.945 --> 00:10:11.190
Of this 500
millisecond budget you


00:10:11.190 --> 00:10:13.440
may be able to allocate only say,


00:10:13.440 --> 00:10:17.055
300 milliseconds to your
speech recognition.


00:10:17.055 --> 00:10:21.825
That gives a latency
requirement for your system.


00:10:21.825 --> 00:10:25.600
Throughput refers to
how many queries per


00:10:25.600 --> 00:10:27.420
second do you need to


00:10:27.420 --> 00:10:30.695
handle given your
compute resources,


00:10:30.695 --> 00:10:33.865
maybe given a certain
number of Cloud Service.


00:10:33.865 --> 00:10:36.995
For example, if you're building
a system that needs to


00:10:36.995 --> 00:10:40.095
handle 1000 queries per second,


00:10:40.095 --> 00:10:43.200
it would be useful to
make sure to check out


00:10:43.200 --> 00:10:47.420
your system so that you have
enough computer resources,


00:10:47.420 --> 00:10:50.420
to hit the QPS requirement.


00:10:50.420 --> 00:10:52.800
Next is logging, when building


00:10:52.800 --> 00:10:55.440
your system it may
be useful to log as


00:10:55.440 --> 00:10:57.239
much of the data as possible


00:10:57.239 --> 00:10:59.345
for analysis and review as well


00:10:59.345 --> 00:11:01.165
as to provide more data


00:11:01.165 --> 00:11:03.920
for retraining your learning
algorithm in the future.


00:11:03.920 --> 00:11:07.005
Finally, security and privacy,


00:11:07.005 --> 00:11:09.120
I find it for
different applications


00:11:09.120 --> 00:11:10.380
the required levels of


00:11:10.380 --> 00:11:12.900
security and privacy
can be very different.


00:11:12.900 --> 00:11:14.990
For example, when I was working


00:11:14.990 --> 00:11:17.700
on electronic health
records, patient records,


00:11:17.700 --> 00:11:19.240
clearly the requirements for


00:11:19.240 --> 00:11:21.335
security and privacy
were very high


00:11:21.335 --> 00:11:23.630
because patient records are


00:11:23.630 --> 00:11:26.330
very highly sensitive
information.


00:11:26.330 --> 00:11:29.735
Depending on your application
you might want to design


00:11:29.735 --> 00:11:33.780
in the appropriate level
of security and privacy,


00:11:33.780 --> 00:11:37.430
based on how sensitive
that data is


00:11:37.430 --> 00:11:41.715
and also sometimes based on
regulatory requirements.


00:11:41.715 --> 00:11:44.810
If you save this
checklist somewhere,


00:11:44.810 --> 00:11:46.750
going through this
when you're designing


00:11:46.750 --> 00:11:49.290
your software might
help you to make


00:11:49.290 --> 00:11:51.549
the appropriate
software engine choices


00:11:51.549 --> 00:11:54.600
when implementing your
prediction service.


00:11:54.600 --> 00:11:57.880
To summarize, deploying a system


00:11:57.880 --> 00:12:01.330
requires two broad sets
of tasks: there is


00:12:01.330 --> 00:12:05.105
writing the software to


00:12:05.105 --> 00:12:07.895
enable you to deploy the
system in production.


00:12:07.895 --> 00:12:11.105
There is what you
need to do to monitor


00:12:11.105 --> 00:12:15.550
the system performance and
to continue to maintain it,


00:12:15.550 --> 00:12:20.140
especially in the
face of concepts


00:12:20.140 --> 00:12:26.625
drift as well as data drift.


00:12:26.625 --> 00:12:28.745
One of the things
you see when you're


00:12:28.745 --> 00:12:30.780
building machine
learning systems is that


00:12:30.780 --> 00:12:34.170
the practices for the
very first deployments


00:12:34.170 --> 00:12:36.725
will be quite different compared


00:12:36.725 --> 00:12:39.550
to when you are
updating or maintaining


00:12:39.550 --> 00:12:43.220
a system that has already
previously been deployed.


00:12:43.220 --> 00:12:45.900
I know that to some engineers
that view deploying


00:12:45.900 --> 00:12:49.190
the machine learning model as
getting to the finish line.


00:12:49.190 --> 00:12:52.425
Unfortunately, I think
the first deployment


00:12:52.425 --> 00:12:55.325
means you may be only
about halfway there,


00:12:55.325 --> 00:12:57.880
and the second half
of your work is just


00:12:57.880 --> 00:13:01.305
starting only after
your first deployment,


00:13:01.305 --> 00:13:05.310
because even after you've
deployed there's a lot of work


00:13:05.310 --> 00:13:09.839
to feed the data back and
maybe to update the model,


00:13:09.839 --> 00:13:13.115
to keep on maintaining the model


00:13:13.115 --> 00:13:16.550
even in the face of
changes to the data.


00:13:16.550 --> 00:13:19.060
One of the things we touch on
the later videos is some of


00:13:19.060 --> 00:13:21.745
the differences between
the first deployment,


00:13:21.745 --> 00:13:23.350
such as if your product


00:13:23.350 --> 00:13:25.280
never had the speech
recognition system.


00:13:25.280 --> 00:13:27.985
But you've trained the
speech recognition system


00:13:27.985 --> 00:13:30.255
and you're deploying
for the first time,


00:13:30.255 --> 00:13:32.225
versus you already have had


00:13:32.225 --> 00:13:34.140
the learning of running
for some time and


00:13:34.140 --> 00:13:38.140
you want to maintain or
update that implementation.


00:13:38.140 --> 00:13:39.660
To summarize, in this video,


00:13:39.660 --> 00:13:42.030
you saw some of the
machine learning or


00:13:42.030 --> 00:13:44.030
statistical related issues such


00:13:44.030 --> 00:13:45.610
as concept drift and data drift.


00:13:45.610 --> 00:13:47.549
As well as some of the software


00:13:47.549 --> 00:13:49.880
engineering-related
issues such as,


00:13:49.880 --> 00:13:53.630
whether you need a batch or
real-time prediction service,


00:13:53.630 --> 00:13:56.429
and whether the compute
and memory requirements


00:13:56.429 --> 00:13:58.375
you have to take into account.


00:13:58.375 --> 00:14:00.485
Now, it turns out that when


00:14:00.485 --> 00:14:02.700
you're deploying a
machine learning model,


00:14:02.700 --> 00:14:05.280
there are a number of
common design patterns,


00:14:05.280 --> 00:14:08.260
a common deployment
patterns that are used


00:14:08.260 --> 00:14:11.630
in many applications across
many different industries.


00:14:11.630 --> 00:14:13.255
In the next video,


00:14:13.255 --> 00:14:14.495
you'll see what are some of


00:14:14.495 --> 00:14:17.025
the most common
deployment patterns,


00:14:17.025 --> 00:14:18.660
so that you can
hopefully pick the


00:14:18.660 --> 00:14:20.665
right one for your application.


00:14:20.665 --> 00:14:23.370
Let's go on to the next video.
WEBVTT


00:00:00.340 --> 00:00:02.503
When you train the learning algorithm,


00:00:02.503 --> 00:00:06.063
the best way to deploy it is usually
not to just turn it on and hope for


00:00:06.063 --> 00:00:09.129
the best because, well,
what if something goes wrong?


00:00:09.129 --> 00:00:14.006
When deploying systems are a number of
common use cases or types of use cases as


00:00:14.006 --> 00:00:18.675
well as different patterns for
how you deploy depending on your use case.


00:00:18.675 --> 00:00:21.640
Let's go through that in this video.


00:00:21.640 --> 00:00:26.216
In the last video, I alluded to some
of the differences between a first


00:00:26.216 --> 00:00:30.040
deployment versus a maintenance and
update deployment.


00:00:30.040 --> 00:00:32.619
Let's flesh this out into
a little bit more detail.


00:00:32.619 --> 00:00:37.223
One type of deployment is if you
are offering a new product or


00:00:37.223 --> 00:00:41.104
capability that you had
not previously offered.


00:00:41.104 --> 00:00:45.913
For example, if you're offering a speech
recognition service that you have not


00:00:45.913 --> 00:00:50.438
offered before, in this case, a common
design pattern is to start up a small


00:00:50.438 --> 00:00:53.740
amount of traffic and
then gradually ramp it up.


00:00:53.740 --> 00:00:58.664
A second common deployment use case is if
there's something that's already being


00:00:58.664 --> 00:01:03.091
done by a person, but we would now like
to use a learning algorithm to either


00:01:03.091 --> 00:01:05.230
automate or assist with that task.


00:01:05.230 --> 00:01:10.491
For example, if you have people in the
factory inspecting smartphones scratches,


00:01:10.491 --> 00:01:14.713
but now you would like to use a learning
algorithm to either assist or


00:01:14.713 --> 00:01:16.540
automate that task.


00:01:16.540 --> 00:01:21.244
The fact that people were previously doing
this gives you a few more options for


00:01:21.244 --> 00:01:22.260
how you deploy.


00:01:22.260 --> 00:01:27.140
And you see shadow mode deployment
takes advantage of this.


00:01:27.140 --> 00:01:32.275
And finally, a third common deployment
case is if you've already been doing


00:01:32.275 --> 00:01:37.094
this task with a previous implementation
of a machine learning system,


00:01:37.094 --> 00:01:41.840
but you now want to replace it
with hopefully an even better one.


00:01:41.840 --> 00:01:47.064
In these cases,
two recurring themes you see are that


00:01:47.064 --> 00:01:51.960
you often want a gradual
ramp up with monitoring.


00:01:51.960 --> 00:01:56.675
In other words, rather than sending tons
of traffic to a maybe not fully proven


00:01:56.675 --> 00:02:00.890
learning algorithm, you may send it
only a small amount of traffic and


00:02:00.890 --> 00:02:04.770
monitor it and then ramp up
the percentage or amount of traffic.


00:02:04.770 --> 00:02:08.178
And the second idea you see
a few times is rollback.


00:02:08.178 --> 00:02:12.834
Meaning that if for some reason the
algorithm isn't working, it's nice if you


00:02:12.834 --> 00:02:17.364
can revert back to the previous system
if indeed there was an earlier system.


00:02:17.364 --> 00:02:22.346
Let's start with an example in visual
inspection where perhaps you've


00:02:22.346 --> 00:02:27.270
had human inspectors inspect
smartphones for defects for scratches.


00:02:27.270 --> 00:02:33.164
And you would now like to automate some
of this work with a learning algorithm.


00:02:33.164 --> 00:02:36.943
When you have people
initially doing a task,


00:02:36.943 --> 00:02:42.530
one common deployment pattern is
to use shadow modes deployment.


00:02:42.530 --> 00:02:47.644
And what that means is that you will start
by having a machine learning algorithm


00:02:47.644 --> 00:02:52.560
shadow the human inspector and
running parallel with the human inspector.


00:02:52.560 --> 00:02:57.008
During this initial phase, the learning
algorithms output is not used for


00:02:57.008 --> 00:02:58.790
any decision in the factory.


00:02:58.790 --> 00:03:04.140
So whatever the learning algorithm says, we're
going to go the human judgment for now.


00:03:04.140 --> 00:03:08.140
So let's say for this smartphone
the human says it's fine, no defect.


00:03:08.140 --> 00:03:09.810
The learning algorithm says it's fine.


00:03:09.810 --> 00:03:14.728
Maybe for this example of a big stretch
down the middle person says it's not


00:03:14.728 --> 00:03:17.159
okay and the learning algorithm agrees.


00:03:17.159 --> 00:03:21.692
And maybe for this example with a smaller
stretch, maybe the person says this is not


00:03:21.692 --> 00:03:26.440
okay, but the learning algorithm makes a
mistake and actually thinks this is okay.


00:03:26.440 --> 00:03:31.256
The purpose of a shadow mode deployment
is that allows you to gather data of how


00:03:31.256 --> 00:03:36.463
the learning algorithm is performing and
how that compares to the human judgment.


00:03:36.463 --> 00:03:41.128
And by something the it offers you can
then verify if the learning algorithm's


00:03:41.128 --> 00:03:45.228
predictions are accurate and
therefore use that to decide whether or


00:03:45.228 --> 00:03:50.840
not to maybe allow the learning algorithm
to make some real decisions in the future.


00:03:50.840 --> 00:03:55.884
So when you already have some system that
is making good decisions and that system


00:03:55.884 --> 00:04:01.013
can be human inspectors or even an older
implementation of a learning algorithm.


00:04:01.013 --> 00:04:04.951
Using a shadow mode deployment can
be a very effective way to let


00:04:04.951 --> 00:04:07.478
you verify the performance of a learning


00:04:07.478 --> 00:04:11.440
algorithm before letting them
make any real decisions.


00:04:11.440 --> 00:04:16.560
When you are ready to let a learning
algorithm start making real decisions,


00:04:16.560 --> 00:04:20.940
a common deployment pattern is
to use a canary deployment.


00:04:20.940 --> 00:04:25.275
So there's a phone, algorithm says it's
okay, rejects that, says that's okay,


00:04:25.275 --> 00:04:27.340
rejects that, rejects that.


00:04:27.340 --> 00:04:32.278
And in a canary deployments you would roll
out to a small fraction, maybe 5%, maybe


00:04:32.278 --> 00:04:37.091
even less of traffic initially and start
let the algorithm making real decisions.


00:04:37.091 --> 00:04:40.901
But by running this on only
a small percentage of the traffic,


00:04:40.901 --> 00:04:45.079
hopefully, if the algorithm makes
any mistakes it will affect only


00:04:45.079 --> 00:04:47.149
a small fraction of the traffic.


00:04:47.149 --> 00:04:51.518
And this gives you more of
an opportunity to monitor the system and


00:04:51.518 --> 00:04:55.491
ramp up the percentage of traffic
it gets only gradually and


00:04:55.491 --> 00:04:59.940
only when you have greater
confidence in this performance.


00:04:59.940 --> 00:05:04.420
The phrase canary deployment is
a reference to the English idiom or


00:05:04.420 --> 00:05:09.220
the English phrase canary in a coal mine,
which refers to how coal miners


00:05:09.220 --> 00:05:13.040
used to use canaries to
spot if there's a gas leak.


00:05:13.040 --> 00:05:17.584
But with canary the deployment,
hopefully this allows you to spot problems


00:05:17.584 --> 00:05:21.915
early on before there are maybe overly
large consequences to a factory or


00:05:21.915 --> 00:05:26.035
other context in which you're
deploying your learning algorithm.


00:05:26.035 --> 00:05:31.640
Another deployment pattern that is
sometimes used is a blue green deployment.


00:05:31.640 --> 00:05:33.340
Let me explain with the picture.


00:05:33.340 --> 00:05:36.353
Say you have a system,
a camera software for


00:05:36.353 --> 00:05:39.560
collecting phone pictures in your factory.


00:05:39.560 --> 00:05:46.035
These phone images are sent to a piece
of software that takes these images and


00:05:46.035 --> 00:05:50.310
routes them into some
visual inspection system.


00:05:50.310 --> 00:05:53.387
In the terminology of
a blue green deployments,


00:05:53.387 --> 00:05:58.412
the old version of your software is called
the blue version and the new version,


00:05:58.412 --> 00:06:03.540
the Learning algorithm you just
implemented is called the green version.


00:06:03.540 --> 00:06:10.116
In a blue green deployment, what you do is
have the router send images to the old or


00:06:10.116 --> 00:06:13.999
the blue version and
have that make decisions.


00:06:13.999 --> 00:06:17.818
And then when you want to
switch over to the new version,


00:06:17.818 --> 00:06:22.695
what you would do is have the router
stop sending images to the old one and


00:06:22.695 --> 00:06:25.940
suddenly switch over to the new version.


00:06:25.940 --> 00:06:30.617
So the way the blue green deployment
is implemented is you would have an old


00:06:30.617 --> 00:06:34.410
prediction service may be
running on some sort of service.


00:06:34.410 --> 00:06:38.440
You will then spin up
a new prediction service,


00:06:38.440 --> 00:06:43.378
the green version, and
you would have the router suddenly


00:06:43.378 --> 00:06:48.041
switch the traffic over from
the old one to the new one.


00:06:48.041 --> 00:06:52.678
The advantage of a blue green deployment
is that there's an easy way to


00:06:52.678 --> 00:06:53.944
enable rollback.


00:06:53.944 --> 00:06:58.551
If something goes wrong, you can just
very quickly have the router go back


00:06:58.551 --> 00:07:03.381
reconfigure their router to send traffic
back to the old or the blue version,


00:07:03.381 --> 00:07:08.238
assuming that you kept your blue version
of the prediction service running.


00:07:08.238 --> 00:07:11.341
In a typical implementation
of a blue green deployment,


00:07:11.341 --> 00:07:15.370
people think of switching over
the traffic 100% all at the same time.


00:07:15.370 --> 00:07:20.032
But of course you can also use a more
gradual version where you slowly send


00:07:20.032 --> 00:07:21.040
traffic over.


00:07:21.040 --> 00:07:25.136
As you can imagine, whether use shadow
mode, canary mode, blue green, or


00:07:25.136 --> 00:07:29.640
some of the deployment pattern, quite a
lot of software is needed to execute this.


00:07:29.640 --> 00:07:34.027
MLOps tools can help with implementing
these deployment patterns or


00:07:34.027 --> 00:07:36.020
you can implement it yourself.


00:07:36.020 --> 00:07:39.389
One of the most useful
frameworks I have found for


00:07:39.389 --> 00:07:44.729
thinking about how to deploy a system is
to think about deployment not as a 0,


00:07:44.729 --> 00:07:49.164
1 is either deploy or not deploy,
but instead to design a system


00:07:49.164 --> 00:07:53.840
thinking about what is
the appropriate degree of automation.


00:07:53.840 --> 00:07:57.026
For example,
in visual inspection of smartphones,


00:07:57.026 --> 00:08:01.505
one extreme would be if there's no
automation, so the human only system.


00:08:01.505 --> 00:08:06.100
Slightly mode automated would be if
your system is running a shadow mode.


00:08:06.100 --> 00:08:08.756
So your learning algorithms
are putting predictions, but


00:08:08.756 --> 00:08:10.580
it's not actually used in the factory.


00:08:10.580 --> 00:08:12.540
So that would be shadow mode.


00:08:12.540 --> 00:08:17.160
A slightly greater degree of automation
would be AI assistance in which


00:08:17.160 --> 00:08:19.932
given a picture like this of a smartphone,


00:08:19.932 --> 00:08:23.172
you may have a human
inspector make the decision.


00:08:23.172 --> 00:08:27.588
But maybe an AI system can affect
the user interface to highlight


00:08:27.588 --> 00:08:31.923
the regions where there's a scratch
to help draw the person's


00:08:31.923 --> 00:08:35.890
attention to where it may be
most useful for them to look.


00:08:35.890 --> 00:08:40.241
The user interface or UI design
is critical for human assistance.


00:08:40.241 --> 00:08:44.559
But this could be a way to get
a slightly greater degree of automation


00:08:44.559 --> 00:08:47.303
while still keeping the human in the loop.


00:08:47.303 --> 00:08:51.432
And even greater degree of automation
maybe partial automation, where given


00:08:51.432 --> 00:08:56.210
a smartphone, if the learning algorithm is
sure it's fine, then that's its decision.


00:08:56.210 --> 00:09:00.640
It is sure it's defective,
then we just go to algorithm's decision.


00:09:00.640 --> 00:09:04.913
But if the learning algorithm is not sure,
in other words,


00:09:04.913 --> 00:09:09.615
if the learning algorithm prediction
is not too confident, 0 or


00:09:09.615 --> 00:09:12.965
1, maybe only then do we
send this to a human.


00:09:12.965 --> 00:09:15.591
So this would be partial automation.


00:09:15.591 --> 00:09:18.425
Where if the learning algorithm
is confident of its prediction,


00:09:18.425 --> 00:09:19.780
we go the learning algorithm.


00:09:19.780 --> 00:09:23.910
But for the hopefully small subset
of images where the algorithm is


00:09:23.910 --> 00:09:27.495
not sure we send that to
a human to get their judgment.


00:09:27.495 --> 00:09:32.328
And the human judgment can also be very
valuable data to feedback to further


00:09:32.328 --> 00:09:34.840
train and improve the algorithm.


00:09:34.840 --> 00:09:39.880
I find that this partial automation is
sometimes a very good design point for


00:09:39.880 --> 00:09:45.238
applications where the learning algorithms
performance isn't good enough for


00:09:45.238 --> 00:09:46.520
full automation.


00:09:46.520 --> 00:09:51.385
And then of course beyond partial
automation, there is full automation


00:09:51.385 --> 00:09:56.540
where we might have the learning
algorithm make every single decision.


00:09:56.540 --> 00:10:02.327
So there is a spectrum of using
only human decisions on the left,


00:10:02.327 --> 00:10:08.240
all the way to using only the AI
system's decisions on the right.


00:10:08.240 --> 00:10:13.464
And many deployment applications
will start from the left and


00:10:13.464 --> 00:10:15.990
gradually move to the right.


00:10:15.990 --> 00:10:19.840
And you do not have to get all
the way to full automation.


00:10:19.840 --> 00:10:23.918
You could choose to stop using AI
assistance or partial automation or


00:10:23.918 --> 00:10:27.081
you could choose to go to
full automation depending on


00:10:27.081 --> 00:10:30.967
the performance of your system and
the needs of the application.


00:10:30.967 --> 00:10:35.432
On this spectrum both AI assistance and


00:10:35.432 --> 00:10:43.840
partial automation are examples
of human in the loop deployments.


00:10:43.840 --> 00:10:46.469
I find that the consumer
Internet applications


00:10:46.469 --> 00:10:50.950
such as if you run a web search engine,
write online speech recognition system.


00:10:50.950 --> 00:10:55.656
A lot of consumer software Internet
businesses have to use full automation


00:10:55.656 --> 00:11:00.585
because it's just not feasible to someone
on the back end doing some work every


00:11:00.585 --> 00:11:04.540
time someone does a web search or
does the product search.


00:11:04.540 --> 00:11:08.962
But outside consumer software Internet,
for example, inspecting things and


00:11:08.962 --> 00:11:09.700
factories.


00:11:09.700 --> 00:11:14.603
They're actually many applications where
the best design point maybe a human


00:11:14.603 --> 00:11:19.340
in the loop deployments rather
than a full automation deployment.


00:11:19.340 --> 00:11:22.383
In this video,
you saw a few patterns of deployments,


00:11:22.383 --> 00:11:26.910
such as a shadow mode deployment,
canary deployment, blue green deployment.


00:11:26.910 --> 00:11:30.765
And you also saw how you can pick
the most appropriate degree of


00:11:30.765 --> 00:11:33.586
automation depending on your application,


00:11:33.586 --> 00:11:38.040
which could be a human in the loop
deployments or full automation.


00:11:38.040 --> 00:11:43.007
As we went through these ideas, you heard
me mention a few times the importance of


00:11:43.007 --> 00:11:47.270
monitoring to help you spot problems
if any so they can address them.


00:11:47.270 --> 00:11:51.761
Let's dive into the details of how to
monitor the system in the next video.
WEBVTT


00:00:00.000 --> 00:00:03.510
How can you monitor a
machine learning system to


00:00:03.510 --> 00:00:05.220
make sure that it is meeting


00:00:05.220 --> 00:00:07.170
your performance expectations?


00:00:07.170 --> 00:00:10.245
In this video, you'll
learn about best practices


00:00:10.245 --> 00:00:13.620
for monitoring deployed
machine learning systems.


00:00:13.620 --> 00:00:15.180
The most common way to


00:00:15.180 --> 00:00:17.280
monitor a machine
learning system is


00:00:17.280 --> 00:00:21.835
to use a dashboard to track
how it is doing over time.


00:00:21.835 --> 00:00:23.360
Depending on your application,


00:00:23.360 --> 00:00:26.045
your dashboards may
monitor different metrics.


00:00:26.045 --> 00:00:27.620
For example, you may have


00:00:27.620 --> 00:00:30.740
one dashboard to monitor
the server load,


00:00:30.740 --> 00:00:33.424
or a different
dashboards to monitor


00:00:33.424 --> 00:00:36.215
diffraction of non-null outputs.


00:00:36.215 --> 00:00:38.570
Sometimes a speech
recognition system output


00:00:38.570 --> 00:00:41.255
is null when the things that
users didn't say anything.


00:00:41.255 --> 00:00:44.315
If this changes
dramatically over time,


00:00:44.315 --> 00:00:47.090
it may be an indication
that something is wrong,


00:00:47.090 --> 00:00:50.405
or one common one I've
seen for a lot of


00:00:50.405 --> 00:00:52.565
structured data task is


00:00:52.565 --> 00:00:55.820
monitoring the fraction
of missing input values.


00:00:55.820 --> 00:00:57.530
If that changes, it may mean that


00:00:57.530 --> 00:01:00.055
something has changed
about your data.


00:01:00.055 --> 00:01:03.290
When you're trying to
decide what to monitor,


00:01:03.290 --> 00:01:06.230
my recommendation is
that you sit down with


00:01:06.230 --> 00:01:08.134
your team and brainstorm


00:01:08.134 --> 00:01:10.190
all the things that
could possibly go wrong.


00:01:10.190 --> 00:01:13.955
Then you want to know about
if something does go wrong.


00:01:13.955 --> 00:01:16.505
For all the things
that could go wrong,


00:01:16.505 --> 00:01:18.830
brainstorm a few statistics or


00:01:18.830 --> 00:01:22.580
a few metrics that will
detect that problem.


00:01:22.580 --> 00:01:24.440
For example, if you're worried


00:01:24.440 --> 00:01:27.049
about user traffic spiking,


00:01:27.049 --> 00:01:29.870
causing the service
to become overloaded,


00:01:29.870 --> 00:01:33.095
then server loads
maybe one metric,


00:01:33.095 --> 00:01:37.315
you could track and so on
for the other examples here.


00:01:37.315 --> 00:01:39.290
When I'm designing


00:01:39.290 --> 00:01:41.900
my monitoring dashboards
for the first time,


00:01:41.900 --> 00:01:44.870
I think it's okay to start


00:01:44.870 --> 00:01:47.840
off with a lot of different
metrics and monitor


00:01:47.840 --> 00:01:49.430
a relatively large set


00:01:49.430 --> 00:01:51.560
and then gradually
remove the ones that


00:01:51.560 --> 00:01:55.310
you find over time not to
be particularly useful.


00:01:55.310 --> 00:01:58.070
Here are some examples
of metrics our views


00:01:58.070 --> 00:02:01.190
or I've seen others use
on a variety of projects.


00:02:01.190 --> 00:02:03.575
First are the software metrics,


00:02:03.575 --> 00:02:05.480
such as memory, compute,


00:02:05.480 --> 00:02:07.715
latency, throughput, server load,


00:02:07.715 --> 00:02:10.010
things that help you
monitor the health


00:02:10.010 --> 00:02:12.890
of your software
implementation of


00:02:12.890 --> 00:02:14.600
the prediction service or


00:02:14.600 --> 00:02:18.765
other pieces of software around
your learning algorithm.


00:02:18.765 --> 00:02:21.395
But these software metrics
will help you make sure


00:02:21.395 --> 00:02:24.080
that your software
is running well.


00:02:24.080 --> 00:02:26.300
Many MLOps tools will
come over the bouts


00:02:26.300 --> 00:02:29.150
already tracking these
software metrics.


00:02:29.150 --> 00:02:31.384
In addition to the
software metrics,


00:02:31.384 --> 00:02:35.030
I would often choose
other metrics that help


00:02:35.030 --> 00:02:38.045
monitor the statistical health


00:02:38.045 --> 00:02:41.030
or the performance of
the learning algorithm.


00:02:41.030 --> 00:02:43.760
Broadly, there are two types of


00:02:43.760 --> 00:02:46.520
metrics you might
brainstorm around.


00:02:46.520 --> 00:02:48.530
One is input metrics,


00:02:48.530 --> 00:02:50.870
which are metrics
that measure has


00:02:50.870 --> 00:02:53.465
your input distribution x change.


00:02:53.465 --> 00:02:55.700
For example, if you are


00:02:55.700 --> 00:02:58.115
building a speech
recognition system,


00:02:58.115 --> 00:03:01.160
you might monitor the
average input length


00:03:01.160 --> 00:03:02.720
in seconds of the length for


00:03:02.720 --> 00:03:04.670
the audio clip fed to your system.


00:03:04.670 --> 00:03:08.080
You might monitor the
average input volume.


00:03:08.080 --> 00:03:11.179
If these change for some reason,


00:03:11.179 --> 00:03:14.000
that might be something
you'll once to take a look


00:03:14.000 --> 00:03:15.650
at just to make sure this


00:03:15.650 --> 00:03:17.960
hasn't hurt the performance
of your algorithm.


00:03:17.960 --> 00:03:19.460
I mentioned just now,


00:03:19.460 --> 00:03:20.900
number or percentage of


00:03:20.900 --> 00:03:23.840
missing values is a
very common metric.


00:03:23.840 --> 00:03:25.610
When using structured data,


00:03:25.610 --> 00:03:28.150
some of which may
have missing values,


00:03:28.150 --> 00:03:31.235
or for the manufacturing
visual inspection example,


00:03:31.235 --> 00:03:34.205
you might monitor
average image brightness


00:03:34.205 --> 00:03:36.650
if you think that lighting
conditions could change,


00:03:36.650 --> 00:03:38.960
and you want to make sure
you know if it does,


00:03:38.960 --> 00:03:42.110
so you can brainstorm
different metrics to see if


00:03:42.110 --> 00:03:46.055
your input distribution
x might have changed.


00:03:46.055 --> 00:03:49.580
A second set of metrics
that help you understand


00:03:49.580 --> 00:03:51.050
if your learning
algorithm is performing


00:03:51.050 --> 00:03:53.435
well are output metrics.


00:03:53.435 --> 00:03:56.180
Such as, how often does


00:03:56.180 --> 00:03:59.000
your speech recognition
system return null,


00:03:59.000 --> 00:04:00.170
the empty string, because


00:04:00.170 --> 00:04:02.030
the things the user
doesn't say anything,


00:04:02.030 --> 00:04:04.490
or if you have built


00:04:04.490 --> 00:04:08.660
a speech recognition system
for web search using voice,


00:04:08.660 --> 00:04:12.755
you might decide to see
how often does the user


00:04:12.755 --> 00:04:14.780
do two very quick searches in


00:04:14.780 --> 00:04:17.015
a row with substantially
the same input.


00:04:17.015 --> 00:04:18.635
That might be a sign that you


00:04:18.635 --> 00:04:21.860
misrecognize their query
the first time round.


00:04:21.860 --> 00:04:23.900
It's an imperfect signal but you


00:04:23.900 --> 00:04:26.870
could try this metric
and see if it helps.


00:04:26.870 --> 00:04:28.610
Or you could monitor
the number of


00:04:28.610 --> 00:04:30.620
times the user first try to


00:04:30.620 --> 00:04:34.820
use the speech system and
then switches over to typing,


00:04:34.820 --> 00:04:36.110
that could be a sign that


00:04:36.110 --> 00:04:39.320
the user got frustrated
or gave up on


00:04:39.320 --> 00:04:41.210
your speech system and


00:04:41.210 --> 00:04:43.890
could indicate
degrading performance.


00:04:43.890 --> 00:04:45.530
Of course, for web search,


00:04:45.530 --> 00:04:47.450
you would also use maybe very


00:04:47.450 --> 00:04:50.315
course metrics like
click-through rate or CTR,


00:04:50.315 --> 00:04:54.040
just to make sure that the
overall system is healthy.


00:04:54.040 --> 00:04:57.050
These output metrics
can help you figure


00:04:57.050 --> 00:04:59.725
out if either your
learning algorithm,


00:04:59.725 --> 00:05:02.775
output y has changed in some way,


00:05:02.775 --> 00:05:04.775
or if something that comes


00:05:04.775 --> 00:05:06.815
even after your learning
algorithms output,


00:05:06.815 --> 00:05:08.540
such as the user's
switching over to


00:05:08.540 --> 00:05:12.140
typing has changed in
some significant way.


00:05:12.140 --> 00:05:13.640
Because input and output metrics


00:05:13.640 --> 00:05:15.095
are application specific,


00:05:15.095 --> 00:05:18.185
most MLOps tools will need to
be configured specifically


00:05:18.185 --> 00:05:19.310
to track the input and


00:05:19.310 --> 00:05:21.425
output metrics for
your application.


00:05:21.425 --> 00:05:25.010
You may already know that
machine learning modeling is


00:05:25.010 --> 00:05:28.490
a highly iterative
process, so as deployment.


00:05:28.490 --> 00:05:30.725
Take modeling, you would


00:05:30.725 --> 00:05:33.699
come up with a machine
learning model and some data,


00:05:33.699 --> 00:05:36.805
train the model,
that's an experiment.


00:05:36.805 --> 00:05:39.650
Then do error analysis and use


00:05:39.650 --> 00:05:42.230
the error analysis to go
back to figure out how


00:05:42.230 --> 00:05:43.700
to improve the model or


00:05:43.700 --> 00:05:46.760
your data and is by
iterating through


00:05:46.760 --> 00:05:49.160
this loop multiple
times that you then


00:05:49.160 --> 00:05:52.535
hopefully gets a good model.


00:05:52.535 --> 00:05:54.845
I encourage you to think of


00:05:54.845 --> 00:05:58.415
deployments as an
iterative process as well.


00:05:58.415 --> 00:06:01.700
When you get your first
deployments up and


00:06:01.700 --> 00:06:06.020
running and put in place a
set of monitoring dashboards.


00:06:06.020 --> 00:06:09.500
But that's only the start
of this iterative process.


00:06:09.500 --> 00:06:11.795
A running system
allows you to get


00:06:11.795 --> 00:06:15.155
real user data or real traffic.


00:06:15.155 --> 00:06:18.230
It is by seeing how
your learning algorithm


00:06:18.230 --> 00:06:21.530
performs on real data
on real traffic that,


00:06:21.530 --> 00:06:25.460
that allows you to do
performance analysis,


00:06:25.460 --> 00:06:28.745
and this in turn
helps you to update


00:06:28.745 --> 00:06:32.375
your deployment and to keep
on monitoring your system.


00:06:32.375 --> 00:06:35.225
In my experience,
it usually takes


00:06:35.225 --> 00:06:37.565
a few tries to converge


00:06:37.565 --> 00:06:40.205
to the right set of
metrics to monitor.


00:06:40.205 --> 00:06:43.100
Sometimes have deploy the
machine learning system,


00:06:43.100 --> 00:06:45.620
and it's not uncommon
for you to deploy


00:06:45.620 --> 00:06:48.710
machine learning system with
an initial set of metrics


00:06:48.710 --> 00:06:51.020
only to run the system for


00:06:51.020 --> 00:06:53.510
a few weeks and then
to realize that


00:06:53.510 --> 00:06:54.650
something could go wrong with


00:06:54.650 --> 00:06:55.670
it that you hadn't thought of


00:06:55.670 --> 00:06:58.925
before and into pick a
new metric to monitor.


00:06:58.925 --> 00:07:01.670
Or for you to have some
metric that you monitor


00:07:01.670 --> 00:07:04.415
for a few weeks and then
decide they're just metric,


00:07:04.415 --> 00:07:06.860
hardly ever changes
in does is inducible,


00:07:06.860 --> 00:07:08.870
and to get rid of that metric in


00:07:08.870 --> 00:07:11.300
favor of focusing attention
on something else.


00:07:11.300 --> 00:07:14.660
After you've chosen a set
of metrics to monitor,


00:07:14.660 --> 00:07:18.590
common practice would be to
set thresholds for alarms.


00:07:18.590 --> 00:07:20.690
You may decide based on this set,


00:07:20.690 --> 00:07:26.370
if the server load
ever goes above 0.91,


00:07:27.010 --> 00:07:29.690
that may trigger an alarm or


00:07:29.690 --> 00:07:32.120
a notification to
let you know or let


00:07:32.120 --> 00:07:34.370
the team know to see if
there's a problem and


00:07:34.370 --> 00:07:36.875
maybe spin up some more servers.


00:07:36.875 --> 00:07:40.400
Or if the fashion of
non-null plus goals


00:07:40.400 --> 00:07:43.565
above or beyond
certain thresholds


00:07:43.565 --> 00:07:45.380
that might trigger an alarm.


00:07:45.380 --> 00:07:48.395
Or if they're not, fraction
of missing values goes


00:07:48.395 --> 00:07:51.665
above or below some
set of thresholds,


00:07:51.665 --> 00:07:54.410
maybe that should
trigger an alarm,


00:07:54.410 --> 00:07:56.960
and it is okay if you adapt


00:07:56.960 --> 00:07:59.960
the metrics and the
thresholds over time to


00:07:59.960 --> 00:08:02.285
make sure that they
are flagging to you


00:08:02.285 --> 00:08:05.690
the most relevant
cases of concern.


00:08:05.690 --> 00:08:09.080
If something goes wrong with
your learning algorithm,


00:08:09.080 --> 00:08:13.820
if is a software issue such
as server load is too high,


00:08:13.820 --> 00:08:15.755
then that may require


00:08:15.755 --> 00:08:18.095
changing the software
implementation,


00:08:18.095 --> 00:08:20.990
or if it is a performance problem


00:08:20.990 --> 00:08:24.155
associated with the accuracy
of the learning algorithm,


00:08:24.155 --> 00:08:28.115
then you may need to
update your model.


00:08:28.115 --> 00:08:30.080
Or if it is an issue associated


00:08:30.080 --> 00:08:32.105
with the accuracy of
the learning algorithm,


00:08:32.105 --> 00:08:35.375
then you may need to go
back to fix that that's why


00:08:35.375 --> 00:08:38.210
many machine learning models
will need a little bit


00:08:38.210 --> 00:08:41.180
of maintenance or
retraining over time.


00:08:41.180 --> 00:08:43.415
Just like almost all software


00:08:43.415 --> 00:08:46.070
needs some level of
maintenance as well.


00:08:46.070 --> 00:08:48.200
When a model needs to be updated,


00:08:48.200 --> 00:08:52.475
you can either retrain it
manually, where in Engineer,


00:08:52.475 --> 00:08:53.840
maybe you will retrain


00:08:53.840 --> 00:08:55.910
the model perform
error analysis and


00:08:55.910 --> 00:08:57.320
the new model and
make sure it looks


00:08:57.320 --> 00:08:59.600
okay before you push
that to deployment.


00:08:59.600 --> 00:09:01.310
Or you could also put in place


00:09:01.310 --> 00:09:04.430
a system where there is
automatic retraining.


00:09:04.430 --> 00:09:07.220
Today, manual retraining is far


00:09:07.220 --> 00:09:09.890
more common than
automatically training for


00:09:09.890 --> 00:09:14.390
many applications developers
are reluctant to learning


00:09:14.390 --> 00:09:17.390
algorithm be fully
automatic in terms of


00:09:17.390 --> 00:09:20.630
deciding to retrain and pushing
new model to production,


00:09:20.630 --> 00:09:23.345
but there are some applications,


00:09:23.345 --> 00:09:25.490
especially in consumer
software Internet,


00:09:25.490 --> 00:09:29.150
where automatically
training does happen.


00:09:29.150 --> 00:09:34.430
We'll talk more about retraining
and how to vet or verify


00:09:34.430 --> 00:09:36.830
a model's performance
before pushing


00:09:36.830 --> 00:09:40.670
a new model out to production
in next week's videos.


00:09:40.670 --> 00:09:43.970
But the key takeaways are that it


00:09:43.970 --> 00:09:47.135
is only by monitoring
the system that


00:09:47.135 --> 00:09:50.750
you can spot if there may
be a problem that may


00:09:50.750 --> 00:09:55.250
cause you to go back to perform
a deeper error analysis,


00:09:55.250 --> 00:10:01.910
or that may cause you to
go back to get more data


00:10:01.910 --> 00:10:05.720
with which you can
update your model so as


00:10:05.720 --> 00:10:09.875
to maintain or improve
your system's performance.


00:10:09.875 --> 00:10:13.220
You learn more about
how to update models


00:10:13.220 --> 00:10:16.490
in the next two weeks
Materials as well.


00:10:16.490 --> 00:10:18.380
In this video,
you'll learn how to


00:10:18.380 --> 00:10:20.810
monitor the performance of
the machine learning system,


00:10:20.810 --> 00:10:22.265
so that in case something


00:10:22.265 --> 00:10:23.795
needs to be maintained or fixed,


00:10:23.795 --> 00:10:25.130
you can be alerted so they


00:10:25.130 --> 00:10:26.705
can take the appropriate action.


00:10:26.705 --> 00:10:28.670
We've talked about how to monitor


00:10:28.670 --> 00:10:31.490
the performance of a single
machine learning model.


00:10:31.490 --> 00:10:33.455
One of the most useful concepts


00:10:33.455 --> 00:10:35.780
is for more complex systems,


00:10:35.780 --> 00:10:37.940
where you don't
have just one model


00:10:37.940 --> 00:10:40.865
with a more complex
machine learning pipeline,


00:10:40.865 --> 00:10:43.205
how do you monitor the
performance of that?


00:10:43.205 --> 00:10:46.170
You'll learn about that
in the next video.
WEBVTT


00:00:00.340 --> 00:00:04.249
Many AI systems are not just a single
machine learning model running


00:00:04.249 --> 00:00:08.660
a prediction service, but instead
involves a pipeline of multiple steps.


00:00:08.660 --> 00:00:11.040
So what a machine learning pipelines and


00:00:11.040 --> 00:00:13.780
how do you build monitoring systems for
that?


00:00:13.780 --> 00:00:16.440
Let's learn about that in this video.


00:00:16.440 --> 00:00:21.263
Let's continue with our speech
recognition example, you've seen how


00:00:21.263 --> 00:00:26.580
a speech recognition system may take as
input audio and I'll put a transcript.


00:00:26.580 --> 00:00:31.472
The way that speech recognition is
typically implemented on mobile apps


00:00:31.472 --> 00:00:36.053
is not like this, but instead is
a slightly more complex pipeline.


00:00:36.053 --> 00:00:42.371
Where the audio is fed to
a module called a VAD or


00:00:42.371 --> 00:00:50.395
a voice activity detection module,


00:00:50.395 --> 00:00:53.403
whose job it is to see
if anyone is speaking.


00:00:53.403 --> 00:00:58.340
And only if the VAD module,
the voice activity detection module thinks


00:00:58.340 --> 00:01:03.273
someone is speaking, does it then
bother to pass the audio on to a speech


00:01:03.273 --> 00:01:08.140
recognition system whose job it is
to then generate the transcript.


00:01:08.140 --> 00:01:13.799
And the reason we use a voice activity
detection or VAD module is because if say,


00:01:13.799 --> 00:01:17.444
your speech recognition
system runs in the cloud.


00:01:17.444 --> 00:01:22.020
You don't want to stream more bandwidth
than you have to to your cloud server.


00:01:22.020 --> 00:01:26.908
And so the voice activity detection
module looks at the long stream


00:01:26.908 --> 00:01:29.698
of audio on your cell phone and clips or


00:01:29.698 --> 00:01:34.323
shortens the audio to just the part
where someone is talking and


00:01:34.323 --> 00:01:39.770
streams only that to the cloud server
to perform the speech recognition.


00:01:39.770 --> 00:01:45.936
So this is an example of a machine
learning pipeline where there is one step


00:01:45.936 --> 00:01:52.420
usually done by learning algorithm as well
to decide if someone is talking or not.


00:01:52.420 --> 00:01:54.291
And then the second step,


00:01:54.291 --> 00:01:59.540
also done by a learning algorithm
to generate the text transcript.


00:01:59.540 --> 00:02:03.731
When you have two learning algorithms,
one doesn't learn to detect


00:02:03.731 --> 00:02:07.499
someone's talking and
one does learn to transcribe speech.


00:02:07.499 --> 00:02:11.270
When you have two such
modules working together,


00:02:11.270 --> 00:02:17.670
changes to the first module may affect the
performance of the second module as well.


00:02:17.670 --> 00:02:23.510
For example, let's say that because of the
way a new cell phone's microphone works.


00:02:23.510 --> 00:02:27.740
The VAD module ends up clipping
the audio differently.


00:02:27.740 --> 00:02:30.200
Maybe it leaves more silence
at the start or end or


00:02:30.200 --> 00:02:31.890
less silence at the start or end.


00:02:31.890 --> 00:02:34.776
And does if the VAD's output changes,


00:02:34.776 --> 00:02:39.600
that will cause the speech
recognition systems input to change.


00:02:39.600 --> 00:02:44.400
And that could cause degraded performance
of the speech recognition system.


00:02:44.400 --> 00:02:47.640
Let's look an example
involving user profiles.


00:02:47.640 --> 00:02:51.385
Maybe I've used the data such as
clickstream data showing what users


00:02:51.385 --> 00:02:52.640
are clicking on.


00:02:52.640 --> 00:02:57.335
And this can be used to build a user
profile that tries to capture


00:02:57.335 --> 00:03:01.080
key attributes or
key characteristics of a user.


00:03:01.080 --> 00:03:05.530
For example, I once built user
profiles that would try to expect many


00:03:05.530 --> 00:03:10.301
attributes of users including whether or
not the user seemed to own a car.


00:03:10.301 --> 00:03:13.019
Because this would help us decide if


00:03:13.019 --> 00:03:17.840
it was worth trying to offer car
insurance office to that user.


00:03:17.840 --> 00:03:22.680
And so whether the user owns
a car could be yes or no or


00:03:22.680 --> 00:03:27.990
unknown, or
maybe other final graduations and these.


00:03:27.990 --> 00:03:32.483
And the typical way that the user profile
is built is with a learning algorithm to


00:03:32.483 --> 00:03:34.640
try to predict if this user of the car.


00:03:34.640 --> 00:03:39.698
This type of user profile, which can have
a very long list of predicted attributes,


00:03:39.698 --> 00:03:42.000
can then be fed to recommend a system.


00:03:42.000 --> 00:03:44.558
Another learning algorithm that then takes


00:03:44.558 --> 00:03:49.640
this understanding of the user to try
to generate product recommendations.


00:03:49.640 --> 00:03:54.360
Now, if something about
the click stream data changes,


00:03:54.360 --> 00:03:59.081
maybe this input distribution changes,
then maybe over


00:03:59.081 --> 00:04:03.993
time if we lose our ability to
figure out if a user owns a car,


00:04:03.993 --> 00:04:08.446
then the percentage of
the unknown tag here may go up.


00:04:08.446 --> 00:04:12.272
And because the user
profiles output changes,


00:04:12.272 --> 00:04:16.386
the input to the recommended
system now changes and


00:04:16.386 --> 00:04:21.940
this might affect the quality
of the product recommendations.


00:04:21.940 --> 00:04:25.330
When you have a machine
learning pipelines,


00:04:25.330 --> 00:04:30.790
these cascading effects in the pipeline
can be complex to keep track on.


00:04:30.790 --> 00:04:36.170
But if the percentage of unknown labels
does go up, this could be something that


00:04:36.170 --> 00:04:41.550
you want to be alerted to so that you can
update the recommend the system if needed


00:04:41.550 --> 00:04:46.890
to make sure you continue to generate
high quality product recommendations.


00:04:46.890 --> 00:04:50.509
So when building these complex
machine learning pipelines,


00:04:50.509 --> 00:04:53.571
which can have machine
learning based components or


00:04:53.571 --> 00:04:57.480
non-machine learning based
components throughout the pipeline.


00:04:57.480 --> 00:05:02.510
I find it useful to brainstorm metrics
to monitor that can detect changes


00:05:02.510 --> 00:05:08.550
including concept drift or data driven or
both, and multiple stages of the pipeline.


00:05:08.550 --> 00:05:13.288
So, metrics to monitor include
software metrics for perhaps each of


00:05:13.288 --> 00:05:19.140
the components in the pipeline, or perhaps
for the overall pipeline as a whole.


00:05:19.140 --> 00:05:22.367
As well as input metrics and
potentially output metrics for


00:05:22.367 --> 00:05:24.580
each of the components of the pipeline.


00:05:24.580 --> 00:05:28.837
And by brainstorming metrics associated
with individual components of


00:05:28.837 --> 00:05:30.180
the pipeline as well.


00:05:30.180 --> 00:05:35.754
This could help you spot problems such
as the voice activity detection system


00:05:35.754 --> 00:05:41.327
of putting longer or shorter audio clears
over time or the user profile system


00:05:41.327 --> 00:05:46.660
suddenly having more unknown attributes
for whether the user owns a car.


00:05:46.660 --> 00:05:50.702
And thereby alert you to
changes in the data that may


00:05:50.702 --> 00:05:54.660
require you to take action
to maintain the model.


00:05:54.660 --> 00:05:59.493
But the principle that you saw in the last
video of brainstorm all the things


00:05:59.493 --> 00:06:04.328
that could go wrong, including things
that could go wrong with individual


00:06:04.328 --> 00:06:08.420
components of the pipeline and
design metrics to track those.


00:06:08.420 --> 00:06:12.772
That principle still applies only now
you're looking at multiple components in


00:06:12.772 --> 00:06:13.620
the pipeline.


00:06:13.620 --> 00:06:17.210
Finally, how quickly does data change?


00:06:17.210 --> 00:06:22.140
The rate at which data changes
is very problem dependent.


00:06:22.140 --> 00:06:25.440
For example, let's see it built
to face recognition system.


00:06:25.440 --> 00:06:30.059
Then the rate at which people's
appearances changes usually isn't


00:06:30.059 --> 00:06:30.861
that fast.


00:06:30.861 --> 00:06:34.930
People's hairstyles and
clothing does change with fashion changes.


00:06:34.930 --> 00:06:38.491
And as cameras get better,
we've been getting higher and


00:06:38.491 --> 00:06:41.940
higher resolution pictures
of people over time.


00:06:41.940 --> 00:06:46.440
But for the most part, people's
appearances don't change that much.


00:06:46.440 --> 00:06:50.095
But there are sometimes things that
can change very quickly as well,


00:06:50.095 --> 00:06:54.253
such as if a factory gets a new batch of
material for how they make cell phones and


00:06:54.253 --> 00:06:56.990
so all the cell phones not
to change in appearance.


00:06:56.990 --> 00:07:02.032
So some applications will have data that
changes over the time scale of months or


00:07:02.032 --> 00:07:02.871
even years.


00:07:02.871 --> 00:07:07.798
And some applications with data that could
suddenly change in a matter of minutes.


00:07:07.798 --> 00:07:12.095
Speaking in very broad generalities,
I find that on average,


00:07:12.095 --> 00:07:15.520
user data generally
changes relatively slowly.


00:07:15.520 --> 00:07:20.583
If you run a consumer facing business
with a very large numbers of users,


00:07:20.583 --> 00:07:22.243
then it is quite rare for


00:07:22.243 --> 00:07:27.740
millions of users to all suddenly change
their behavior all at the same time.


00:07:27.740 --> 00:07:29.085
And so user data,


00:07:29.085 --> 00:07:34.130
if a large number of users will
usually change relatively slowly.


00:07:34.130 --> 00:07:38.980
There are a few exceptions, of course,
COVID-19 being one of them were a shock to


00:07:38.980 --> 00:07:44.140
society actually cause a lot of people's
behavior that all change at the same time.


00:07:44.140 --> 00:07:48.678
And if you look at web search traffic,
you will see trends maybe a holiday or


00:07:48.678 --> 00:07:52.221
a new movie and
people start searching for something new.


00:07:52.221 --> 00:07:53.550
They just became popular.


00:07:53.550 --> 00:07:57.986
So there are exceptions, but on average,
if you have a very large group of users,


00:07:57.986 --> 00:07:59.546
there are only a few forces.


00:07:59.546 --> 00:08:03.386
They can simultaneously change
the behavior of a lot of people or


00:08:03.386 --> 00:08:04.840
at the same time.


00:08:04.840 --> 00:08:10.250
In contrast, if you work on a B2B or
business to business application,


00:08:10.250 --> 00:08:15.322
I find an enterprise data or
business data can shift quite quickly.


00:08:15.322 --> 00:08:19.140
Because the factory making
cellphones may suddenly decide.


00:08:19.140 --> 00:08:23.448
So use a new coating for the cell
phones and suddenly the entire dataset


00:08:23.448 --> 00:08:27.340
changes because the cell phones
suddenly all look different.


00:08:27.340 --> 00:08:32.058
But if you're providing a machine
learning system to a company,


00:08:32.058 --> 00:08:36.775
then sometimes if the CEO of that
company decides to change the way


00:08:36.775 --> 00:08:41.610
that business operates,
all of that data can shift very quickly.


00:08:41.610 --> 00:08:46.204
I know that these two bullets
are speaking in generalities and


00:08:46.204 --> 00:08:50.140
there are certain exceptions
to both of these.


00:08:50.140 --> 00:08:54.879
But maybe this will give you a way of
thinking about how quickly your data is


00:08:54.879 --> 00:08:57.340
likely to change or not change.


00:08:57.340 --> 00:09:03.240
So that's it, congratulations on making
it to the end of this first weeks videos.


00:09:03.240 --> 00:09:06.626
I hope you also take a look at
the practice quizzes which will let you


00:09:06.626 --> 00:09:10.400
practice all of these concepts and
make sure you deeply understand them.


00:09:10.400 --> 00:09:11.324
And if you want,


00:09:11.324 --> 00:09:15.815
you can also take a look at this week's
optional programming exercise which will


00:09:15.815 --> 00:09:19.740
let you deploy a machine learning
model on your own computer.


00:09:19.740 --> 00:09:23.932
And I also look forward to seeing you
in next week's videos where we'll dive


00:09:23.932 --> 00:09:27.536
together much more deeply into
the modeling part of the full cycle


00:09:27.536 --> 00:09:29.270
of machine learning project.


00:09:29.270 --> 00:09:30.961
I look forward to seeing you next week.
WEBVTT


00:00:00.000 --> 00:00:03.330
Hi, welcome back. In this week,


00:00:03.330 --> 00:00:06.120
you learn about some best
practices for building


00:00:06.120 --> 00:00:07.950
a machine learning model that is


00:00:07.950 --> 00:00:10.470
worthy of a production
deployment.


00:00:10.470 --> 00:00:13.380
One of my friends,
Adam Cotes joke that


00:00:13.380 --> 00:00:14.820
the way he listened to me


00:00:14.820 --> 00:00:16.590
give advice to machine
learning teams,


00:00:16.590 --> 00:00:18.555
he felt the way I get advice was


00:00:18.555 --> 00:00:21.000
quite consistent from
project to project,


00:00:21.000 --> 00:00:23.310
so that he could
almost replace me with


00:00:23.310 --> 00:00:26.190
an if then else
sequence of statements.


00:00:26.190 --> 00:00:28.350
I found too when several senior


00:00:28.350 --> 00:00:30.600
machinery engineers
look at the project,


00:00:30.600 --> 00:00:31.920
the advice they tend to give


00:00:31.920 --> 00:00:34.345
is also remarkably consistent.


00:00:34.345 --> 00:00:36.900
What you learned in this
week is whether some of


00:00:36.900 --> 00:00:39.120
the key challenges
of trying to build


00:00:39.120 --> 00:00:41.820
a production-ready
machine learning model,


00:00:41.820 --> 00:00:44.370
things like how do you
handle new datasets?


00:00:44.370 --> 00:00:46.650
Or what if you do
well in the test set,


00:00:46.650 --> 00:00:47.775
but for some reason,


00:00:47.775 --> 00:00:51.420
that still isn't good enough
for your actual application?


00:00:51.420 --> 00:00:54.750
I hope that after this
week's materials,


00:00:54.750 --> 00:00:57.330
you'll be able to
very efficiently


00:00:57.330 --> 00:01:00.330
know how to improve your
machine learning model,


00:01:00.330 --> 00:01:02.025
to solve the most
important problems


00:01:02.025 --> 00:01:04.065
that then make it
deployment's ready.


00:01:04.065 --> 00:01:05.700
Let's dive in. This week,


00:01:05.700 --> 00:01:07.215
our focus will be on


00:01:07.215 --> 00:01:09.540
the modeling part of


00:01:09.540 --> 00:01:12.615
the full cycle of a
machine learning project,


00:01:12.615 --> 00:01:15.209
and you learn some suggestions


00:01:15.209 --> 00:01:17.100
for how to select
and train the model,


00:01:17.100 --> 00:01:19.485
and how to perform
error analysis,


00:01:19.485 --> 00:01:22.160
and use that to drive
model improvements.


00:01:22.160 --> 00:01:25.620
One of the themes you hear me
refer to multiple times is


00:01:25.620 --> 00:01:27.794
model-centric AI development


00:01:27.794 --> 00:01:31.245
versus data-centric
AI development.


00:01:31.245 --> 00:01:33.445
The way that AI has grown up,


00:01:33.445 --> 00:01:35.160
there's been a lot of emphasis on


00:01:35.160 --> 00:01:37.200
how to choose the right model,


00:01:37.200 --> 00:01:38.460
such as maybe how to choose


00:01:38.460 --> 00:01:40.505
the right neural
network architecture.


00:01:40.505 --> 00:01:43.470
I found that for
practical projects,


00:01:43.470 --> 00:01:45.060
it can be even more


00:01:45.060 --> 00:01:48.239
useful to take a more
data-centric approach,


00:01:48.239 --> 00:01:49.830
where you focus not just on


00:01:49.830 --> 00:01:52.725
improving the neural
network architecture,


00:01:52.725 --> 00:01:55.230
but on making sure
you are feeding


00:01:55.230 --> 00:01:58.455
your algorithm high-quality data.


00:01:58.455 --> 00:02:00.630
That ultimately lets you be more


00:02:00.630 --> 00:02:03.345
efficient in getting your
system to perform well.


00:02:03.345 --> 00:02:05.070
But the way I engage in


00:02:05.070 --> 00:02:07.200
data-centric AI
development is not


00:02:07.200 --> 00:02:09.840
to just go and try to
collect more data,


00:02:09.840 --> 00:02:11.595
which can be very time-consuming,


00:02:11.595 --> 00:02:14.640
but to instead use
tools to help me


00:02:14.640 --> 00:02:18.030
improve the data in the most
efficient possible way.


00:02:18.030 --> 00:02:21.415
You'll learn some ways for
how to do that in this week.


00:02:21.415 --> 00:02:23.490
I'm excited to go through


00:02:23.490 --> 00:02:26.565
this week's materials with
you on training models.


00:02:26.565 --> 00:02:29.550
But first, let's look at
some key challenges that


00:02:29.550 --> 00:02:32.745
many fields face when building
machine learning models.


00:02:32.745 --> 00:02:34.920
By understanding
these key challenges,


00:02:34.920 --> 00:02:37.620
you'd be better able to
spot them ahead of time,


00:02:37.620 --> 00:02:40.545
and adjust them more
efficiently for your projects.


00:02:40.545 --> 00:02:42.910
Let's go on to the next video.
WEBVTT


00:00:00.640 --> 00:00:04.284
What is hard about trading machine or
any model that does well?


00:00:04.284 --> 00:00:07.040
Let's look at some key challenges.


00:00:07.040 --> 00:00:11.986
One framework that I hope you keep in
mind when developing machine learning


00:00:11.986 --> 00:00:17.169
systems is that, AI systems of machine
learning systems comprise both code,


00:00:17.169 --> 00:00:21.140
meaning the algorithm or
the model as well as data.


00:00:21.140 --> 00:00:25.681
There's been a lot of emphasis in the last
several decades on how to improve


00:00:25.681 --> 00:00:26.350
the code.


00:00:26.350 --> 00:00:31.888
In fact a lot of a I research had grown up
by researchers downloading data sets and


00:00:31.888 --> 00:00:36.210
trying to find an overall model
that does well on the dataset.


00:00:36.210 --> 00:00:37.877
But for many applications,


00:00:37.877 --> 00:00:42.740
you have the flexibility to change
the data if you don't like the data.


00:00:42.740 --> 00:00:47.158
And so, there are many projects
where the algorithm or


00:00:47.158 --> 00:00:50.241
model is basically a solved problem.


00:00:50.241 --> 00:00:54.128
Some model you download off get
hub will do well enough, and


00:00:54.128 --> 00:00:58.794
they'll be more efficient to spend
a lot of your time improving the data


00:00:58.794 --> 00:01:03.570
because the data usually has been
much more customized to your problem.


00:01:03.570 --> 00:01:08.911
This is a view that will carry throughout
this week and next week's materials.


00:01:08.911 --> 00:01:14.235
Diving into more detail,
when building a machine learning system,


00:01:14.235 --> 00:01:20.028
you may have an algorithm or a model,
this would be your code and some data.


00:01:20.028 --> 00:01:23.553
And it's by training your
algorithm on the data that you


00:01:23.553 --> 00:01:28.440
then have your machine learning
model that can make predictions.


00:01:28.440 --> 00:01:33.350
And of course hyper parameters
are an additional input to this process.


00:01:33.350 --> 00:01:36.724
It is important for many applications
to make sure you have a well to


00:01:36.724 --> 00:01:40.600
learning rates and regularization
parameter and maybe a few other things.


00:01:40.600 --> 00:01:43.881
The hyper parameters are important, but


00:01:43.881 --> 00:01:49.505
because the space of hyper parameters
is usually relatively limited,


00:01:49.505 --> 00:01:55.150
I'm going to spend more of our time
focusing on the code and on the data.


00:01:55.150 --> 00:01:58.767
So model development is
a highly iterative process.


00:01:58.767 --> 00:02:04.660
You usually start off with some model and
hyper parameters and data training model,


00:02:04.660 --> 00:02:09.640
and then take the model to carry error
analysis, and use that to help you


00:02:09.640 --> 00:02:14.380
decide how to improve the model or
the hyper parameters or the data.


00:02:14.380 --> 00:02:19.377
Because machine learning it's such
an empirical process, being able to go


00:02:19.377 --> 00:02:24.550
through this loop many times very quickly,
is key to improving performance.


00:02:24.550 --> 00:02:28.922
But one of the things that will
help you improve performance to is,


00:02:28.922 --> 00:02:34.075
each time through the loop, being able
to make good choices about how to modify


00:02:34.075 --> 00:02:38.862
the data or how to modify the model or
how to modify the hyper parameters.


00:02:38.862 --> 00:02:43.320
After you've done this enough times and
achieve a good model,


00:02:43.320 --> 00:02:48.703
one last step that's often useful is to
carry out a richer error analysis and


00:02:48.703 --> 00:02:53.664
have your system go through a final
audit to make sure that it is working


00:02:53.664 --> 00:02:57.060
before you push it to
a production deployment.


00:02:57.060 --> 00:02:59.568
So why is model development hard?


00:02:59.568 --> 00:03:04.746
When building a model,
I think there are three key milestones


00:03:04.746 --> 00:03:08.939
that most projects should
aspire to accomplish.


00:03:08.939 --> 00:03:14.090
First is you probably want to make sure
you do well, at least on the training set.


00:03:14.090 --> 00:03:19.917
So, if you're predicting housing prices
as a function of the size of a house,


00:03:19.917 --> 00:03:25.145
are you at least able to fit a line
that is your training set quite well?


00:03:25.145 --> 00:03:28.439
After you've done well
on the training set,


00:03:28.439 --> 00:03:33.641
you then have to ask if your algorithm
does well on the development set or


00:03:33.641 --> 00:03:38.640
the holdout cross validation set,
and then also the test set.


00:03:38.640 --> 00:03:42.814
If your algorithm isn't even
doing well on the training set,


00:03:42.814 --> 00:03:47.170
then it's very unlikely to do well
on the dev set or the test set.


00:03:47.170 --> 00:03:51.985
So I think of step one as
something you have to do first as


00:03:51.985 --> 00:03:56.605
a milestone on your way
towards achieving step two.


00:03:56.605 --> 00:04:00.278
And then after you do well
on the dev set or test set,


00:04:00.278 --> 00:04:05.150
you also have to make sure that
you're learning algorithm does well


00:04:05.150 --> 00:04:10.470
according to the business metrics or
according to the project's goals.


00:04:10.470 --> 00:04:16.063
Over the last several decades,
a lot of machine learning development,


00:04:16.063 --> 00:04:20.839
was driven by the goal of doing
well on the dev set or test set.


00:04:20.839 --> 00:04:26.009
Unfortunately for many problems,
having a high test set accuracy


00:04:26.009 --> 00:04:30.630
is not sufficient for
achieving the goals of the project.


00:04:30.630 --> 00:04:35.597
And this has led to a lot of frustration
and disagreements between the machine


00:04:35.597 --> 00:04:40.490
learning team, which is very good at
doing this and business teams which care


00:04:40.490 --> 00:04:45.340
more about the business metrics or
some other goals of the project.


00:04:45.340 --> 00:04:49.413
So you may be wondering: "Hey Andrew,
how is it possibly true that


00:04:49.413 --> 00:04:53.810
achieving low average test set error
isn't good enough for a project?"


00:04:53.810 --> 00:04:58.570
There are few common patterns that I've
seen across many projects where you need


00:04:58.570 --> 00:05:01.510
something beyond low
average test set error, and


00:05:01.510 --> 00:05:06.340
people spot these issues will help you
be more efficient in addressing them.


00:05:06.340 --> 00:05:09.161
Let's dive more into this
topic in the next video.
WEBVTT


00:00:00.000 --> 00:00:02.070
The job of a machine
learning engineer


00:00:02.070 --> 00:00:03.450
would be much simpler if


00:00:03.450 --> 00:00:05.400
the only thing we
ever had to do was


00:00:05.400 --> 00:00:07.665
do well on the holdout test set.


00:00:07.665 --> 00:00:10.995
As hard as it is to do well
in the holdout test set,


00:00:10.995 --> 00:00:13.875
unfortunately, sometimes
that isn't enough.


00:00:13.875 --> 00:00:15.120
Let's take a look at some of


00:00:15.120 --> 00:00:16.950
the other things we
sometimes need to


00:00:16.950 --> 00:00:20.340
accomplish in order to
make a project successful.


00:00:20.340 --> 00:00:21.810
We've already talked about


00:00:21.810 --> 00:00:24.180
concept drift and
data drift last week,


00:00:24.180 --> 00:00:27.870
but here are some additional
challenges we may have to


00:00:27.870 --> 00:00:32.255
address for a production
machine learning project.


00:00:32.255 --> 00:00:34.530
First, a machine learning system


00:00:34.530 --> 00:00:37.515
may have low average
test set error,


00:00:37.515 --> 00:00:40.440
but if its performance
on a set of


00:00:40.440 --> 00:00:43.575
disproportionately important
examples isn't good enough,


00:00:43.575 --> 00:00:46.140
then the machine learning
system will still


00:00:46.140 --> 00:00:49.500
not be acceptable for
production deployment.


00:00:49.500 --> 00:00:52.275
Let me use an example
from Web search.


00:00:52.275 --> 00:00:54.600
There are a lot of
web search queries


00:00:54.600 --> 00:00:56.760
like these: Apple pie recipe,


00:00:56.760 --> 00:00:58.875
latest movies,
wireless data plan,


00:00:58.875 --> 00:01:01.155
I want to learn about
the Diwali Festival.


00:01:01.155 --> 00:01:03.450
These types of queries
are sometimes called


00:01:03.450 --> 00:01:05.790
informational or
transactional queries,


00:01:05.790 --> 00:01:09.000
where I want to learn
about apple pies or


00:01:09.000 --> 00:01:12.330
maybe I want to buy a new
wireless data plan and you


00:01:12.330 --> 00:01:13.860
might be willing to forgive


00:01:13.860 --> 00:01:16.485
a web search engine
that doesn't give you


00:01:16.485 --> 00:01:18.600
the best apple pie recipe


00:01:18.600 --> 00:01:19.860
because there are a
lot of good apple


00:01:19.860 --> 00:01:21.765
pie recipes on the Internet.


00:01:21.765 --> 00:01:24.645
For informational and
transactional queries,


00:01:24.645 --> 00:01:26.100
a web search engine wants to


00:01:26.100 --> 00:01:27.945
return the most relevant results,


00:01:27.945 --> 00:01:30.105
but users are willing to forgive


00:01:30.105 --> 00:01:32.460
maybe ranking the best result,


00:01:32.460 --> 00:01:34.245
Number two or Number three.


00:01:34.245 --> 00:01:35.925
There's a different type of


00:01:35.925 --> 00:01:38.925
web search query
such as Stanford,


00:01:38.925 --> 00:01:40.955
or Reddit, or YouTube.


00:01:40.955 --> 00:01:43.380
These are called
navigational queries,


00:01:43.380 --> 00:01:45.480
where the user has a
very clear intent,


00:01:45.480 --> 00:01:48.660
very clear desire to
go to Stanford.edu,


00:01:48.660 --> 00:01:51.075
or Reddit.com, or YouTube.com.


00:01:51.075 --> 00:01:55.065
When a user has a very
clear navigational intent,


00:01:55.065 --> 00:01:56.850
they will tend to be very


00:01:56.850 --> 00:01:58.830
unforgiving if a
web search engine


00:01:58.830 --> 00:02:00.060
does anything other than


00:02:00.060 --> 00:02:04.500
return Stanford.edu as the
Number one ranked results and


00:02:04.500 --> 00:02:06.090
the search engine
that doesn't give


00:02:06.090 --> 00:02:08.010
the right results will quickly


00:02:08.010 --> 00:02:10.725
lose the trust of its users.


00:02:10.725 --> 00:02:13.560
Navigational queries
in this context are


00:02:13.560 --> 00:02:15.690
a disproportionately
important set of


00:02:15.690 --> 00:02:18.510
examples and if you have
a learning algorithm


00:02:18.510 --> 00:02:21.540
that improves your average
test set accuracy for


00:02:21.540 --> 00:02:23.565
web search but messes up


00:02:23.565 --> 00:02:26.400
just a small handful of
navigational queries,


00:02:26.400 --> 00:02:29.970
that may not be acceptable
for deployment.


00:02:29.970 --> 00:02:31.770
The challenge, of course,


00:02:31.770 --> 00:02:34.439
is that average test set accuracy


00:02:34.439 --> 00:02:37.290
tends to weight all
examples equally,


00:02:37.290 --> 00:02:39.450
whereas, in web search,


00:02:39.450 --> 00:02:44.100
some queries are
disproportionately important.


00:02:44.100 --> 00:02:46.470
Now one thing you
could do is try to


00:02:46.470 --> 00:02:48.660
give these examples
a higher weight.


00:02:48.660 --> 00:02:50.895
That could work for
some applications,


00:02:50.895 --> 00:02:52.500
but in my experience,


00:02:52.500 --> 00:02:54.540
just changing the weights
of different examples


00:02:54.540 --> 00:02:57.030
doesn't always solve
the entire problem.


00:02:57.030 --> 00:03:00.780
Closely related to this
is the question of


00:03:00.780 --> 00:03:04.490
performance on key
slices of the data set.


00:03:04.490 --> 00:03:06.180
For example, let's
say you've built


00:03:06.180 --> 00:03:07.890
a machine learning algorithm for


00:03:07.890 --> 00:03:09.690
loan approval to decide who is


00:03:09.690 --> 00:03:12.150
likely to repay a
loan and thus to


00:03:12.150 --> 00:03:15.165
recommend approving certain
loans for approval.


00:03:15.165 --> 00:03:16.530
For such a system,


00:03:16.530 --> 00:03:18.480
you will probably want to make


00:03:18.480 --> 00:03:20.910
sure that your system does not


00:03:20.910 --> 00:03:23.969
unfairly discriminate
against loan applicants


00:03:23.969 --> 00:03:25.710
according to their ethnicity,


00:03:25.710 --> 00:03:28.365
gender, maybe their location,


00:03:28.365 --> 00:03:31.805
their language, or other
protected attributes.


00:03:31.805 --> 00:03:33.930
Many countries also have laws or


00:03:33.930 --> 00:03:38.460
regulations that mandates
that financial systems


00:03:38.460 --> 00:03:40.590
and loan approval processes not


00:03:40.590 --> 00:03:45.075
discriminate on the basis of
a certain set of attributes,


00:03:45.075 --> 00:03:47.730
sometimes called
protected attributes.


00:03:47.730 --> 00:03:50.190
Even if a learning algorithm for


00:03:50.190 --> 00:03:53.940
loan approval achieves high
average test set accuracy,


00:03:53.940 --> 00:03:55.890
it would not be acceptable for


00:03:55.890 --> 00:03:58.170
production deployment if it


00:03:58.170 --> 00:04:03.495
exhibits an unacceptable level
of bias or discrimination.


00:04:03.495 --> 00:04:06.360
Whereas the A.I. community
has had a lot of


00:04:06.360 --> 00:04:09.180
discussion about
fairness to individuals,


00:04:09.180 --> 00:04:10.770
and rightly so because this is


00:04:10.770 --> 00:04:14.625
an important topic we have
to address and do well on,


00:04:14.625 --> 00:04:17.580
the issue of fairness or


00:04:17.580 --> 00:04:21.250
performance of key slices also
occurs in other settings.


00:04:21.250 --> 00:04:23.820
Let's say you run an
online shopping website,


00:04:23.820 --> 00:04:27.480
so an e-commerce website
where you advocate and sell


00:04:27.480 --> 00:04:29.660
products from many
different manufacturers


00:04:29.660 --> 00:04:31.620
and many different
brands of retailers.


00:04:31.620 --> 00:04:33.480
You might want to make sure that


00:04:33.480 --> 00:04:37.200
your system treats
fairly all major user,


00:04:37.200 --> 00:04:39.615
retailer, and product categories.


00:04:39.615 --> 00:04:41.460
For example, even if


00:04:41.460 --> 00:04:42.750
a machine learning system


00:04:42.750 --> 00:04:44.820
has high average
test set accuracy,


00:04:44.820 --> 00:04:47.370
maybe it recommends better
products on average.


00:04:47.370 --> 00:04:51.300
If it gives really
irrelevant recommendations


00:04:51.300 --> 00:04:53.745
to all users of one ethnicity,


00:04:53.745 --> 00:04:55.740
that may be unacceptable,


00:04:55.740 --> 00:04:59.910
or if it always pushes products


00:04:59.910 --> 00:05:04.120
from large retailers and
ignores the smaller brands,


00:05:04.120 --> 00:05:05.950
that could also be harmful


00:05:05.950 --> 00:05:07.870
to the business because
you may then lose


00:05:07.870 --> 00:05:09.940
all the small
retailers and it would


00:05:09.940 --> 00:05:12.430
also feel unfair to build
a recommender system


00:05:12.430 --> 00:05:13.960
that only ever recommends


00:05:13.960 --> 00:05:16.090
products from the large
brands and ignores


00:05:16.090 --> 00:05:18.490
the smaller businesses or it had


00:05:18.490 --> 00:05:20.260
a product recommender that


00:05:20.260 --> 00:05:23.045
gave highly relevant
recommendations,


00:05:23.045 --> 00:05:24.520
but for some reason would


00:05:24.520 --> 00:05:27.020
never recommend
electronics products,


00:05:27.020 --> 00:05:29.560
then maybe the retailers


00:05:29.560 --> 00:05:31.720
that sell electronics
would be quite


00:05:31.720 --> 00:05:34.510
reasonably upset
and this may not be


00:05:34.510 --> 00:05:38.200
the right thing for the retailers
on your platform or for


00:05:38.200 --> 00:05:42.450
the long term health of your
business even if the average


00:05:42.450 --> 00:05:44.410
test set accuracy shows that by


00:05:44.410 --> 00:05:46.990
not recommending
electronics products,


00:05:46.990 --> 00:05:49.330
you're showing slightly
more relevant results


00:05:49.330 --> 00:05:51.645
to your users for some reason.


00:05:51.645 --> 00:05:55.060
One thing you'll learn
later this week


00:05:55.060 --> 00:05:58.660
is how to carry out analysis on


00:05:58.660 --> 00:06:02.290
key slices of the data
to make sure that


00:06:02.290 --> 00:06:06.140
you spot and address potential
problems like these.


00:06:06.140 --> 00:06:09.670
Next is the issue of rare classes


00:06:09.670 --> 00:06:13.950
and specifically of skewed
data distributions.


00:06:13.950 --> 00:06:17.170
In medical diagnosis,
it's not uncommon


00:06:17.170 --> 00:06:21.385
for many patients not to
have a certain disease,


00:06:21.385 --> 00:06:25.730
and so if you have
a data set which is


00:06:25.730 --> 00:06:30.220
99 percent negative examples
because 99 percent of


00:06:30.220 --> 00:06:31.750
the population doesn't have


00:06:31.750 --> 00:06:36.260
a certain disease but
one percent positive.


00:06:36.260 --> 00:06:38.415
Then you can achieve


00:06:38.415 --> 00:06:41.190
very good test set accuracy by


00:06:41.190 --> 00:06:44.115
writing a program that
just says print "0".


00:06:44.115 --> 00:06:46.620
Don't need a learning algorithm. Just
write this one line of


00:06:46.620 --> 00:06:50.175
code and you have 99 percent
accuracy on your dataset.


00:06:50.175 --> 00:06:52.260
But clearly, print "0" is not


00:06:52.260 --> 00:06:57.300
a very useful algorithm
for disease diagnosis.


00:06:57.300 --> 00:06:59.310
By the way, this
actually did happen


00:06:59.310 --> 00:07:01.080
to me once where my
team had trained


00:07:01.080 --> 00:07:03.300
a huge neural
network found we had


00:07:03.300 --> 00:07:06.150
99 percent average
accuracy and we found and


00:07:06.150 --> 00:07:09.180
achieved it by printing
"0" all the time,


00:07:09.180 --> 00:07:11.655
so we basically trained a giant neural
network that did


00:07:11.655 --> 00:07:14.460
exactly the same
thing as print "0",


00:07:14.460 --> 00:07:15.810
and of course, when
we discovered,


00:07:15.810 --> 00:07:18.420
we then went back
to fix the problem.


00:07:18.420 --> 00:07:20.835
Hopefully this won't
happen to you.


00:07:20.835 --> 00:07:23.040
Closely related to the issue of


00:07:23.040 --> 00:07:25.410
skewed data distributions
which is often


00:07:25.410 --> 00:07:27.840
a discussion of
positive and negatives


00:07:27.840 --> 00:07:30.600
is accuracy on rare classes.


00:07:30.600 --> 00:07:34.500
I was working with my friend
Pranav Ross Baker and


00:07:34.500 --> 00:07:38.415
others on diagnosis
from chest X-rays


00:07:38.415 --> 00:07:42.405
and we were diagnosing
causes and we were


00:07:42.405 --> 00:07:46.995
working on deep learning to
spot different conditions.


00:07:46.995 --> 00:07:50.640
There were some relatively
common conditions,


00:07:50.640 --> 00:07:53.430
these are technical
medical terminology,


00:07:53.430 --> 00:07:56.685
but for a medical
condition called effusion,


00:07:56.685 --> 00:08:00.465
we had about 10,000 images and so


00:08:00.465 --> 00:08:04.980
we were able to achieve a
high level of performance,


00:08:04.980 --> 00:08:08.250
whereas for much rarer
condition hernia,


00:08:08.250 --> 00:08:10.890
we had about a hundred images and


00:08:10.890 --> 00:08:13.905
so performance was much worse.


00:08:13.905 --> 00:08:19.545
It turns out that from a
medical standpoint is not


00:08:19.545 --> 00:08:22.845
acceptable for diagnosis system


00:08:22.845 --> 00:08:26.865
to ignore obvious
cases of hernia.


00:08:26.865 --> 00:08:28.560
If a patient shows up and


00:08:28.560 --> 00:08:30.975
an X-ray clearly shows
they have hernia,


00:08:30.975 --> 00:08:33.495
a learning algorithm
that misses that


00:08:33.495 --> 00:08:36.915
diagnosis would be problematic,


00:08:36.915 --> 00:08:40.215
but because this was a
relatively rare class,


00:08:40.215 --> 00:08:43.200
the overall average
test set accuracy


00:08:43.200 --> 00:08:45.690
of the algorithm
was not that bad,


00:08:45.690 --> 00:08:47.880
and in fact the algorithm
could have completely


00:08:47.880 --> 00:08:50.190
ignored all cases of hernia and


00:08:50.190 --> 00:08:52.350
it would have had only
a modest impact on


00:08:52.350 --> 00:08:55.395
this average test accuracy,


00:08:55.395 --> 00:08:57.630
because cases of hernia were


00:08:57.630 --> 00:09:01.125
rare and the algorithm could
pretty much ignore it


00:09:01.125 --> 00:09:05.310
without hurting this average
test set accuracy that much if


00:09:05.310 --> 00:09:07.365
average test set accuracy gives


00:09:07.365 --> 00:09:11.205
equal weight to every single
example in the test set.


00:09:11.205 --> 00:09:15.960
I have heard pretty much this
exact same conversation too


00:09:15.960 --> 00:09:18.284
many times in too many companies


00:09:18.284 --> 00:09:20.445
and the conversation
goes like this,


00:09:20.445 --> 00:09:22.410
a machine learning engineer says,


00:09:22.410 --> 00:09:24.165
"I did well in the test set!",


00:09:24.165 --> 00:09:27.315
"This works! Let's use it!"


00:09:27.315 --> 00:09:30.870
and a private owner or
business owner says,


00:09:30.870 --> 00:09:32.730
"but this doesn't work for my


00:09:32.730 --> 00:09:36.870
application" and the machine
that the engineer replies,


00:09:36.870 --> 00:09:39.885
"but I did well on the test set!"


00:09:39.885 --> 00:09:41.505
my advice to you,


00:09:41.505 --> 00:09:44.025
if you ever find yourself
in this conversation,


00:09:44.025 --> 00:09:46.230
is don't get defensive.


00:09:46.230 --> 00:09:49.200
We as a community have built


00:09:49.200 --> 00:09:52.050
lots of tools for doing
well on the test set,


00:09:52.050 --> 00:09:53.640
and that's to be celebrated.


00:09:53.640 --> 00:09:55.980
I think it's great,
but we often need to


00:09:55.980 --> 00:09:58.410
go beyond that because just doing


00:09:58.410 --> 00:10:00.675
well on the test set isn't enough


00:10:00.675 --> 00:10:03.495
for many production applications.


00:10:03.495 --> 00:10:06.000
When I'm building a
machine learning system,


00:10:06.000 --> 00:10:10.200
I view it as my job not just
to do well on the test set,


00:10:10.200 --> 00:10:13.230
but to produce a machine
learning system that


00:10:13.230 --> 00:10:16.200
solves the actual business
or application needs,


00:10:16.200 --> 00:10:18.720
and I hope you take a
similar view as well.


00:10:18.720 --> 00:10:21.480
Later this week, we'll go
through some techniques,


00:10:21.480 --> 00:10:23.655
usually involving error analysis,


00:10:23.655 --> 00:10:25.470
maybe error analysis on slices of


00:10:25.470 --> 00:10:28.680
the data that will allow
you to spot some of


00:10:28.680 --> 00:10:31.410
these issues that
require going beyond


00:10:31.410 --> 00:10:34.380
average test set accuracy
and help you with


00:10:34.380 --> 00:10:39.280
tools to tackle these
broader challenges as well.
WEBVTT


00:00:00.000 --> 00:00:02.880
When starting work on a
machine learning project,


00:00:02.880 --> 00:00:05.670
one of the most useful
first step to take is to


00:00:05.670 --> 00:00:08.760
establish a baseline
and is usually


00:00:08.760 --> 00:00:10.350
only after you've established


00:00:10.350 --> 00:00:12.810
a baseline level of
performance that you


00:00:12.810 --> 00:00:14.550
can then have tools
to efficiently


00:00:14.550 --> 00:00:17.270
improve on that baseline level.


00:00:17.270 --> 00:00:19.679
Let's dive into
some best practices


00:00:19.679 --> 00:00:21.855
for quickly
establishing that base.


00:00:21.855 --> 00:00:24.075
Let me use the speech
recognition example.


00:00:24.075 --> 00:00:26.040
Let's say you've
established that there are


00:00:26.040 --> 00:00:29.205
four major categories
of speech in your data.


00:00:29.205 --> 00:00:31.140
Clear speech, which
is when someone


00:00:31.140 --> 00:00:33.075
speaks without much
background noise.


00:00:33.075 --> 00:00:34.440
Speech with car noise in


00:00:34.440 --> 00:00:36.085
the background as if they were in


00:00:36.085 --> 00:00:39.525
a car when they use your
speech recognition system.


00:00:39.525 --> 00:00:41.070
Speech with people noise in


00:00:41.070 --> 00:00:43.440
the background so that
they're outdoors with other


00:00:43.440 --> 00:00:45.240
people's out in the background or


00:00:45.240 --> 00:00:47.340
speech on a low
bandwidth connection,


00:00:47.340 --> 00:00:49.680
what it sounds like
if you're using


00:00:49.680 --> 00:00:52.170
a cell phone with a very
bad cell phone connection.


00:00:52.170 --> 00:00:53.550
If you're accuracy on


00:00:53.550 --> 00:00:57.270
these four categories of
speeches, 94, 89, 87,


00:00:57.270 --> 00:00:58.635
and 70 percent accuracy,


00:00:58.635 --> 00:01:00.930
you might be tempted
to say, well,


00:01:00.930 --> 00:01:03.345
it does worse on low
bandwidth audio,


00:01:03.345 --> 00:01:05.715
so let's focus our
attention on that.


00:01:05.715 --> 00:01:08.280
But before leaping
to that conclusion,


00:01:08.280 --> 00:01:11.250
it'd be useful to
establish a baseline level


00:01:11.250 --> 00:01:14.580
of performance on all
four of these categories.


00:01:14.580 --> 00:01:18.450
You can do that by asking
some human transcriptionists


00:01:18.450 --> 00:01:22.290
to label your data and
measuring their accuracy.


00:01:22.290 --> 00:01:24.029
What is human level performance


00:01:24.029 --> 00:01:26.310
on these four
categories of speech?


00:01:26.310 --> 00:01:30.000
In this example, we find
that if we can improve


00:01:30.000 --> 00:01:31.920
our performance on clear speech


00:01:31.920 --> 00:01:33.750
up to human level performance,


00:01:33.750 --> 00:01:35.669
looks like there's a potential


00:01:35.669 --> 00:01:37.765
for a one percent
improvement there.


00:01:37.765 --> 00:01:40.725
If we can raise our
performance up to


00:01:40.725 --> 00:01:42.170
human level performance on


00:01:42.170 --> 00:01:44.495
audio of car noise
in the background,


00:01:44.495 --> 00:01:47.015
maybe four percent improvement,


00:01:47.015 --> 00:01:50.440
two percent improvement and


00:01:50.440 --> 00:01:55.665
slightly zero percent improvement
on low bandwidth audio.


00:01:55.665 --> 00:01:57.920
Whereas we had previously


00:01:57.920 --> 00:02:00.350
said without the human
level of performance,


00:02:00.350 --> 00:02:02.480
we may have thought working on


00:02:02.480 --> 00:02:04.640
low bandwidth audio
was most promising.


00:02:04.640 --> 00:02:06.590
With this analysis,
we realized that


00:02:06.590 --> 00:02:08.960
maybe the low bandwidth
audio was so garbled.


00:02:08.960 --> 00:02:11.930
Even people, humans
can't recognize what was


00:02:11.930 --> 00:02:15.845
said and it may not be that
fruitful to work on that.


00:02:15.845 --> 00:02:18.980
Instead, it may be more
fruitful to focus our attention


00:02:18.980 --> 00:02:20.870
on improving speech recognition


00:02:20.870 --> 00:02:22.900
with car noise in the background.


00:02:22.900 --> 00:02:27.440
In this example, using
human level performance,


00:02:27.440 --> 00:02:29.370
which are sometimes
abbreviated to


00:02:29.370 --> 00:02:31.520
HLP, Human Level Performance,


00:02:31.520 --> 00:02:33.530
gives you a point of


00:02:33.530 --> 00:02:37.130
comparison or a
baseline that helps you


00:02:37.130 --> 00:02:39.860
decide where to
focus your efforts


00:02:39.860 --> 00:02:43.620
on car noise data rather
than on low bandwidth data.


00:02:43.620 --> 00:02:46.010
It turns out the
best practices for


00:02:46.010 --> 00:02:48.395
establishing a baseline
are quite different,


00:02:48.395 --> 00:02:50.270
depending on whether
you're working on


00:02:50.270 --> 00:02:53.500
unstructured or structured data.


00:02:53.500 --> 00:02:57.605
Unstructured data refers
to data sets like images,


00:02:57.605 --> 00:03:00.070
maybe pictures of cats or audio,


00:03:00.070 --> 00:03:03.800
like our speech recognition
example or natural language,


00:03:03.800 --> 00:03:05.810
like text from
restaurant reviews.


00:03:05.810 --> 00:03:08.900
Unstructured data
tends to be data


00:03:08.900 --> 00:03:12.155
that humans are very
good at interpreting.


00:03:12.155 --> 00:03:15.830
In fact, humans evolve
to be very good at


00:03:15.830 --> 00:03:18.355
understanding images and audio


00:03:18.355 --> 00:03:20.700
and maybe language as well.


00:03:20.700 --> 00:03:24.450
Because humans are so good
at unstructured data tasks,


00:03:24.450 --> 00:03:28.199
measuring human level
performance or HLP,


00:03:28.199 --> 00:03:31.770
is often a good way to
establish a baseline


00:03:31.770 --> 00:03:35.580
if you are working on
unstructured data.


00:03:35.580 --> 00:03:38.490
In contrast, structured data are


00:03:38.490 --> 00:03:40.740
the giant databases or


00:03:40.740 --> 00:03:43.890
the giant Excel spreadsheets
you might have,


00:03:43.890 --> 00:03:48.085
such as if you run
an eCom website,


00:03:48.085 --> 00:03:50.535
the data showing which user


00:03:50.535 --> 00:03:53.670
purchased at what time
and for what price,


00:03:53.670 --> 00:03:56.415
that will be stored
in a giant database.


00:03:56.415 --> 00:04:00.270
This type of data stored in
a giant Excel spreadsheet


00:04:00.270 --> 00:04:04.800
or some more robust database
would be an example


00:04:04.800 --> 00:04:06.630
of structured data or


00:04:06.630 --> 00:04:08.835
your product and inventory data


00:04:08.835 --> 00:04:11.460
that would also be stored as


00:04:11.460 --> 00:04:14.820
structured data.
Because humans are not


00:04:14.820 --> 00:04:18.830
as good at looking at data
like this to make predictions.


00:04:18.830 --> 00:04:22.545
We certainly didn't evolve to
look at giant spreadsheets.


00:04:22.545 --> 00:04:25.170
Human level
performance is usually


00:04:25.170 --> 00:04:29.795
a less useful baseline for
structured data applications.


00:04:29.795 --> 00:04:31.500
I find that machine learning


00:04:31.500 --> 00:04:34.300
developments best practice
is quite different,


00:04:34.300 --> 00:04:35.860
depending on whether
you're working on


00:04:35.860 --> 00:04:39.460
an unstructured data or
structured data problem.


00:04:39.460 --> 00:04:41.305
Keeping in mind this difference,


00:04:41.305 --> 00:04:43.180
let's take a look at some ways to


00:04:43.180 --> 00:04:46.615
establish baselines for both
of these types of problems.


00:04:46.615 --> 00:04:48.530
We've already talked about


00:04:48.530 --> 00:04:50.450
human level performance
as a baseline,


00:04:50.450 --> 00:04:54.275
particularly for
unstructured data problems.


00:04:54.275 --> 00:04:57.140
Another way to establish
a baseline is to do


00:04:57.140 --> 00:05:00.485
a literature search
for state-of-the-art or


00:05:00.485 --> 00:05:03.080
look at open source
results to see


00:05:03.080 --> 00:05:04.850
what others reports they are


00:05:04.850 --> 00:05:07.415
able to accomplish on
this type of problem.


00:05:07.415 --> 00:05:09.170
For example, if you're building


00:05:09.170 --> 00:05:11.390
a speech recognition system and


00:05:11.390 --> 00:05:13.610
others report a certain level


00:05:13.610 --> 00:05:15.910
of accuracy on data
that's similar to yours,


00:05:15.910 --> 00:05:17.890
then that may give
you a starting point.


00:05:17.890 --> 00:05:20.389
Using open-source, you may also


00:05:20.389 --> 00:05:22.280
consider coming out


00:05:22.280 --> 00:05:24.275
with a quick-and-dirty
implementation.


00:05:24.275 --> 00:05:25.610
Now, this is going to the system,


00:05:25.610 --> 00:05:28.040
but just a quick-and-dirty
implementation that could


00:05:28.040 --> 00:05:30.980
start to give you a sense
of what may be possible.


00:05:30.980 --> 00:05:33.830
Finally, if you
already have a machine


00:05:33.830 --> 00:05:37.130
learning system running
for your application,


00:05:37.130 --> 00:05:40.490
then the performance of
your previous system,


00:05:40.490 --> 00:05:43.760
performance of your older
system can also help you


00:05:43.760 --> 00:05:46.160
establish a baseline that


00:05:46.160 --> 00:05:48.450
you can then aspire
to improve on.


00:05:48.450 --> 00:05:51.200
What a baseline system
or a baseline level of


00:05:51.200 --> 00:05:52.940
performance does is it helps to


00:05:52.940 --> 00:05:55.070
indicate what might be possible.


00:05:55.070 --> 00:05:56.975
In some cases, such as


00:05:56.975 --> 00:05:59.375
if you're using human
level performance,


00:05:59.375 --> 00:06:01.695
especially on unstructured
data problems,


00:06:01.695 --> 00:06:04.550
this baseline can also
give you a sense of what


00:06:04.550 --> 00:06:07.550
is the irreducible error
or what is Bayes error.


00:06:07.550 --> 00:06:09.740
In other words, what is
the best that anyone could


00:06:09.740 --> 00:06:11.090
possibly hope for in


00:06:11.090 --> 00:06:13.100
terms of performance
on this problem,


00:06:13.100 --> 00:06:15.260
such as helping us realize that


00:06:15.260 --> 00:06:17.930
maybe the low bandwidth audio is


00:06:17.930 --> 00:06:19.880
so bad that is just not


00:06:19.880 --> 00:06:22.710
possible to have more
than 70 percent accuracy,


00:06:22.710 --> 00:06:24.650
as in our earlier example.


00:06:24.650 --> 00:06:26.480
By helping us to get


00:06:26.480 --> 00:06:29.239
a very rough sense of
what might be possible,


00:06:29.239 --> 00:06:31.970
it can help us be much more


00:06:31.970 --> 00:06:35.865
efficient in terms of
prioritizing what to work on.


00:06:35.865 --> 00:06:39.110
Sometimes I've seen some
business teams push


00:06:39.110 --> 00:06:41.255
a machine learning team to


00:06:41.255 --> 00:06:44.090
guarantee that the
learning algorithm will be


00:06:44.090 --> 00:06:46.130
80 percent accurate or 90 percent


00:06:46.130 --> 00:06:48.080
or 99 percent accurate before


00:06:48.080 --> 00:06:50.300
the machine learning team
has even had a chance


00:06:50.300 --> 00:06:52.765
to establish a rough baseline.


00:06:52.765 --> 00:06:54.020
This, unfortunately,


00:06:54.020 --> 00:06:55.340
puts the machine learning team


00:06:55.340 --> 00:06:56.990
in a very difficult position.


00:06:56.990 --> 00:06:58.860
If you are in that position,


00:06:58.860 --> 00:07:00.770
I would urge you to
consider pushing


00:07:00.770 --> 00:07:03.770
back and asking for
time to establish


00:07:03.770 --> 00:07:06.890
a rough baseline level of
performance before giving


00:07:06.890 --> 00:07:09.140
a more firm prediction
about how accurate


00:07:09.140 --> 00:07:12.440
the machine learning system
can eventually get to be.


00:07:12.440 --> 00:07:14.150
It helps you to make your case,


00:07:14.150 --> 00:07:17.620
feel free to tell them
that I asked you to do so.


00:07:17.620 --> 00:07:20.810
I think establishing that
baseline first will help


00:07:20.810 --> 00:07:24.610
set you and your team up
better for long-term success.


00:07:24.610 --> 00:07:27.379
Now to tell us about the
importance of baseline,


00:07:27.379 --> 00:07:28.730
there are few additional tips


00:07:28.730 --> 00:07:30.260
I want to share with you about


00:07:30.260 --> 00:07:31.310
how to get started


00:07:31.310 --> 00:07:33.515
quickly on the machine
learning project.


00:07:33.515 --> 00:07:35.300
Let's go on to the next video


00:07:35.300 --> 00:07:38.160
to take a look at
some of these tips.
WEBVTT


00:00:00.000 --> 00:00:02.725
Let me share with
you a few tips for


00:00:02.725 --> 00:00:05.320
getting started on
machine learning project.


00:00:05.320 --> 00:00:07.360
This video will be
a little bit of


00:00:07.360 --> 00:00:09.310
a grab bag of different ideas,


00:00:09.310 --> 00:00:10.885
but I hope nonetheless


00:00:10.885 --> 00:00:13.015
many of these ideas
will be useful to you.


00:00:13.015 --> 00:00:15.580
We've talked about how
machine learning is


00:00:15.580 --> 00:00:17.625
an iterative process
where you start with


00:00:17.625 --> 00:00:20.440
a model, data, hyperparameters,


00:00:20.440 --> 00:00:24.910
training model, carry
out error analysis,


00:00:24.910 --> 00:00:28.860
and then use that to drive
further improvements.


00:00:28.860 --> 00:00:30.940
After you've done
this a few times,


00:00:30.940 --> 00:00:32.590
gone around the
loop enough times,


00:00:32.590 --> 00:00:34.855
when you have a
good enough model,


00:00:34.855 --> 00:00:36.220
you might then carry out


00:00:36.220 --> 00:00:40.550
a final performance audit
before taking it to production.


00:00:40.550 --> 00:00:42.670
In order to get started on


00:00:42.670 --> 00:00:46.339
this first step of
coming of the model,


00:00:46.339 --> 00:00:48.160
here are some suggestions.


00:00:48.160 --> 00:00:50.830
When I'm starting on a
machine learning project,


00:00:50.830 --> 00:00:52.780
I almost always start with


00:00:52.780 --> 00:00:55.430
a quick literature search
to see what's possible,


00:00:55.430 --> 00:00:57.460
so you can look at
online courses,


00:00:57.460 --> 00:01:01.055
look at blogs, look at
open source projects.


00:01:01.055 --> 00:01:05.260
My advice to you
if your goal is to


00:01:05.260 --> 00:01:07.570
build a practical
production system


00:01:07.570 --> 00:01:09.715
and not to do research is,


00:01:09.715 --> 00:01:11.410
don't obsess about finding


00:01:11.410 --> 00:01:14.290
the latest, greatest algorithm.


00:01:14.290 --> 00:01:16.750
Instead, spend half a day,


00:01:16.750 --> 00:01:21.010
maybe a small number of days
reading blog posts and pick


00:01:21.010 --> 00:01:25.484
something reasonable that
lets you get started quickly,


00:01:25.484 --> 00:01:28.185
if you can find an open
source implementation,


00:01:28.185 --> 00:01:30.280
that can also help you establish


00:01:30.280 --> 00:01:32.505
a baseline more efficiently.


00:01:32.505 --> 00:01:36.430
I find that for many
practical applications,


00:01:36.430 --> 00:01:41.170
a reasonable algorithm with
good data will often do


00:01:41.170 --> 00:01:43.360
just fine and will
in fact outperform


00:01:43.360 --> 00:01:46.725
a great algorithm with
not so good data.


00:01:46.725 --> 00:01:50.080
Don't obsess about taking
the algorithm that was just


00:01:50.080 --> 00:01:53.290
published in some
conference last week,


00:01:53.290 --> 00:01:55.495
that is the most
cutting edge algorithm,


00:01:55.495 --> 00:01:57.445
instead find
something reasonable,


00:01:57.445 --> 00:01:59.890
find a good open
source implementation


00:01:59.890 --> 00:02:02.715
and use that to
get going quickly.


00:02:02.715 --> 00:02:04.600
Because being able to get


00:02:04.600 --> 00:02:08.915
started on this first
step of this loop,


00:02:08.915 --> 00:02:11.000
can make you more efficient in


00:02:11.000 --> 00:02:13.430
iterating through more times,


00:02:13.430 --> 00:02:15.320
and that will help you get


00:02:15.320 --> 00:02:17.780
to good performance more quickly.


00:02:17.780 --> 00:02:21.940
Second question I have often been asked
, is, "Hey Andrew,


00:02:21.940 --> 00:02:25.459
do I need to take into account
deployment constraints


00:02:25.459 --> 00:02:29.000
such as compute constraints
when picking a model?"


00:02:29.000 --> 00:02:31.535
My answer is, yes you should take


00:02:31.535 --> 00:02:33.050
deployment constraints such as


00:02:33.050 --> 00:02:34.580
compute constraints into account,


00:02:34.580 --> 00:02:36.350
if the baseline is already


00:02:36.350 --> 00:02:39.410
established and you're
relatively confident


00:02:39.410 --> 00:02:41.475
that this project
will work and thus


00:02:41.475 --> 00:02:44.895
your goal is to build
and deploy a system.


00:02:44.895 --> 00:02:47.975
But if you have not yet even
established a baseline,


00:02:47.975 --> 00:02:50.240
or if you're not yet
sure if this project


00:02:50.240 --> 00:02:52.805
will work and be
worthy of deployment,


00:02:52.805 --> 00:02:54.320
then I will say no,


00:02:54.320 --> 00:02:55.700
or maybe not necessarily.


00:02:55.700 --> 00:02:57.800
If you are in a stage of


00:02:57.800 --> 00:03:00.680
the project where your first
goal is to just establish


00:03:00.680 --> 00:03:03.890
a baseline and determine
what is possible and if


00:03:03.890 --> 00:03:07.295
this project is even worth
pursuing for the long term,


00:03:07.295 --> 00:03:09.410
then it might be okay to ignore


00:03:09.410 --> 00:03:11.780
deployment constraints
and just find


00:03:11.780 --> 00:03:14.300
some open source
implementation and


00:03:14.300 --> 00:03:17.045
try it out to see what
might be possible,


00:03:17.045 --> 00:03:20.060
even if that open source
implementation is so


00:03:20.060 --> 00:03:22.130
computationally
intensive that you know


00:03:22.130 --> 00:03:24.620
you will never be
able to deploy that.


00:03:24.620 --> 00:03:26.600
Of course, no harm taking


00:03:26.600 --> 00:03:28.640
deployment constraints
into account as


00:03:28.640 --> 00:03:30.980
well at this phase
of the project,


00:03:30.980 --> 00:03:35.105
but it might also be
okay if you don't


00:03:35.105 --> 00:03:37.040
and focus on more


00:03:37.040 --> 00:03:40.070
efficiently establishing
the baseline first.


00:03:40.070 --> 00:03:42.350
Finally, when trying out


00:03:42.350 --> 00:03:44.585
a learning algorithm
for the first time,


00:03:44.585 --> 00:03:47.345
before running it
on all your data,


00:03:47.345 --> 00:03:48.830
I would urge you to run


00:03:48.830 --> 00:03:50.600
a few quick sanity checks


00:03:50.600 --> 00:03:52.795
for your code and your algorithm.


00:03:52.795 --> 00:03:56.090
For example, I will
usually try to


00:03:56.090 --> 00:03:59.270
overfit a very small
training dataset


00:03:59.270 --> 00:04:02.300
before spending hours
or sometimes even


00:04:02.300 --> 00:04:03.860
overnight or days training


00:04:03.860 --> 00:04:06.020
the algorithm on a large dataset.


00:04:06.020 --> 00:04:08.570
Maybe even try to
make sure you can


00:04:08.570 --> 00:04:11.000
fit one training example,


00:04:11.000 --> 00:04:14.045
especially, if the output
is a complex output.


00:04:14.045 --> 00:04:15.770
For example, I was once


00:04:15.770 --> 00:04:17.990
working on a speech
recognition system


00:04:17.990 --> 00:04:20.090
where the goal was
to input audio and


00:04:20.090 --> 00:04:22.220
have a learning algorithm
output a transcript.


00:04:22.220 --> 00:04:24.350
When I trained my algorithm on


00:04:24.350 --> 00:04:26.825
just one example, one audio clip,


00:04:26.825 --> 00:04:29.660
when I trained my speech
recognition system on


00:04:29.660 --> 00:04:32.480
just one audio clip
on the training set,


00:04:32.480 --> 00:04:34.205
which is just one audio clip,


00:04:34.205 --> 00:04:35.760
my system outputs this,


00:04:35.760 --> 00:04:37.160
it outputs space, space,


00:04:37.160 --> 00:04:38.720
space, space, space, space.


00:04:38.720 --> 00:04:42.200
Clearly it wasn't working
and because my speech system


00:04:42.200 --> 00:04:46.310
couldn't even accurately
transcribe one training example,


00:04:46.310 --> 00:04:48.710
there wasn't much
point to spending


00:04:48.710 --> 00:04:51.725
hours and hours training it
on a giant training set.


00:04:51.725 --> 00:04:53.840
Or for image segmentation,


00:04:53.840 --> 00:04:57.500
if your goal is to take
as input pictures like


00:04:57.500 --> 00:05:01.270
this and segment out
the cats in the image,


00:05:01.270 --> 00:05:04.370
then before spending
hours training


00:05:04.370 --> 00:05:08.119
your system on hundreds
or thousands of images,


00:05:08.119 --> 00:05:10.735
a worthy sanity check would be to


00:05:10.735 --> 00:05:14.100
feed it just one image
and see if it can


00:05:14.100 --> 00:05:17.569
at least overfit that
one training example


00:05:17.569 --> 00:05:20.745
before scaling up to
a larger dataset.


00:05:20.745 --> 00:05:22.820
The advantage of this is


00:05:22.820 --> 00:05:25.100
you may be able to
train your algorithm on


00:05:25.100 --> 00:05:27.590
one or a small handful
of examples in


00:05:27.590 --> 00:05:30.610
just minutes or
maybe even seconds


00:05:30.610 --> 00:05:33.830
and this lets you find
bugs much more quickly.


00:05:33.830 --> 00:05:37.020
Finally, for image
classification problems,


00:05:37.020 --> 00:05:39.960
even if you have 10,000


00:05:39.960 --> 00:05:42.110
images or 100,000 images


00:05:42.110 --> 00:05:44.300
or a million images
in your training set,


00:05:44.300 --> 00:05:46.220
it might be worthwhile to very


00:05:46.220 --> 00:05:48.290
quickly train your algorithm on


00:05:48.290 --> 00:05:52.100
a small subset of just
10 or maybe 100 images,


00:05:52.100 --> 00:05:53.660
because you can do that quickly.


00:05:53.660 --> 00:05:57.830
If your algorithm can't even
do well on 100 images, well,


00:05:57.830 --> 00:06:01.025
then it's clearly not going
to do well on 10,000 images,


00:06:01.025 --> 00:06:02.780
so this would be another useful


00:06:02.780 --> 00:06:04.820
sanity check for your code.


00:06:04.820 --> 00:06:08.180
Now, after you've trained
a machine learning model,


00:06:08.180 --> 00:06:09.920
after you've trained
your first model,


00:06:09.920 --> 00:06:11.960
one of the most
important things is,


00:06:11.960 --> 00:06:15.080
how do you carry out error
analysis to help you


00:06:15.080 --> 00:06:18.830
decide how to improve the
performance of your algorithm?


00:06:18.830 --> 00:06:20.990
Let's go on to the
next video to dive


00:06:20.990 --> 00:06:25.140
into error analysis and
performance auditing.
WEBVTT


00:00:00.740 --> 00:00:03.840
The first time you train a learning algorithm,


00:00:03.840 --> 00:00:09.340
you can almost guarantee that it
won't work not the first time out.


00:00:09.340 --> 00:00:12.918
So I think of the heart of
the machine learning development


00:00:12.918 --> 00:00:16.130
process as error analysis,
which if you do it well,


00:00:16.130 --> 00:00:19.561
I can tell you what's the most
efficient use of your time


00:00:19.561 --> 00:00:24.840
in terms of what you should do to improve
your learning algorithm's performance.


00:00:24.840 --> 00:00:26.810
Let's start with an example.


00:00:26.810 --> 00:00:31.761
Let me walk through an error analysis
example using speech recognition.


00:00:31.761 --> 00:00:37.139
When I'm carrying out error analysis,
this is pretty much what I would do myself


00:00:37.139 --> 00:00:42.141
in a spreadsheet to get a handle on
whether the errors of the speech system.


00:00:42.141 --> 00:00:47.007
You might listen to maybe 100
mislabeled examples from your


00:00:47.007 --> 00:00:49.780
dev set from the development set.


00:00:49.780 --> 00:00:54.484
So let's say the first example
was labeled with the ground truth


00:00:54.484 --> 00:00:57.030
label "stir fried lettuce recipe".


00:00:57.030 --> 00:01:00.570
But you're learning algorithm's prediction
was "stir fry letters recipe".


00:01:00.570 --> 00:01:03.082
If you have a couple of hypothesis,


00:01:03.082 --> 00:01:06.790
but what are the major types
of data in your dataset?


00:01:06.790 --> 00:01:12.110
Maybe you think some of the data has car
noise, some of the data has people noise.


00:01:12.110 --> 00:01:14.562
Then you can build a spreadsheet and


00:01:14.562 --> 00:01:19.490
I literally do this in a spreadsheet
with a couple of columns like this.


00:01:19.490 --> 00:01:23.679
And when you listen to this example,
if this example has car noise in


00:01:23.679 --> 00:01:26.692
the background,
you can then make a check mark or


00:01:26.692 --> 00:01:32.440
other annotation in your spreadsheet to
indicate that this example had car noise.


00:01:32.440 --> 00:01:34.904
Then you listen to the second example,


00:01:34.904 --> 00:01:39.236
maybe sweeten coffee caught
mis-transcribed as Swedish coffee and


00:01:39.236 --> 00:01:43.240
maybe this example had people
noise in the background.


00:01:43.240 --> 00:01:49.906
And maybe one example with sail away song
was mis transcribed sell away song and


00:01:49.906 --> 00:01:55.671
this again had people noise and
let's catch up with trans drivers.


00:01:55.671 --> 00:01:56.570
Let's catch up.


00:01:56.570 --> 00:02:00.540
And maybe this example had both
car noise and people noise.


00:02:00.540 --> 00:02:05.856
Note that these tags up on top don't
have to be mutually exclusive.


00:02:05.856 --> 00:02:10.090
During this process of error analysis,
as you listen to audio clips,


00:02:10.090 --> 00:02:12.990
you may come up with ideas for
additional tags.


00:02:12.990 --> 00:02:17.368
Let's say this for for example,
had a very low bandwidth connection and


00:02:17.368 --> 00:02:21.520
reflecting on the areas
you're spotting you remember.


00:02:21.520 --> 00:02:25.796
Maybe quite a few of the audio clips
have a low bandwidth connection,


00:02:25.796 --> 00:02:30.660
at this point you may decide to add a new
column to your spreadsheet with one more


00:02:30.660 --> 00:02:32.519
tag that says low bandwidth.


00:02:32.519 --> 00:02:36.589
And check that and maybe go back to
see if some of the other examples


00:02:36.589 --> 00:02:39.340
also had a low bandwidth connection.


00:02:39.340 --> 00:02:42.159
So even though I went through this example


00:02:42.159 --> 00:02:45.785
using a slide when I'm doing
error analysis myself,


00:02:45.785 --> 00:02:50.701
sometimes I literally fire up
a spreadsheet program like Google sheet or


00:02:50.701 --> 00:02:56.340
Excel or on a Mac, the numbers program and
do it like this in the spreadsheet.


00:02:56.340 --> 00:03:01.342
This process hopes you understand
whether the categories


00:03:01.342 --> 00:03:06.751
as denoted by tags that may be
the source of more of the errors and


00:03:06.751 --> 00:03:11.840
does may be worthy of further effort and
attention.


00:03:11.840 --> 00:03:16.212
Until now, error analysis has typically
been done via a manual process, say,


00:03:16.212 --> 00:03:20.140
in the Jupiter notebook or
tracking errors in spreadsheet.


00:03:20.140 --> 00:03:23.110
I still sometimes do it that way and
if that's how you're doing it too,


00:03:23.110 --> 00:03:24.140
that's fine.


00:03:24.140 --> 00:03:28.777
But there are also emerging MLOps
tools that making this process easier for


00:03:28.777 --> 00:03:29.650
developers.


00:03:29.650 --> 00:03:34.696
For example, when my team Landing AI works
on computer vision applications, the whole


00:03:34.696 --> 00:03:40.140
team now uses LandingLens, which makes
this much easier than the spreadsheet.


00:03:40.140 --> 00:03:44.156
You've heard me say that training
a model is an iterative process,


00:03:44.156 --> 00:03:46.830
deploying a model is an iterative process.


00:03:46.830 --> 00:03:52.500
Maybe it should come as no surprise
that error analysis is also an iterative


00:03:52.500 --> 00:03:58.350
process where what a typical process would
be is you might examine and tag some


00:03:58.350 --> 00:04:04.210
set of examples with an initial set of
tags such as car noise and people noise.


00:04:04.210 --> 00:04:08.447
And based on examining this
initial set of examples,


00:04:08.447 --> 00:04:13.070
you may come back and
say you want to propose some new tags.


00:04:13.070 --> 00:04:19.940
with the new tags, you can then go back
to examine and tag even more examples.


00:04:19.940 --> 00:04:25.300
Let me step through a few other
examples of what such tags could be.


00:04:25.300 --> 00:04:26.900
Take visual inspection.


00:04:26.900 --> 00:04:30.240
You know, the problem of
finding defects in smart phones.


00:04:30.240 --> 00:04:32.935
Some of the tags could be
specific class labels,


00:04:32.935 --> 00:04:36.431
such as this is going to have a scratch or
does evident and so on.


00:04:36.431 --> 00:04:41.760
So it's fine if some of these tags
are associated with specific class labels


00:04:41.760 --> 00:04:45.090
y or
some of the tax could be image properties.


00:04:45.090 --> 00:04:47.340
Is this picture of the phone blurry?


00:04:47.340 --> 00:04:49.940
Is it against the dark background or
a light background?


00:04:49.940 --> 00:04:53.260
Is there a unwanted
reflection in this picture?


00:04:53.260 --> 00:04:57.840
The tags could also come from
other forms of metadata.


00:04:57.840 --> 00:04:59.151
What is the film model?


00:04:59.151 --> 00:05:03.111
What is the factory which is
the manufacturing line that captured


00:05:03.111 --> 00:05:04.420
the specific image?


00:05:04.420 --> 00:05:07.961
And the goal of this type of process
where you come over tag label.


00:05:07.961 --> 00:05:13.061
More data come over tag, is to try to
come up with a few categories where you


00:05:13.061 --> 00:05:18.003
could productively improve the algorithm
such as in our earlier speech


00:05:18.003 --> 00:05:22.810
example deciding to work on speech
with car noise in the background.


00:05:22.810 --> 00:05:27.634
Let me step through just one more example,
product recommendations for


00:05:27.634 --> 00:05:29.458
an online e commerce site.


00:05:29.458 --> 00:05:34.003
You might look at what products
a system is recommending to users and


00:05:34.003 --> 00:05:38.640
find the clearly incorrect or
irrelevant recommendations.


00:05:38.640 --> 00:05:43.480
And try to figure out if there
are specific user demographics such


00:05:43.480 --> 00:05:48.232
as are we really badly recommending
products to younger women or


00:05:48.232 --> 00:05:50.890
to older men or to something else?


00:05:50.890 --> 00:05:53.763
Or are there specific product features or


00:05:53.763 --> 00:05:59.438
specific product categories where
the recommendations are particularly poor.


00:05:59.438 --> 00:06:03.832
And by alternatively brainstorming and
applying such tags,


00:06:03.832 --> 00:06:06.956
you can hopefully come
up with a few ideas for


00:06:06.956 --> 00:06:12.550
categories of data that we're trying
to improve your algorithm's performance on.


00:06:12.550 --> 00:06:18.273
As you go through these different tags
here are some useful numbers to look at.


00:06:18.273 --> 00:06:21.540
First what fraction of
errors have that tag?


00:06:21.540 --> 00:06:26.165
For example,
if you listen to 100 audio clips and


00:06:26.165 --> 00:06:31.634
find that 12% of them were
labeled with the car noise type,


00:06:31.634 --> 00:06:37.749
then that gives you a sense of how
important is it to work on car noise.


00:06:37.749 --> 00:06:43.425
It tells you also that even if you
fix all of the car noise issues,


00:06:43.425 --> 00:06:50.440
the performance may improve only by 12%,
which is actually not bad.


00:06:50.440 --> 00:06:56.701
Or you can ask all the data with that
tag what fraction is misclassified?


00:06:56.701 --> 00:07:02.251
So far we've only talked about tagging the
mislabeled examples for time efficiency.


00:07:02.251 --> 00:07:05.681
You might focus your attention
on tagging the mislabeled,


00:07:05.681 --> 00:07:07.480
the misclassified examples.


00:07:07.480 --> 00:07:11.397
But if this tag you can apply
to both correctly labeled and


00:07:11.397 --> 00:07:16.375
two mislabeled examples, then you can
ask of all the data of that tag,


00:07:16.375 --> 00:07:18.840
what fraction is misclassified?


00:07:18.840 --> 00:07:23.907
So for example,
if you find that of all the data with


00:07:23.907 --> 00:07:28.540
car noise, 18% of it is mistranscribed,


00:07:28.540 --> 00:07:32.767
then that tells you that
the performance on data with this


00:07:32.767 --> 00:07:36.466
type of tag has only a certain
level of accuracy and


00:07:36.466 --> 00:07:40.990
tells you how hard these examples
with car noise really are.


00:07:40.990 --> 00:07:44.600
You might also ask what fraction
of all the data has that tag.


00:07:44.600 --> 00:07:49.448
This tells you how important relative to
your entire data set are examples with


00:07:49.448 --> 00:07:50.113
that tag.


00:07:50.113 --> 00:07:53.520
So what fraction of your
entire data set has car noise?


00:07:53.520 --> 00:07:55.638
And then lastly, how much room for


00:07:55.638 --> 00:07:59.040
improvement is there
on data with that tag?


00:07:59.040 --> 00:08:03.889
And one example that you've already
seen for how to do this analysis


00:08:03.889 --> 00:08:09.240
is to measure human level
performance on data with that tag.


00:08:09.240 --> 00:08:12.108
So by brainstorming different tags,


00:08:12.108 --> 00:08:16.321
you can segment your data
into different categories and


00:08:16.321 --> 00:08:22.540
then use questions like these to try to
decide what to prioritize working on.


00:08:22.540 --> 00:08:26.461
Let's dive more deeply into an example
of doing this in the next video.
WEBVTT


00:00:00.000 --> 00:00:03.360
In the last video, you learned
about brainstorming and


00:00:03.360 --> 00:00:06.299
tagging your data with
different attributes.


00:00:06.299 --> 00:00:08.190
Let's see how you
can use this to


00:00:08.190 --> 00:00:10.695
prioritize where to
focus your attention.


00:00:10.695 --> 00:00:13.050
Here's the example we
had previously with


00:00:13.050 --> 00:00:16.650
four tags and the accuracy
of the algorithm,


00:00:16.650 --> 00:00:19.950
human level performance
and what's the gap between


00:00:19.950 --> 00:00:23.340
the current accuracy and
human level performance.


00:00:23.340 --> 00:00:25.080
Rather than deciding to work on


00:00:25.080 --> 00:00:28.365
car noise because the
gap to HLP is bigger,


00:00:28.365 --> 00:00:31.305
one other useful
factor to look at


00:00:31.305 --> 00:00:34.620
is what's the percentage
of data with that tag?


00:00:34.620 --> 00:00:39.465
Let's say that 60 percent of
your data is clean speech,


00:00:39.465 --> 00:00:44.115
four percent is data
with car noise,


00:00:44.115 --> 00:00:46.545
30 percent has people noise,


00:00:46.545 --> 00:00:49.920
and six percent is
low bandwidth audio.


00:00:49.920 --> 00:00:54.440
This tells us that if we
could take clean speech


00:00:54.440 --> 00:00:56.000
and raise our accuracy from


00:00:56.000 --> 00:00:59.270
94-95 percent on all
the clean speech,


00:00:59.270 --> 00:01:01.400
then multiplying
one percent with


00:01:01.400 --> 00:01:03.905
60 percent just tells us that,


00:01:03.905 --> 00:01:06.470
if we can improve our
performance on clean speech,


00:01:06.470 --> 00:01:07.940
the human level performance,


00:01:07.940 --> 00:01:10.520
our overall speech
system would be


00:01:10.520 --> 00:01:13.309
0.6 percent more accurate,


00:01:13.309 --> 00:01:15.020
because we would do one percent


00:01:15.020 --> 00:01:17.795
better on 60 percent
of the data.


00:01:17.795 --> 00:01:21.910
This will raise average
accuracy by 0.6 percent.


00:01:21.910 --> 00:01:23.995
On the car noise,


00:01:23.995 --> 00:01:26.015
if we can improve


00:01:26.015 --> 00:01:29.085
the performance by four percent


00:01:29.085 --> 00:01:31.305
on four percent of the data,


00:01:31.305 --> 00:01:34.930
multiplying that
out, that gives us a


00:01:35.000 --> 00:01:40.520
0.16 percent improvement and
multiply these out as well,


00:01:40.520 --> 00:01:43.700
we get 0.6 percent and well,


00:01:43.700 --> 00:01:45.470
this is essentially zero percent


00:01:45.470 --> 00:01:47.425
because you can't
make that any better.


00:01:47.425 --> 00:01:50.510
Whereas previously, we
had said there's a lot of


00:01:50.510 --> 00:01:53.110
room for improvement
in car noise,


00:01:53.110 --> 00:01:55.760
in this slightly
richer analysis,


00:01:55.760 --> 00:01:58.235
we see that because people noise


00:01:58.235 --> 00:02:01.265
accounts for such a large
fraction of the data,


00:02:01.265 --> 00:02:06.995
it may be more worthwhile to
work on either people noise,


00:02:06.995 --> 00:02:09.005
or maybe on clean speech


00:02:09.005 --> 00:02:11.810
because there's actually
larger potential for


00:02:11.810 --> 00:02:14.135
improvements in both of those


00:02:14.135 --> 00:02:17.135
than for speech with car noise.


00:02:17.135 --> 00:02:21.485
To summarize, when
prioritizing what to work on,


00:02:21.485 --> 00:02:23.150
you might decide on


00:02:23.150 --> 00:02:26.240
the most important categories
to work on based on,


00:02:26.240 --> 00:02:29.135
how much room for improvement
there is, such as,


00:02:29.135 --> 00:02:31.010
compared to human-level
performance


00:02:31.010 --> 00:02:33.520
or according to some
baseline comparison.


00:02:33.520 --> 00:02:35.690
How frequently does
that category appear?


00:02:35.690 --> 00:02:38.240
You can also take
into account how easy


00:02:38.240 --> 00:02:40.835
it is to improve accuracy
in that category.


00:02:40.835 --> 00:02:43.310
For example, if you
have some ideas for how


00:02:43.310 --> 00:02:45.845
to improve the accuracy
of speech with car noise,


00:02:45.845 --> 00:02:47.510
maybe your data augmentation,


00:02:47.510 --> 00:02:51.680
that might cause you to
prioritize that category more


00:02:51.680 --> 00:02:54.410
highly than some other
category where you just


00:02:54.410 --> 00:02:57.180
don't have as many ideas for
how to improve the system.


00:02:57.180 --> 00:02:59.720
Then finally, how important it


00:02:59.720 --> 00:03:02.750
is to improve performance
on that category.


00:03:02.750 --> 00:03:04.670
For example, you may decide


00:03:04.670 --> 00:03:06.620
that improving performance with


00:03:06.620 --> 00:03:08.870
car noise is
especially important


00:03:08.870 --> 00:03:11.525
because when you're driving,


00:03:11.525 --> 00:03:15.860
you have a stronger
desire to do search,


00:03:15.860 --> 00:03:19.790
especially search on maps
and find addresses without


00:03:19.790 --> 00:03:21.740
needing to use your
hands if your hands


00:03:21.740 --> 00:03:24.455
are supposed to be holding
the steering wheel.


00:03:24.455 --> 00:03:26.855
There is no mathematical formula


00:03:26.855 --> 00:03:28.670
that will tell you
what to work on.


00:03:28.670 --> 00:03:31.070
But by looking
at these factors,


00:03:31.070 --> 00:03:35.220
I hope you'd be able to make
more fruitful decisions.


00:03:35.220 --> 00:03:37.684
Once you've decided that
there's a category,


00:03:37.684 --> 00:03:39.620
or maybe a few
categories where you


00:03:39.620 --> 00:03:42.065
want to improve the
average performance,


00:03:42.065 --> 00:03:44.990
one fruitful approach
is to consider adding


00:03:44.990 --> 00:03:48.710
data or improving the quality
of that data for that one,


00:03:48.710 --> 00:03:51.635
or maybe a small
handful of categories.


00:03:51.635 --> 00:03:53.840
For example, if you
want to improve


00:03:53.840 --> 00:03:57.005
performance on speech
with car noise,


00:03:57.005 --> 00:04:01.405
you might go out and collect
more data with car noise.


00:04:01.405 --> 00:04:03.620
Or if you have a way of using


00:04:03.620 --> 00:04:06.980
data augmentation to get more
data from data category,


00:04:06.980 --> 00:04:08.750
that will be another way to


00:04:08.750 --> 00:04:10.520
improve your algorithm's
performance.


00:04:10.520 --> 00:04:14.315
One topic that we'll
discuss next week is how to


00:04:14.315 --> 00:04:18.220
improve label accuracy
or data quality.


00:04:18.220 --> 00:04:20.720
You'll learn more about
this when we talk about


00:04:20.720 --> 00:04:24.235
the data phase of the machine
learning project lifecycle.


00:04:24.235 --> 00:04:25.805
In machine learning, we


00:04:25.805 --> 00:04:28.160
always would like
to have more data,


00:04:28.160 --> 00:04:31.820
but going out to collect
more data generically,


00:04:31.820 --> 00:04:34.700
can be very time-consuming
and expensive.


00:04:34.700 --> 00:04:37.505
By carrying out an
analysis like this,


00:04:37.505 --> 00:04:39.410
when you are then going through


00:04:39.410 --> 00:04:41.000
this iterative process of


00:04:41.000 --> 00:04:42.845
improving your
learning algorithm,


00:04:42.845 --> 00:04:44.960
you can be much more focused in


00:04:44.960 --> 00:04:47.995
exactly what types
of data you collect.


00:04:47.995 --> 00:04:50.030
Because if you decide to collect


00:04:50.030 --> 00:04:53.150
more data with car noise
or maybe people noise,


00:04:53.150 --> 00:04:55.610
you can be much more specific in


00:04:55.610 --> 00:04:58.550
going out to collect
more of just that data


00:04:58.550 --> 00:05:00.260
or using data augmentation


00:05:00.260 --> 00:05:01.970
without wasting time
trying to collect


00:05:01.970 --> 00:05:05.755
more data from a low bandwidth
cell phone connection.


00:05:05.755 --> 00:05:09.890
This focus on
improving your data on


00:05:09.890 --> 00:05:11.900
the tags that you
have determined are


00:05:11.900 --> 00:05:14.375
most fruitful for
you to work on,


00:05:14.375 --> 00:05:16.490
that can help you be much more


00:05:16.490 --> 00:05:18.020
efficient in how you


00:05:18.020 --> 00:05:20.030
improve your learning
algorithm's performance.


00:05:20.030 --> 00:05:22.759
I found this type of
error analysis procedure


00:05:22.759 --> 00:05:25.160
very useful for
many of my projects


00:05:25.160 --> 00:05:28.010
and I hope it will
help you too in


00:05:28.010 --> 00:05:31.345
building production-ready
machine learning systems.


00:05:31.345 --> 00:05:34.430
Next, one of the most
common challenges


00:05:34.430 --> 00:05:37.250
we run into is skewed datasets.


00:05:37.250 --> 00:05:39.470
Let's go on to the next
video to go through


00:05:39.470 --> 00:05:43.800
some techniques for
managing skewed datasets.
WEBVTT


00:00:00.000 --> 00:00:03.360
In the last video, you learned
about brainstorming and


00:00:03.360 --> 00:00:06.299
tagging your data with
different attributes.


00:00:06.299 --> 00:00:08.190
Let's see how you
can use this to


00:00:08.190 --> 00:00:10.695
prioritize where to
focus your attention.


00:00:10.695 --> 00:00:13.050
Here's the example we
had previously with


00:00:13.050 --> 00:00:16.650
four tags and the accuracy
of the algorithm,


00:00:16.650 --> 00:00:19.950
human level performance
and what's the gap between


00:00:19.950 --> 00:00:23.340
the current accuracy and
human level performance.


00:00:23.340 --> 00:00:25.080
Rather than deciding to work on


00:00:25.080 --> 00:00:28.365
car noise because the
gap to HLP is bigger,


00:00:28.365 --> 00:00:31.305
one other useful
factor to look at


00:00:31.305 --> 00:00:34.620
is what's the percentage
of data with that tag?


00:00:34.620 --> 00:00:39.465
Let's say that 60 percent of
your data is clean speech,


00:00:39.465 --> 00:00:44.115
four percent is data
with car noise,


00:00:44.115 --> 00:00:46.545
30 percent has people noise,


00:00:46.545 --> 00:00:49.920
and six percent is
low bandwidth audio.


00:00:49.920 --> 00:00:54.440
This tells us that if we
could take clean speech


00:00:54.440 --> 00:00:56.000
and raise our accuracy from


00:00:56.000 --> 00:00:59.270
94-95 percent on all
the clean speech,


00:00:59.270 --> 00:01:01.400
then multiplying
one percent with


00:01:01.400 --> 00:01:03.905
60 percent just tells us that,


00:01:03.905 --> 00:01:06.470
if we can improve our
performance on clean speech,


00:01:06.470 --> 00:01:07.940
the human level performance,


00:01:07.940 --> 00:01:10.520
our overall speech
system would be


00:01:10.520 --> 00:01:13.309
0.6 percent more accurate,


00:01:13.309 --> 00:01:15.020
because we would do one percent


00:01:15.020 --> 00:01:17.795
better on 60 percent
of the data.


00:01:17.795 --> 00:01:21.910
This will raise average
accuracy by 0.6 percent.


00:01:21.910 --> 00:01:23.995
On the car noise,


00:01:23.995 --> 00:01:26.015
if we can improve


00:01:26.015 --> 00:01:29.085
the performance by four percent


00:01:29.085 --> 00:01:31.305
on four percent of the data,


00:01:31.305 --> 00:01:34.930
multiplying that
out, that gives us a


00:01:35.000 --> 00:01:40.520
0.16 percent improvement and
multiply these out as well,


00:01:40.520 --> 00:01:43.700
we get 0.6 percent and well,


00:01:43.700 --> 00:01:45.470
this is essentially zero percent


00:01:45.470 --> 00:01:47.425
because you can't
make that any better.


00:01:47.425 --> 00:01:50.510
Whereas previously, we
had said there's a lot of


00:01:50.510 --> 00:01:53.110
room for improvement
in car noise,


00:01:53.110 --> 00:01:55.760
in this slightly
richer analysis,


00:01:55.760 --> 00:01:58.235
we see that because people noise


00:01:58.235 --> 00:02:01.265
accounts for such a large
fraction of the data,


00:02:01.265 --> 00:02:06.995
it may be more worthwhile to
work on either people noise,


00:02:06.995 --> 00:02:09.005
or maybe on clean speech


00:02:09.005 --> 00:02:11.810
because there's actually
larger potential for


00:02:11.810 --> 00:02:14.135
improvements in both of those


00:02:14.135 --> 00:02:17.135
than for speech with car noise.


00:02:17.135 --> 00:02:21.485
To summarize, when
prioritizing what to work on,


00:02:21.485 --> 00:02:23.150
you might decide on


00:02:23.150 --> 00:02:26.240
the most important categories
to work on based on,


00:02:26.240 --> 00:02:29.135
how much room for improvement
there is, such as,


00:02:29.135 --> 00:02:31.010
compared to human-level
performance


00:02:31.010 --> 00:02:33.520
or according to some
baseline comparison.


00:02:33.520 --> 00:02:35.690
How frequently does
that category appear?


00:02:35.690 --> 00:02:38.240
You can also take
into account how easy


00:02:38.240 --> 00:02:40.835
it is to improve accuracy
in that category.


00:02:40.835 --> 00:02:43.310
For example, if you
have some ideas for how


00:02:43.310 --> 00:02:45.845
to improve the accuracy
of speech with car noise,


00:02:45.845 --> 00:02:47.510
maybe your data augmentation,


00:02:47.510 --> 00:02:51.680
that might cause you to
prioritize that category more


00:02:51.680 --> 00:02:54.410
highly than some other
category where you just


00:02:54.410 --> 00:02:57.180
don't have as many ideas for
how to improve the system.


00:02:57.180 --> 00:02:59.720
Then finally, how important it


00:02:59.720 --> 00:03:02.750
is to improve performance
on that category.


00:03:02.750 --> 00:03:04.670
For example, you may decide


00:03:04.670 --> 00:03:06.620
that improving performance with


00:03:06.620 --> 00:03:08.870
car noise is
especially important


00:03:08.870 --> 00:03:11.525
because when you're driving,


00:03:11.525 --> 00:03:15.860
you have a stronger
desire to do search,


00:03:15.860 --> 00:03:19.790
especially search on maps
and find addresses without


00:03:19.790 --> 00:03:21.740
needing to use your
hands if your hands


00:03:21.740 --> 00:03:24.455
are supposed to be holding
the steering wheel.


00:03:24.455 --> 00:03:26.855
There is no mathematical formula


00:03:26.855 --> 00:03:28.670
that will tell you
what to work on.


00:03:28.670 --> 00:03:31.070
But by looking
at these factors,


00:03:31.070 --> 00:03:35.220
I hope you'd be able to make
more fruitful decisions.


00:03:35.220 --> 00:03:37.684
Once you've decided that
there's a category,


00:03:37.684 --> 00:03:39.620
or maybe a few
categories where you


00:03:39.620 --> 00:03:42.065
want to improve the
average performance,


00:03:42.065 --> 00:03:44.990
one fruitful approach
is to consider adding


00:03:44.990 --> 00:03:48.710
data or improving the quality
of that data for that one,


00:03:48.710 --> 00:03:51.635
or maybe a small
handful of categories.


00:03:51.635 --> 00:03:53.840
For example, if you
want to improve


00:03:53.840 --> 00:03:57.005
performance on speech
with car noise,


00:03:57.005 --> 00:04:01.405
you might go out and collect
more data with car noise.


00:04:01.405 --> 00:04:03.620
Or if you have a way of using


00:04:03.620 --> 00:04:06.980
data augmentation to get more
data from data category,


00:04:06.980 --> 00:04:08.750
that will be another way to


00:04:08.750 --> 00:04:10.520
improve your algorithm's
performance.


00:04:10.520 --> 00:04:14.315
One topic that we'll
discuss next week is how to


00:04:14.315 --> 00:04:18.220
improve label accuracy
or data quality.


00:04:18.220 --> 00:04:20.720
You'll learn more about
this when we talk about


00:04:20.720 --> 00:04:24.235
the data phase of the machine
learning project lifecycle.


00:04:24.235 --> 00:04:25.805
In machine learning, we


00:04:25.805 --> 00:04:28.160
always would like
to have more data,


00:04:28.160 --> 00:04:31.820
but going out to collect
more data generically,


00:04:31.820 --> 00:04:34.700
can be very time-consuming
and expensive.


00:04:34.700 --> 00:04:37.505
By carrying out an
analysis like this,


00:04:37.505 --> 00:04:39.410
when you are then going through


00:04:39.410 --> 00:04:41.000
this iterative process of


00:04:41.000 --> 00:04:42.845
improving your
learning algorithm,


00:04:42.845 --> 00:04:44.960
you can be much more focused in


00:04:44.960 --> 00:04:47.995
exactly what types
of data you collect.


00:04:47.995 --> 00:04:50.030
Because if you decide to collect


00:04:50.030 --> 00:04:53.150
more data with car noise
or maybe people noise,


00:04:53.150 --> 00:04:55.610
you can be much more specific in


00:04:55.610 --> 00:04:58.550
going out to collect
more of just that data


00:04:58.550 --> 00:05:00.260
or using data augmentation


00:05:00.260 --> 00:05:01.970
without wasting time
trying to collect


00:05:01.970 --> 00:05:05.755
more data from a low bandwidth
cell phone connection.


00:05:05.755 --> 00:05:09.890
This focus on
improving your data on


00:05:09.890 --> 00:05:11.900
the tags that you
have determined are


00:05:11.900 --> 00:05:14.375
most fruitful for
you to work on,


00:05:14.375 --> 00:05:16.490
that can help you be much more


00:05:16.490 --> 00:05:18.020
efficient in how you


00:05:18.020 --> 00:05:20.030
improve your learning
algorithm's performance.


00:05:20.030 --> 00:05:22.759
I found this type of
error analysis procedure


00:05:22.759 --> 00:05:25.160
very useful for
many of my projects


00:05:25.160 --> 00:05:28.010
and I hope it will
help you too in


00:05:28.010 --> 00:05:31.345
building production-ready
machine learning systems.


00:05:31.345 --> 00:05:34.430
Next, one of the most
common challenges


00:05:34.430 --> 00:05:37.250
we run into is skewed datasets.


00:05:37.250 --> 00:05:39.470
Let's go on to the next
video to go through


00:05:39.470 --> 00:05:43.800
some techniques for
managing skewed datasets.
WEBVTT


00:00:00.000 --> 00:00:02.190
Data sets where the ratio of


00:00:02.190 --> 00:00:04.049
positive to negative examples


00:00:04.049 --> 00:00:08.165
is very far from 50-50 are
called skewed data sets.


00:00:08.165 --> 00:00:11.655
Let's look at some special
techniques for handling them.


00:00:11.655 --> 00:00:14.265
Let me start with a
manufacturing example.


00:00:14.265 --> 00:00:18.540
If a manufacturing company
makes smartphones,


00:00:18.540 --> 00:00:22.740
hopefully, the vast majority
of them are not defective.


00:00:22.740 --> 00:00:27.165
If 99.7 percent have no
defect and are labeled


00:00:27.165 --> 00:00:29.085
y equals 0 and


00:00:29.085 --> 00:00:32.760
only a small fraction
is labeled y equals 1,


00:00:32.760 --> 00:00:36.705
then print 0,


00:00:36.705 --> 00:00:39.810
which is not a very impressive
learning algorithm.


00:00:39.810 --> 00:00:43.595
We achieve 99.7 percent accuracy.


00:00:43.595 --> 00:00:46.120
For medical diagnosis, which was


00:00:46.120 --> 00:00:49.375
the example we went through
in an earlier video,


00:00:49.375 --> 00:00:52.340
if 99 percent of patients
don't have a disease,


00:00:52.340 --> 00:00:56.290
then an algorithm that predicts
no one ever has a disease


00:00:56.290 --> 00:01:00.845
will have 99 percent accuracy
or speech recognition.


00:01:00.845 --> 00:01:04.389
If you're building a system
for wake word detection,


00:01:04.389 --> 00:01:06.730
sometimes also called
trigger word detection,


00:01:06.730 --> 00:01:10.060
these are systems that
listen and see if you say


00:01:10.060 --> 00:01:14.355
a special word like Alexa
or Okay Google or Hey Zoe,


00:01:14.355 --> 00:01:17.855
most of the time that special
wake word or trigger word


00:01:17.855 --> 00:01:21.585
is not being spoken by anyone
at that moment in time.


00:01:21.585 --> 00:01:24.875
When I had built wake
word detection systems,


00:01:24.875 --> 00:01:27.460
the data sets were actually quite
skewed. One of the data


00:01:27.460 --> 00:01:30.750
sets I used had 96.7


00:01:30.750 --> 00:01:32.700
percent negative examples and


00:01:32.700 --> 00:01:35.080
3.3 percent positive examples.


00:01:35.080 --> 00:01:38.320
When you have a very
skewed data set like this,


00:01:38.320 --> 00:01:42.580
low accuracy is not
that useful a metric to


00:01:42.580 --> 00:01:47.260
look at because print zero
can get very high accuracy.


00:01:47.260 --> 00:01:49.750
Instead, it's more useful to


00:01:49.750 --> 00:01:52.420
build something called
the confusion matrix.


00:01:52.420 --> 00:01:55.660
A confusion matrix
is a matrix where


00:01:55.660 --> 00:01:59.400
one axis is labeled
with the actual label,


00:01:59.400 --> 00:02:01.245
is the ground truth label,


00:02:01.245 --> 00:02:04.140
y equals 0 or y equals 1


00:02:04.140 --> 00:02:08.110
and whose other axis is
labeled with the prediction.


00:02:08.110 --> 00:02:10.944
Was your learning
algorithms prediction


00:02:10.944 --> 00:02:15.180
y equals 0 or y equals 1?


00:02:15.180 --> 00:02:18.720
If you're building
a confusion matrix,


00:02:18.720 --> 00:02:23.170
you fill in with each
of these four cells,


00:02:23.170 --> 00:02:24.570
the total number of


00:02:24.570 --> 00:02:26.940
examples say the
number of examples in


00:02:26.940 --> 00:02:28.830
your dev set in
your development


00:02:28.830 --> 00:02:32.285
set to fell into each
of these four buckets.


00:02:32.285 --> 00:02:38.460
Let's say that 905 examples
in your development set had


00:02:38.460 --> 00:02:40.755
a ground-truth
label of y equals 0


00:02:40.755 --> 00:02:44.295
and then you might
write 905 there.


00:02:44.295 --> 00:02:46.020
These examples are called


00:02:46.020 --> 00:02:48.360
true negatives because they were


00:02:48.360 --> 00:02:51.090
actually negative
and your algorithm


00:02:51.090 --> 00:02:52.500
predicted they were negative.


00:02:52.500 --> 00:02:55.740
Next, lets fill in
the true positives,


00:02:55.740 --> 00:02:57.965
which are the examples where
the actual ground truth of the


00:02:57.965 --> 00:03:00.385
label is one and the
prediction is one,


00:03:00.385 --> 00:03:05.145
maybe there are 68 of
them, true positives.


00:03:05.145 --> 00:03:07.200
The false negatives are


00:03:07.200 --> 00:03:11.205
the examples where your
algorithm thought it was negative,


00:03:11.205 --> 00:03:13.350
but it was not.


00:03:13.350 --> 00:03:16.365
The actual label is positive,


00:03:16.365 --> 00:03:19.290
these are false negatives.


00:03:19.290 --> 00:03:21.895
The 18 of that and lastly,


00:03:21.895 --> 00:03:23.880
false positives
are the ones where


00:03:23.880 --> 00:03:25.920
your algorithm
thought it was positive,


00:03:25.920 --> 00:03:31.920
but that turned out to be
false, nine false positives.


00:03:31.920 --> 00:03:36.565
The precision of
a learning algorithm,


00:03:36.565 --> 00:03:39.615
if I sum up over the columns,


00:03:39.615 --> 00:03:48.360
905 plus 9 is 914 and
18 plus 68 is 86.


00:03:48.360 --> 00:03:52.740
This is indeed a pretty
skewed data set where out of


00:03:52.740 --> 00:03:55.020
1000 examples there were


00:03:55.020 --> 00:04:00.495
940 negative examples and
just 86 positive examples,


00:04:00.495 --> 00:04:05.475
8.6 percent positive,
91.4 percent negative.


00:04:05.475 --> 00:04:10.840
The precision of your
learning algorithm


00:04:10.840 --> 00:04:12.730
is defined as follows,


00:04:12.730 --> 00:04:15.070
it asks of all the examples


00:04:15.070 --> 00:04:18.205
that the algorithm thought
were positive examples,


00:04:18.205 --> 00:04:21.005
what fraction did they get?


00:04:21.005 --> 00:04:26.355
Precision is defined
as true positives


00:04:26.355 --> 00:04:31.935
divided by true positives
plus false positives.


00:04:31.935 --> 00:04:36.930
In other words, it
looks at this row.


00:04:36.930 --> 00:04:39.080
Of all the examples that


00:04:39.080 --> 00:04:41.165
your algorithm thought
had a label of one,


00:04:41.165 --> 00:04:44.030
which is 68 plus 9 of them,


00:04:44.030 --> 00:04:46.610
68 of them were actually right.


00:04:46.610 --> 00:04:53.505
The precision is
68 over 68 plus 9,


00:04:53.505 --> 00:04:56.655
which is 88.3 percent.


00:04:56.655 --> 00:05:02.430
In contrast, the recall
asks: Of all the examples


00:05:02.430 --> 00:05:05.310
that were actually positive,


00:05:05.310 --> 00:05:08.730
what fraction did your
algorithm get right?


00:05:08.730 --> 00:05:12.795
Recall is defined
as true positives


00:05:12.795 --> 00:05:17.250
divided by true positives
plus false negatives,


00:05:17.250 --> 00:05:22.859
which in this case is
68 over 68 plus 18,


00:05:22.859 --> 00:05:28.140
which is 79.1 percent.


00:05:28.140 --> 00:05:33.570
The metrics are precision and
recall are more useful than


00:05:33.570 --> 00:05:37.320
raw accuracy when it
comes to evaluating


00:05:37.320 --> 00:05:39.180
the performance of
learning algorithms


00:05:39.180 --> 00:05:41.315
on very skewed data sets.


00:05:41.315 --> 00:05:43.230
Let's see what happens if


00:05:43.230 --> 00:05:46.635
your learning algorithm
outputs zero all the time.


00:05:46.635 --> 00:05:50.565
It turns out it won't
do very well on recall.


00:05:50.565 --> 00:05:53.175
Taking this example
of where we had


00:05:53.175 --> 00:05:57.824
914 negative examples and
86 positive examples,


00:05:57.824 --> 00:06:01.390
if the algorithm outputs
zero all the time.


00:06:01.390 --> 00:06:04.520
This is what the confusion
matrix will look like,


00:06:04.520 --> 00:06:08.525
914 times it'll output zero
with a grand total of zero,


00:06:08.525 --> 00:06:10.305
and 86 times it'll output


00:06:10.305 --> 00:06:12.810
zero with a ground truth of one.


00:06:12.810 --> 00:06:16.670
Precision is true
positives divided


00:06:16.670 --> 00:06:20.195
by true positives
plus false positives,


00:06:20.195 --> 00:06:22.730
which in this case
turns out to be zero


00:06:22.730 --> 00:06:26.825
over zero plus zero,


00:06:26.825 --> 00:06:28.890
which is not defined,


00:06:28.890 --> 00:06:30.845
and unless your
algorithm actually


00:06:30.845 --> 00:06:34.290
outputs no positive
labels at all,


00:06:34.290 --> 00:06:36.105
you get some of the number that


00:06:36.105 --> 00:06:38.130
hopefully isn't zero over zero.


00:06:38.130 --> 00:06:40.780
But more importantly,
if you look at recall,


00:06:40.780 --> 00:06:42.935
which is true positives over


00:06:42.935 --> 00:06:45.815
true positives plus
false negatives,


00:06:45.815 --> 00:06:53.975
this turns out to be
zero over zero plus 86,


00:06:53.975 --> 00:06:57.245
which is zero percent,


00:06:57.245 --> 00:07:03.050
and so the 0.0 algorithm
achieves zero percent recall,


00:07:03.050 --> 00:07:06.465
which gives you an easy way
to flag that this is not


00:07:06.465 --> 00:07:10.095
detecting any useful,
positive examples.


00:07:10.095 --> 00:07:12.785
The learning algorithm
with some precision,


00:07:12.785 --> 00:07:15.310
even the high value of
precision is not that


00:07:15.310 --> 00:07:19.250
useful usually if this
recall is so low.


00:07:19.250 --> 00:07:22.550
The standard metrics when
I look at when comparing


00:07:22.550 --> 00:07:24.855
different models on
skewed data sets


00:07:24.855 --> 00:07:27.600
are precision and recall.


00:07:27.600 --> 00:07:31.070
Where looking at these
numbers helps you figure out


00:07:31.070 --> 00:07:35.089
and of all the examples that
are truly positive examples,


00:07:35.089 --> 00:07:38.465
what fraction did the
algorithm manage to catch?


00:07:38.465 --> 00:07:40.970
Sometimes you have one model with


00:07:40.970 --> 00:07:43.610
a better recall and


00:07:43.610 --> 00:07:46.595
a different model with
a better precision.


00:07:46.595 --> 00:07:49.485
How do you compare
two different models?


00:07:49.485 --> 00:07:51.750
There's a common way of combining


00:07:51.750 --> 00:07:54.735
precision and recall
using this formula,


00:07:54.735 --> 00:07:57.065
which is called the F_1 score.


00:07:57.065 --> 00:08:01.460
One intuition behind
the F_1 score is


00:08:01.460 --> 00:08:03.770
that you want an algorithm to


00:08:03.770 --> 00:08:06.765
do well on both
precision and recall,


00:08:06.765 --> 00:08:08.655
and if it does worse on


00:08:08.655 --> 00:08:12.505
either precision or
recall, that's pretty bad.


00:08:12.505 --> 00:08:17.150
F_1 is a way of combining
precision and recall that


00:08:17.150 --> 00:08:19.725
emphasizes whichever of P or


00:08:19.725 --> 00:08:22.860
R precision or recall is worse.


00:08:22.860 --> 00:08:26.205
In mathematics, this is
technically called a


00:08:26.205 --> 00:08:29.780
harmonic mean between
precision and recall,


00:08:29.780 --> 00:08:31.990
which is like taking
the average but placing


00:08:31.990 --> 00:08:34.955
more emphasis on whichever
is the lower number.


00:08:34.955 --> 00:08:38.784
If you compute the F_1
score of these two models,


00:08:38.784 --> 00:08:41.025
it turns out to be


00:08:41.025 --> 00:08:45.365
83.4 percent using the
formula below here.


00:08:45.365 --> 00:08:48.255
Model 2 has a very bad recall,


00:08:48.255 --> 00:08:51.290
so its F_1 score is


00:08:51.290 --> 00:08:56.965
actually quite low as well
and this lets us tell,


00:08:56.965 --> 00:08:59.700
maybe more clearly
that Model 1


00:08:59.700 --> 00:09:03.215
appears to be a superior
model than Model 2.


00:09:03.215 --> 00:09:05.410
For your application,
you may have


00:09:05.410 --> 00:09:07.850
a different weighting
between position and recall,


00:09:07.850 --> 00:09:09.195
and so F_1 isn't


00:09:09.195 --> 00:09:11.640
the only way to combine
precision and recall,


00:09:11.640 --> 00:09:13.420
it's just one metric
that's commonly


00:09:13.420 --> 00:09:15.250
used for many applications.


00:09:15.250 --> 00:09:17.480
Let me step through
one more example where


00:09:17.480 --> 00:09:20.520
precision and recall is useful.


00:09:20.520 --> 00:09:21.910
So far, we've talked about


00:09:21.910 --> 00:09:25.350
the binary classification
problem with skewed data sets.


00:09:25.350 --> 00:09:28.250
It turns out to also
frequently be useful


00:09:28.250 --> 00:09:31.255
for multi-class
classification problems.


00:09:31.255 --> 00:09:34.665
If you are detecting
defects in smartphones,


00:09:34.665 --> 00:09:37.130
you may want to
detect scratches on


00:09:37.130 --> 00:09:39.680
them or dents or pit marks.


00:09:39.680 --> 00:09:41.780
This is what it looks
like if someone


00:09:41.780 --> 00:09:44.500
took a screwdriver and
poked their cell phone,


00:09:44.500 --> 00:09:46.500
or discoloration of


00:09:46.500 --> 00:09:49.930
the cell phone's LCD
screen or other material.


00:09:49.930 --> 00:09:53.395
Maybe all four of these defects
are actually quite rare


00:09:53.395 --> 00:09:55.220
that you might want to develop


00:09:55.220 --> 00:09:57.655
an algorithm that can
detect all four of them.


00:09:57.655 --> 00:10:00.400
One way to evaluate
how your algorithm is


00:10:00.400 --> 00:10:03.450
doing on all four
of these defects,


00:10:03.450 --> 00:10:06.110
each of which can be quite rare,


00:10:06.110 --> 00:10:09.105
would be to look at precision
and recall of each of


00:10:09.105 --> 00:10:12.345
these four types of
defects individually.


00:10:12.345 --> 00:10:16.310
In this example, the
learning algorithm has 82.1


00:10:16.310 --> 00:10:17.840
percent precision on finding


00:10:17.840 --> 00:10:20.685
scratches and 99.2
percent recall.


00:10:20.685 --> 00:10:22.835
You find in manufacturing that


00:10:22.835 --> 00:10:25.510
many factories will
want high recall


00:10:25.510 --> 00:10:27.860
because you really
don't want to let


00:10:27.860 --> 00:10:30.765
the phone go out
that is defective.


00:10:30.765 --> 00:10:31.965
But if an algorithm has


00:10:31.965 --> 00:10:33.870
slightly lower
precision, that's okay,


00:10:33.870 --> 00:10:38.570
because through a human
re-examining the phone,


00:10:38.570 --> 00:10:40.070
they will hopefully figure


00:10:40.070 --> 00:10:41.750
out that the phone
is actually okay,


00:10:41.750 --> 00:10:45.830
so many factories will
emphasize high recall.


00:10:45.830 --> 00:10:51.275
By combining precision and
recall using F_1 as follows,


00:10:51.275 --> 00:10:55.340
this gives you a single number
evaluation metric for how


00:10:55.340 --> 00:10:56.880
well your algorithm is doing


00:10:56.880 --> 00:10:59.220
on the four different types of


00:10:59.220 --> 00:11:02.680
defects and can also
help you benchmark to


00:11:02.680 --> 00:11:04.845
human-level performance and also


00:11:04.845 --> 00:11:07.610
prioritize what to work on next.


00:11:07.610 --> 00:11:10.220
Instead of accuracy on scratches,


00:11:10.220 --> 00:11:12.465
dents, pit marks,
and discolorations,


00:11:12.465 --> 00:11:15.710
using F_1 score can help you to


00:11:15.710 --> 00:11:19.190
prioritize the most fruitful type


00:11:19.190 --> 00:11:21.800
of defect to try to work on.


00:11:21.800 --> 00:11:25.265
The reason we use F_1 is because,


00:11:25.265 --> 00:11:30.125
maybe all four defects are
very rare and so accuracy


00:11:30.125 --> 00:11:32.620
would be very high even if


00:11:32.620 --> 00:11:36.030
the algorithm was missing
a lot of these defects.


00:11:36.030 --> 00:11:39.825
I hope that these tools
will help you both evaluate


00:11:39.825 --> 00:11:43.175
your algorithm as well as
prioritize what to work on,


00:11:43.175 --> 00:11:46.375
both in problems with
skewed data sets and


00:11:46.375 --> 00:11:50.030
for problems with
multiple rare classes.


00:11:50.030 --> 00:11:53.594
Now, to wrap up the
section on Error Analysis,


00:11:53.594 --> 00:11:57.055
there's one final concept I
hope to go over with you,


00:11:57.055 --> 00:11:59.150
which is Performance Auditing.


00:11:59.150 --> 00:12:03.435
I found for many projects this
is a key step to make sure


00:12:03.435 --> 00:12:05.785
the learning algorithm
is working well enough


00:12:05.785 --> 00:12:08.830
before you push it out to
a production deployment.


00:12:08.830 --> 00:12:12.230
Let's take a look at
Performance Auditing.
WEBVTT


00:00:00.240 --> 00:00:05.071
Even when your learning algorithm is
doing well on accuracy or F1 score or


00:00:05.071 --> 00:00:09.829
some appropriate metric. It's often worth
one last performance audit before you


00:00:09.829 --> 00:00:11.320
push it to production.


00:00:11.320 --> 00:00:16.680
And this can sometimes save you from
significant post deployment problems.


00:00:16.680 --> 00:00:18.640
Let's take a look.


00:00:18.640 --> 00:00:20.340
You've seen this diagram before.


00:00:20.340 --> 00:00:24.797
After you've gone around this move
multiple times to develop a good learning


00:00:24.797 --> 00:00:26.040
algorithm.


00:00:26.040 --> 00:00:30.340
It's worthwhile auditing this
performance one last time.


00:00:31.440 --> 00:00:36.603
Here's a framework for
how you can double check your system for


00:00:36.603 --> 00:00:41.690
accuracy for fairness/bias and
for other possible problems.


00:00:41.690 --> 00:00:46.760
Step one is brainstorm the different
ways the system might go wrong.


00:00:46.760 --> 00:00:47.733
For example,


00:00:47.733 --> 00:00:53.270
does the algorithm perform sufficiently
well on different subsets of the data?


00:00:53.270 --> 00:00:58.460
Such as individuals of a certain ethnicity
or individuals of different genders?


00:00:58.460 --> 00:01:03.274
Or does the algorithm make certain
errors such as false positives and


00:01:03.274 --> 00:01:07.922
false negatives which you might
worry about in skewed datasets or


00:01:07.922 --> 00:01:12.010
how does it perform on certain rare and
important classes.


00:01:12.010 --> 00:01:16.607
So the types of issues we talked
about in the key challenges video


00:01:16.607 --> 00:01:18.070
earlier this week.


00:01:18.070 --> 00:01:22.173
Any of them that concern you,
You might include them in this


00:01:22.173 --> 00:01:25.786
brainstormed ways that
the system might go wrong for


00:01:25.786 --> 00:01:30.250
all the ways that you're worried
about the system going wrong.


00:01:30.250 --> 00:01:35.008
You might then establish metrics to
assess the performance of your algorithm


00:01:35.008 --> 00:01:36.490
against these issues.


00:01:36.490 --> 00:01:41.405
One very common design patterns
you see is that you often be


00:01:41.405 --> 00:01:45.440
evaluating performance
on slices of the data.


00:01:45.440 --> 00:01:50.449
So rather than evaluating performance
on your entire dev set,


00:01:50.449 --> 00:01:55.640
you may be taking out all of
the individuals of a certain ethnicity,


00:01:55.640 --> 00:02:01.195
all the individuals of a certain gender or
all of the examples where there


00:02:01.195 --> 00:02:06.510
is a scratch defect on the smartphone but
to take a subset of the data.


00:02:06.510 --> 00:02:11.608
Also called a slice of the data
to analyze performance on


00:02:11.608 --> 00:02:18.460
those slices in order to check against
these things that may the problems.


00:02:18.460 --> 00:02:23.355
>> After establishing appropriate metrics,
MLOps tools can also help trigger


00:02:23.355 --> 00:02:27.540
an automatic evaluation for
each model to audit this performance.


00:02:27.540 --> 00:02:32.508
For instance, tensorflow has a package for
tensorflow model analysis or


00:02:32.508 --> 00:02:37.234
TFMA that computes detailed metrics
on new machine learning models,


00:02:37.234 --> 00:02:39.260
on different slices of data.


00:02:39.260 --> 00:02:43.140
You learn more about this
too in the next course.


00:02:43.140 --> 00:02:48.014
>> And as part of this process,
I would also advise you to get buy-in


00:02:48.014 --> 00:02:52.124
from the business of
the product owner that these


00:02:52.124 --> 00:02:56.809
are the most appropriate set
of problems to worry about and


00:02:56.809 --> 00:03:03.140
a reasonable set of metrics to assess
against these possible problems.


00:03:03.140 --> 00:03:07.973
And if you do find a problem,
then it is great that you discovered


00:03:07.973 --> 00:03:12.360
this problem before pushing
your system to production and


00:03:12.360 --> 00:03:16.834
you can then go back to update
the system to address it before


00:03:16.834 --> 00:03:21.540
deploying a system that may
cause problems downstream.


00:03:21.540 --> 00:03:24.885
Let's walk through this
framework with an example,


00:03:24.885 --> 00:03:30.386
I'm going to use speech recognition again,
if you build a speech recognition system,


00:03:30.386 --> 00:03:34.210
you might then brainstorm the way
the system might go wrong.


00:03:34.210 --> 00:03:38.277
So one thing I've looked at the fall for
systems I worked on


00:03:38.277 --> 00:03:43.040
was accuracy on different genders and
different ethnicities.


00:03:43.040 --> 00:03:47.940
For example, a speech system that
does poorly on certain genders may be


00:03:47.940 --> 00:03:50.410
problematic or also ethnicities.


00:03:50.410 --> 00:03:55.232
One type of analysis I've done before
is to carry out analysis of our


00:03:55.232 --> 00:04:00.478
accuracy depending on the perceived
accent of the speaker because we want


00:04:00.478 --> 00:04:05.469
to understand if the speech systems
performance was a huge function of


00:04:05.469 --> 00:04:10.800
the accent of the speaker or you might
worry about the accuracy on different


00:04:10.800 --> 00:04:15.820
devices because different devices
may have different microphones.


00:04:15.820 --> 00:04:20.867
And so if you do much worse on one
brand of cell phones so that if there


00:04:20.867 --> 00:04:27.011
is a problem, you can proactively fix it.
Or finally, this might not be an example


00:04:27.011 --> 00:04:31.320
you would have thought of but
prevalence of rude mis-transcriptions.


00:04:31.320 --> 00:04:35.792
Here's one example of something that
actually happened to some of


00:04:35.792 --> 00:04:37.710
deeplearning.ai's courses.


00:04:37.710 --> 00:04:42.903
One of our instructors, Laurence Maroney
was talking about GANs, generative


00:04:42.903 --> 00:04:48.403
adversarial networks, but because the
transcription system was mistranscribing


00:04:48.403 --> 00:04:53.320
GANs because this unfortunately is
not a common word in english language.


00:04:53.320 --> 00:04:58.156
And so, the subtitles had a lot
of references to gun and gang,


00:04:58.156 --> 00:05:05.160
which were mistranscriptions of what the
instructor actually said, which is GAN.


00:05:05.160 --> 00:05:09.409
So it made it look like there's
a lot of gun violence in that


00:05:09.409 --> 00:05:12.934
deeplearning.ai course and
we actually had to go in to fix it


00:05:12.934 --> 00:05:17.700
because we didn't want that much
gun gang violence in the subtitles.


00:05:17.700 --> 00:05:22.369
It turns out more generally
that mistranscribing someone's


00:05:22.369 --> 00:05:27.036
speech into a rude word or
a swear word that's perceived much


00:05:27.036 --> 00:05:31.450
more negatively than a more
neutral mis transcription.


00:05:31.450 --> 00:05:36.046
And so I've built speech systems as
well where we pay special attention to


00:05:36.046 --> 00:05:40.797
avoiding mis transcriptions that
resulted in the speech system thinking


00:05:40.797 --> 00:05:45.773
someone said a swear word when maybe
they didn't actually say that swear word.


00:05:45.773 --> 00:05:50.710
Based on this list of brainstorm ways
that the speech system might go wrong,


00:05:50.710 --> 00:05:55.149
you can then establish metrics to assess
performance against these issues on


00:05:55.149 --> 00:05:57.080
the appropriate slices of data.


00:05:57.080 --> 00:06:02.281
For example, you can measure the mean
accuracy of the speech system for


00:06:02.281 --> 00:06:07.831
different genders and for different
accents represented in the data set and


00:06:07.831 --> 00:06:11.387
also check for
accuracy on different devices and


00:06:11.387 --> 00:06:15.060
check for offensive or
rude words in the output.


00:06:15.060 --> 00:06:19.705
I find that the ways a system
might go wrong turns out to


00:06:19.705 --> 00:06:22.200
be very problem dependent.


00:06:22.200 --> 00:06:28.094
Different industries, different tasks
will have very different standards and


00:06:28.094 --> 00:06:31.906
in fact today our standards in A I for
what to consider


00:06:31.906 --> 00:06:36.870
an unacceptable level of bias or
what is there and what is not there.


00:06:36.870 --> 00:06:40.651
Those standards are still
continuing to evolve in AI and


00:06:40.651 --> 00:06:42.760
in many specific industries.


00:06:42.760 --> 00:06:45.819
So I would advise you to do a search for


00:06:45.819 --> 00:06:50.657
your industry to see what is
acceptable and to keep current


00:06:50.657 --> 00:06:55.890
with standards of fairness and
all of our growing awareness for


00:06:55.890 --> 00:07:00.740
how to make our systems more fair and
less biased.


00:07:00.740 --> 00:07:05.603
One last tip, I find that rather than
just one person trying to brainstorm what


00:07:05.603 --> 00:07:09.728
could go wrong for high stakes
applications if you can have a team or


00:07:09.728 --> 00:07:14.373
sometimes even external advisors help
you brainstorm things that you want


00:07:14.373 --> 00:07:17.319
to watch out for
that can reduce the risk of you or


00:07:17.319 --> 00:07:21.780
your team being caught later by
something that you hadn't thought of.


00:07:21.780 --> 00:07:24.746
I know that standards
are still evolving for


00:07:24.746 --> 00:07:29.310
what we consider fair and sufficiently
biased in many industries, but


00:07:29.310 --> 00:07:33.642
this is one of the topics I think would
be good for us to get ahead of and


00:07:33.642 --> 00:07:38.813
to proactively try to identify, measure
against and solve problems rather than


00:07:38.813 --> 00:07:43.790
deploy a system to be surprised much
later by some unexpected consequences.


00:07:43.790 --> 00:07:49.410
So that's it for
performance auditing. With this,


00:07:49.410 --> 00:07:54.185
I hope you have higher confidence in your
learning algorithm when you go out to


00:07:54.185 --> 00:07:55.661
push it to production.
WEBVTT


00:00:00.000 --> 00:00:02.640
Let's say that error analysis has


00:00:02.640 --> 00:00:04.920
caused you to decide to focus on


00:00:04.920 --> 00:00:06.240
improving your learning algorithm's


00:00:06.240 --> 00:00:10.050
performance on data with
a certain category or tag,


00:00:10.050 --> 00:00:13.275
say speech with car
noise in the background.


00:00:13.275 --> 00:00:15.360
Let's take a look
at how you can take


00:00:15.360 --> 00:00:17.925
a data centric approach


00:00:17.925 --> 00:00:20.820
to improving your learning
algorithm's performance.


00:00:20.820 --> 00:00:22.560
You've heard me speak before


00:00:22.560 --> 00:00:27.390
about model centric versus
data centric AI development.


00:00:27.390 --> 00:00:31.155
Here's a little more
detail on what I mean.


00:00:31.155 --> 00:00:34.380
With a model centric
view of AI developments,


00:00:34.380 --> 00:00:36.570
you would take the
data you have and


00:00:36.570 --> 00:00:38.670
then try to work really hard to


00:00:38.670 --> 00:00:42.630
develop a model that does as
well as possible on the data.


00:00:42.630 --> 00:00:44.790
Because a lot of
academic research on


00:00:44.790 --> 00:00:47.190
AI was driven by researchers,


00:00:47.190 --> 00:00:49.575
downloading a benchmark data set


00:00:49.575 --> 00:00:52.225
and trying to do well
on that benchmark,


00:00:52.225 --> 00:00:56.580
most academic research
on AI is model centric,


00:00:56.580 --> 00:01:00.060
because the benchmark data
set is a fixed quantity.


00:01:00.060 --> 00:01:02.969
In this view, model
centric development,


00:01:02.969 --> 00:01:06.390
you would hold the data fixed
and iteratively improve.


00:01:06.390 --> 00:01:09.075
In this model centric view,


00:01:09.075 --> 00:01:11.490
you would hold the data fixed and


00:01:11.490 --> 00:01:14.785
iteratively improve
the code or the model.


00:01:14.785 --> 00:01:17.490
There's still an
important role to play


00:01:17.490 --> 00:01:20.010
in trying to come up
with better models,


00:01:20.010 --> 00:01:23.040
but that's a different
view of AI developments


00:01:23.040 --> 00:01:26.925
which I think is more useful
for many applications.


00:01:26.925 --> 00:01:29.010
Which is to shift a bit from


00:01:29.010 --> 00:01:31.875
a model centric to more of
a data centric view.


00:01:31.875 --> 00:01:34.020
In this view, we think of


00:01:34.020 --> 00:01:36.630
the quality of the
data as paramount,


00:01:36.630 --> 00:01:41.010
and you can use tools
such as error analysis or


00:01:41.010 --> 00:01:43.680
data augmentation
to systematically


00:01:43.680 --> 00:01:46.440
improve the data quality.


00:01:46.440 --> 00:01:48.330
For many applications, I


00:01:48.330 --> 00:01:50.340
find that if your
data is good enough,


00:01:50.340 --> 00:01:53.300
there are multiple models
that will do just fine.


00:01:53.300 --> 00:01:56.415
In this view, you can
instead hold the code


00:01:56.415 --> 00:01:59.940
fixed and iteratively
improve the data.


00:01:59.940 --> 00:02:02.745
There's a role for model
centric development,


00:02:02.745 --> 00:02:06.565
and there's a role for
data centric development.


00:02:06.565 --> 00:02:08.940
If you've been used
to model centric


00:02:08.940 --> 00:02:10.530
thinking for most of


00:02:10.530 --> 00:02:12.115
your experience with
machine learning,


00:02:12.115 --> 00:02:13.320
I would urge you to


00:02:13.320 --> 00:02:16.515
consider taking a data
centric view as well,


00:02:16.515 --> 00:02:17.940
where when you're trying to


00:02:17.940 --> 00:02:20.145
improve your learning
outcomes performance,


00:02:20.145 --> 00:02:25.200
try asking how can you make
your data set even better?


00:02:25.200 --> 00:02:28.650
One of the most important
ways to improve


00:02:28.650 --> 00:02:32.610
the quality of a data set
is data augmentation.


00:02:32.610 --> 00:02:35.310
Let's go on to the next
video where we'll start


00:02:35.310 --> 00:02:38.770
to take a look at
data augmentation.
WEBVTT


00:00:00.540 --> 00:00:05.518
There's a picture, a conceptual picture
that I found useful for thinking about


00:00:05.518 --> 00:00:10.590
data augmentation and how this can help
the performance of a learning algorithm.


00:00:10.590 --> 00:00:14.979
Let me share this picture of you since I
think you find it useful to when trying to


00:00:14.979 --> 00:00:17.530
decide whether to use data augmentation.


00:00:17.530 --> 00:00:19.540
Take speech recognition.


00:00:19.540 --> 00:00:26.575
There could be many different types of
noise in speech input such as car noise,


00:00:26.575 --> 00:00:31.691
play noise, train noise,
machine noise, cafe noise or


00:00:31.691 --> 00:00:37.640
library noise, which isn't that loud or
food court noise.


00:00:37.640 --> 00:00:42.631
Maybe these types of noises are more
similar to each other because they're all


00:00:42.631 --> 00:00:47.773
mechanical types of noise and these types
of noise maybe a little bit more similar


00:00:47.773 --> 00:00:52.570
to each other with mainly people talking
and interacting with each other.


00:00:52.570 --> 00:00:57.552
So let me share of your picture that I
keep in mind when I'm planning out my


00:00:57.552 --> 00:01:01.801
activities on getting more data
through data augmentation or


00:01:01.801 --> 00:01:06.240
through actual data collection
of any of these types of data.


00:01:06.240 --> 00:01:12.430
In this diagram, the vertical axis
represents performance, say accuracy.


00:01:12.430 --> 00:01:14.529
And on the horizontal axis, and


00:01:14.529 --> 00:01:18.830
this is a conceptual kind of
a thought experiment type of access.


00:01:18.830 --> 00:01:22.540
I'm going to represent
the space of possible inputs.


00:01:22.540 --> 00:01:25.675
So for
example there speech with car noise and


00:01:25.675 --> 00:01:29.481
plane noise and
train noise sound a bit like car noise.


00:01:29.481 --> 00:01:33.268
So they're quite similar and
machine noise a little bit further away,


00:01:33.268 --> 00:01:36.865
by machine noise, I'm picturing
the sounds of a washing machine or


00:01:36.865 --> 00:01:38.521
a very loud air conditioners.


00:01:38.521 --> 00:01:42.651
Then you may have speech with cafe noise,
library noise or


00:01:42.651 --> 00:01:47.340
food court ,and those are maybe
more similar to each other.


00:01:47.340 --> 00:01:49.887
Then to these types of mechanical noise.


00:01:49.887 --> 00:01:54.820
Your system will have different levels
of performance on these different types


00:01:54.820 --> 00:01:55.500
of input.


00:01:55.500 --> 00:01:59.897
Let's say the performance is this for
data of play noise,


00:01:59.897 --> 00:02:03.690
that of car noise,
train noise, machine noise.


00:02:03.690 --> 00:02:10.710
And it does worse on data with library
noise, cafe noise, food court noise.


00:02:10.710 --> 00:02:13.321
And so I think of their as being a curve.


00:02:13.321 --> 00:02:19.958
Or maybe think of this like a one
dimensional piece of rubber band or


00:02:19.958 --> 00:02:25.409
like a rubber sheet that shows
how accurate your speech


00:02:25.409 --> 00:02:31.040
system is as a function of
the type of input it gets.


00:02:31.040 --> 00:02:35.958
A human will have some other
level of performance on these


00:02:35.958 --> 00:02:38.230
different types of data.


00:02:38.230 --> 00:02:43.903
So maybe a human is a bit better,
will play noise bit better in car noise,


00:02:43.903 --> 00:02:48.739
and so on and maybe they are much
better then your algorithm on


00:02:48.739 --> 00:02:52.661
library noise, cafe noise and
food court noise.


00:02:52.661 --> 00:03:00.940
So the human level performance is
represented via some other curve.


00:03:00.940 --> 00:03:07.640
And let me just label this as
the current models performance in blue.


00:03:07.640 --> 00:03:12.540
So this gap represents an opportunity for
improvement.


00:03:12.540 --> 00:03:16.284
Now, what happens if you
use data augmentation or


00:03:16.284 --> 00:03:21.187
maybe not data augmentation but
go out to a bunch of actual cafes,


00:03:21.187 --> 00:03:26.540
to collect a lot more data with
cafe noise in the background.


00:03:26.540 --> 00:03:31.866
What you'll do is, you'll take this
point imagine grabbing a hold of this


00:03:31.866 --> 00:03:36.953
blue rubber bands or this rubber sheet,
and pulling it upward like so.


00:03:36.953 --> 00:03:40.191
That's what you're doing if you collect or


00:03:40.191 --> 00:03:45.352
somehow gets more data with cafe noise and
add that your training set,


00:03:45.352 --> 00:03:51.240
you're pulling up the performance of
the algorithm on inputs with cafe noise.


00:03:51.240 --> 00:03:53.577
And what that will tend to do,


00:03:53.577 --> 00:03:58.640
is pull up this rubber sheet in
the adjacent region as well.


00:03:58.640 --> 00:04:03.840
So if performance on cafe noise goes up,
probably performance


00:04:03.840 --> 00:04:08.951
on the nearby points will go up too and
performance on far away.


00:04:08.951 --> 00:04:12.640
Points may or may not go up as much.


00:04:12.640 --> 00:04:16.326
It turns out that for
unstructured data problems,


00:04:16.326 --> 00:04:20.697
pulling up one piece of this
rubber sheet is unlikely to cause


00:04:20.697 --> 00:04:25.520
a different piece of the rubber
sheet to dip down really far below.


00:04:25.520 --> 00:04:30.369
Instead, pulling up one point causes
nearby points to be pulled up quite a lot


00:04:30.369 --> 00:04:34.770
and far away points may be pulled up
a little bit, or if you're lucky,


00:04:34.770 --> 00:04:36.740
maybe more than a little bit.


00:04:36.740 --> 00:04:42.485
But when I'm planning how to improve
my learning algorithm's performance and


00:04:42.485 --> 00:04:44.743
where I hope to get it to, and


00:04:44.743 --> 00:04:50.582
getting more data in those places to
iteratively pull up with those pieces or


00:04:50.582 --> 00:04:56.830
those parts of the rubber sheet to get
them closer to human level performance.


00:04:56.830 --> 00:04:59.951
And when you pull up
part of the rubber sheet,


00:04:59.951 --> 00:05:04.160
the location of the biggest gap
may shift to somewhere else.


00:05:04.160 --> 00:05:09.913
And error analysis will tell you what is
the location of this new biggest gap,


00:05:09.913 --> 00:05:14.515
that may then be worth your effort,
to collect more data on and


00:05:14.515 --> 00:05:17.988
therefore to try to pull
up one piece at a time.


00:05:17.988 --> 00:05:23.283
And this turns out to be a pretty
efficient way to decide where on the blue


00:05:23.283 --> 00:05:28.048
rubber sheet to pull up next to
try to get performance closer to,


00:05:28.048 --> 00:05:30.361
say human level performance.


00:05:31.440 --> 00:05:36.655
I hope this analogy of a rubber band or
rubber sheet and repeatedly pulling


00:05:36.655 --> 00:05:42.122
up a point on this rubber sheet will help
you predict the effects of collecting


00:05:42.122 --> 00:05:48.340
more data that's associated with
a specific category or a specific tag.


00:05:48.340 --> 00:05:50.480
How do you get more of this data?


00:05:50.480 --> 00:05:54.159
Let's take a look at how you can
perform data augmentation and


00:05:54.159 --> 00:05:57.001
some best practices doing so
in the next video.
WEBVTT


00:00:00.000 --> 00:00:02.010
Data augmentation can be


00:00:02.010 --> 00:00:05.025
a very efficient way
to get more data,


00:00:05.025 --> 00:00:08.070
especially for
unstructured data problems


00:00:08.070 --> 00:00:11.175
such as images,
audio, maybe text.


00:00:11.175 --> 00:00:13.800
But when carrying out
data augmentation,


00:00:13.800 --> 00:00:16.545
there're a lot of choices
you have to make.


00:00:16.545 --> 00:00:18.105
What are the parameters?


00:00:18.105 --> 00:00:21.330
How do you design the
data augmentation setup?


00:00:21.330 --> 00:00:24.630
Let's dive into this to look
at some best practices.


00:00:24.630 --> 00:00:26.055
Take speech recognition.


00:00:26.055 --> 00:00:28.710
Given an audio clip like this,


00:00:28.710 --> 00:00:30.690
''AI is the new electricity''.


00:00:30.690 --> 00:00:35.790
If you take background cafe
noise that sounds like this


00:00:35.790 --> 00:00:43.900
[NOISE] and add these two
audio clips together.


00:00:43.900 --> 00:00:47.485
Literally, take the two
waveforms and sum them up,


00:00:47.485 --> 00:00:48.730
then you can create


00:00:48.730 --> 00:00:51.365
a synthetic example
that sounds like this,


00:00:51.365 --> 00:00:55.350
''AI is the new electricity''.


00:00:55.350 --> 00:00:57.330
Sounds like someone saying,


00:00:57.330 --> 00:01:01.390
AI is the new electricity
in a noisy cafe.


00:01:01.390 --> 00:01:03.940
This is one form of
data augmentation that


00:01:03.940 --> 00:01:06.130
lets you efficiently
create a lot of


00:01:06.130 --> 00:01:10.840
data that sounds like data
collected in the cafe.


00:01:10.840 --> 00:01:12.970
Or if you take the
same audio clip,


00:01:12.970 --> 00:01:14.725
''AI is the new electricity'',


00:01:14.725 --> 00:01:19.560
and add it to background
music [MUSIC],


00:01:19.560 --> 00:01:21.510
that it sounds like
someone saying it


00:01:21.510 --> 00:01:24.045
with maybe the radio
on in the background.


00:01:24.045 --> 00:01:25.755
''AI Is the new electricity''.


00:01:25.755 --> 00:01:28.890
Now when carrying out
data augmentation,


00:01:28.890 --> 00:01:32.405
there're a few decisions
you need to make.


00:01:32.405 --> 00:01:34.720
What types of background noise
should you use


00:01:34.720 --> 00:01:39.725
and how loud should the background
noise be relative to the speech.


00:01:39.725 --> 00:01:41.860
Let's take a look at some ways of


00:01:41.860 --> 00:01:44.395
making these decisions
systematically.


00:01:44.395 --> 00:01:46.390
The goal of data augmentation,


00:01:46.390 --> 00:01:48.370
is to create examples


00:01:48.370 --> 00:01:50.305
that your learning
algorithm can learn from.


00:01:50.305 --> 00:01:52.840
As a framework for doing that,


00:01:52.840 --> 00:01:55.780
I encourage you to think
of how you can create


00:01:55.780 --> 00:02:00.400
realistic examples that the
algorithm does poorly on,


00:02:00.400 --> 00:02:02.200
because if the algorithm already


00:02:02.200 --> 00:02:03.790
does well in those examples,


00:02:03.790 --> 00:02:06.360
then there's less for
it to learn from.


00:02:06.360 --> 00:02:08.620
But you want the
examples to still


00:02:08.620 --> 00:02:10.900
be ones that a human or


00:02:10.900 --> 00:02:13.120
maybe some other baseline can do


00:02:13.120 --> 00:02:15.790
well on, because otherwise,


00:02:15.790 --> 00:02:16.840
one way to generate


00:02:16.840 --> 00:02:19.270
examples that the
algorithm does poorly on,


00:02:19.270 --> 00:02:22.000
would be to just create
examples that are so


00:02:22.000 --> 00:02:26.425
noisy that no one can
hear what anyone said,


00:02:26.425 --> 00:02:28.135
but that's not helpful.


00:02:28.135 --> 00:02:30.685
You want examples that


00:02:30.685 --> 00:02:33.775
are hard enough to
challenge the algorithm,


00:02:33.775 --> 00:02:36.460
but not so hard that they're


00:02:36.460 --> 00:02:38.230
impossible for any human or


00:02:38.230 --> 00:02:40.320
any algorithm to ever do well on.


00:02:40.320 --> 00:02:42.170
That's why when I'm


00:02:42.170 --> 00:02:45.925
generating new examples
using data augmentation,


00:02:45.925 --> 00:02:47.770
I try to generate


00:02:47.770 --> 00:02:50.715
examples that meets
both of these criteria.


00:02:50.715 --> 00:02:53.050
Now, one way that some people do


00:02:53.050 --> 00:02:56.620
data augmentation is to
generate an augmented data set,


00:02:56.620 --> 00:02:58.810
and then train the
learning algorithm and


00:02:58.810 --> 00:03:01.330
see if the algorithm does
better on the dev set.


00:03:01.330 --> 00:03:04.180
Then fiddle around with
the parameters for


00:03:04.180 --> 00:03:06.040
data augmentation and change


00:03:06.040 --> 00:03:08.325
the learning algorithm
again and so on.


00:03:08.325 --> 00:03:11.290
This turns out to be quite
inefficient because every


00:03:11.290 --> 00:03:14.365
time you change your data
augmentation parameters,


00:03:14.365 --> 00:03:15.580
you need to train


00:03:15.580 --> 00:03:17.785
your new network or train
your learning algorithm


00:03:17.785 --> 00:03:21.305
all over and this can
take a long time.


00:03:21.305 --> 00:03:24.170
Instead, I found that
using these principles,


00:03:24.170 --> 00:03:26.650
allows you to sanity check that


00:03:26.650 --> 00:03:29.980
your new data generated
using data augmentation is


00:03:29.980 --> 00:03:32.680
useful without actually having to


00:03:32.680 --> 00:03:35.590
spend maybe hours or
sometimes days of


00:03:35.590 --> 00:03:38.200
training a learning
algorithm on that data to


00:03:38.200 --> 00:03:39.610
verify that it will


00:03:39.610 --> 00:03:41.655
result in the
performance improvement.


00:03:41.655 --> 00:03:44.620
Specifically, here's a
checklist you might go


00:03:44.620 --> 00:03:47.955
through when you are
generating new data.


00:03:47.955 --> 00:03:50.530
One, does it sound realistic.


00:03:50.530 --> 00:03:52.715
You want your audio to
actually sound like


00:03:52.715 --> 00:03:54.430
realistic audio of the sort


00:03:54.430 --> 00:03:56.490
that you want your
algorithm to perform on.


00:03:56.490 --> 00:03:59.465
Two, is the X to Y mapping clear?


00:03:59.465 --> 00:04:03.235
In other words, can humans
still recognize what was said?


00:04:03.235 --> 00:04:06.040
This is to verify point two here.


00:04:06.040 --> 00:04:08.300
Three, is the algorithm


00:04:08.300 --> 00:04:11.705
currently doing poorly
on this new data.


00:04:11.705 --> 00:04:15.095
That helps you verify point one.


00:04:15.095 --> 00:04:16.810
If you can generate data


00:04:16.810 --> 00:04:18.805
that meets all of these criteria,


00:04:18.805 --> 00:04:20.500
then that would give you


00:04:20.500 --> 00:04:22.690
a higher chance that when you put


00:04:22.690 --> 00:04:24.010
this data into your training


00:04:24.010 --> 00:04:26.750
set and retrain the algorithm,


00:04:26.750 --> 00:04:29.380
then that will result
in you successfully


00:04:29.380 --> 00:04:31.960
pulling up part of
this rubber sheet.


00:04:31.960 --> 00:04:34.105
Let's look at one more example,


00:04:34.105 --> 00:04:35.860
using images this time.


00:04:35.860 --> 00:04:38.590
Let's say that you
have a very small set


00:04:38.590 --> 00:04:41.465
of images of smartphones
with scratches.


00:04:41.465 --> 00:04:44.605
Here's how you may be able
to use data augmentation.


00:04:44.605 --> 00:04:47.600
You can take the image
and flip it horizontally.


00:04:47.600 --> 00:04:50.300
This results in a
pretty realistic image.


00:04:50.300 --> 00:04:52.310
The phone buttons are
now on the other side,


00:04:52.310 --> 00:04:54.594
but this could be
a useful example


00:04:54.594 --> 00:04:56.545
to add to your training set.


00:04:56.545 --> 00:05:00.465
Or you could implement
contrast changes


00:05:00.465 --> 00:05:02.650
or actually brighten up the image


00:05:02.650 --> 00:05:05.830
here so the scratch is a
little bit more visible.


00:05:05.830 --> 00:05:09.515
Or you could try
darkening the image,


00:05:09.515 --> 00:05:11.260
but in this example,


00:05:11.260 --> 00:05:13.900
the image is now so
dark that even I as


00:05:13.900 --> 00:05:15.430
a person can't really tell


00:05:15.430 --> 00:05:17.700
if there's a scratch
there or not.


00:05:17.700 --> 00:05:20.950
Whereas these two
examples on top would


00:05:20.950 --> 00:05:23.680
pass the checklist
we had earlier,


00:05:23.680 --> 00:05:26.385
that the human can still
detect the scratch well,


00:05:26.385 --> 00:05:28.580
this example is too dark,


00:05:28.580 --> 00:05:30.790
it would fail that checklists.


00:05:30.790 --> 00:05:32.480
I would try to choose


00:05:32.480 --> 00:05:36.219
the data augmentation scheme
that generates more examples


00:05:36.219 --> 00:05:38.510
that look like the
ones on top and few of


00:05:38.510 --> 00:05:41.640
the ones that look like the
ones here at the bottom.


00:05:41.640 --> 00:05:44.440
In fact, going off
the principle that we


00:05:44.440 --> 00:05:47.320
want images that look realistic,


00:05:47.320 --> 00:05:48.880
that humans can do well on


00:05:48.880 --> 00:05:51.205
and hopefully the
algorithm does poorly on,


00:05:51.205 --> 00:05:52.450
you can also use


00:05:52.450 --> 00:05:54.550
more sophisticated
techniques such


00:05:54.550 --> 00:05:56.440
as take a picture of a phone with


00:05:56.440 --> 00:06:00.100
no scratches and use Photoshop in


00:06:00.100 --> 00:06:04.375
order to artificially
draw a scratch.


00:06:04.375 --> 00:06:07.765
This technique, literally
using Photoshop,


00:06:07.765 --> 00:06:11.665
can also be an effective way
to generate more examples,


00:06:11.665 --> 00:06:14.755
because this example
of a scratch here,


00:06:14.755 --> 00:06:17.495
you may or may not be able
to see it depending on


00:06:17.495 --> 00:06:19.620
the video compression and image


00:06:19.620 --> 00:06:21.690
contrast where you're
watching this video,


00:06:21.690 --> 00:06:23.700
but with a scratch here,


00:06:23.700 --> 00:06:26.445
this looks like a pretty
realistic scratch


00:06:26.445 --> 00:06:28.695
that's actually generated
with Photoshop.


00:06:28.695 --> 00:06:31.950
I as a person can
recognize the scratch


00:06:31.950 --> 00:06:33.630
and so if the learning algorithm


00:06:33.630 --> 00:06:35.265
isn't detecting this right now,


00:06:35.265 --> 00:06:37.660
this would be a great
example to add.


00:06:37.660 --> 00:06:41.175
I've also used more advanced
techniques like GANs,


00:06:41.175 --> 00:06:43.080
Generative Adversarial
Networks to


00:06:43.080 --> 00:06:45.450
synthesize scratches like
these automatically,


00:06:45.450 --> 00:06:47.100
although I found that techniques


00:06:47.100 --> 00:06:49.140
like that can also be overkill,


00:06:49.140 --> 00:06:51.325
meaning that there're simpler
techniques that are much


00:06:51.325 --> 00:06:53.685
faster to implement that work


00:06:53.685 --> 00:06:55.860
just fine without
the complexity of


00:06:55.860 --> 00:06:59.545
building a GAN to
synthesize scratches.


00:06:59.545 --> 00:07:05.400
You may have heard of the
term model iteration,


00:07:05.400 --> 00:07:09.460
which refers to iteratively
training a model using


00:07:09.460 --> 00:07:11.260
error analysis and then trying to


00:07:11.260 --> 00:07:13.870
decide how to improve the model.


00:07:13.870 --> 00:07:17.485
Taking a data-centric
approach AI development,


00:07:17.485 --> 00:07:20.855
sometimes it's useful
to instead use


00:07:20.855 --> 00:07:23.200
a data iteration loop where you


00:07:23.200 --> 00:07:26.125
repeatedly take the
data and the model,


00:07:26.125 --> 00:07:29.555
train your learning
algorithm, do error analysis,


00:07:29.555 --> 00:07:31.600
and as you go through this loop,


00:07:31.600 --> 00:07:33.460
focus on how to add


00:07:33.460 --> 00:07:37.000
data or improve the
quality of the data.


00:07:37.000 --> 00:07:39.520
For many practical applications,


00:07:39.520 --> 00:07:42.870
taking this data
iteration loop approach,


00:07:42.870 --> 00:07:46.985
with a robust hyperparameter
search, that's important too.


00:07:46.985 --> 00:07:49.480
Taking of data iteration
loop approach,


00:07:49.480 --> 00:07:51.324
results in faster improvements


00:07:51.324 --> 00:07:53.460
to your learning
algorithm performance,


00:07:53.460 --> 00:07:55.160
depending on your problem.


00:07:55.160 --> 00:07:58.960
When you're working on an
unstructured data problem,


00:07:58.960 --> 00:08:01.030
data augmentation, if you can


00:08:01.030 --> 00:08:03.790
create new data that
seems realistic,


00:08:03.790 --> 00:08:05.950
that humans can do quite well on,


00:08:05.950 --> 00:08:08.070
but the algorithm struggles on,


00:08:08.070 --> 00:08:10.290
that can be an efficient way to


00:08:10.290 --> 00:08:13.050
improve your learning
algorithm performance.


00:08:13.050 --> 00:08:15.690
If you fall through
error analysis,


00:08:15.690 --> 00:08:17.010
that your learning
algorithm does


00:08:17.010 --> 00:08:19.635
poorly on speech with cafe noise,


00:08:19.635 --> 00:08:22.800
data augmentation to
generate more data with


00:08:22.800 --> 00:08:24.030
cafe noise could be


00:08:24.030 --> 00:08:25.290
an efficient way to


00:08:25.290 --> 00:08:27.515
improve your learning
algorithm performance.


00:08:27.515 --> 00:08:30.300
Now, when you add
data to your system,


00:08:30.300 --> 00:08:32.580
the question I've
often been asked is,


00:08:32.580 --> 00:08:37.240
can adding data hurt your
learning algorithm's performance?


00:08:37.240 --> 00:08:38.690
Usually, for


00:08:38.690 --> 00:08:41.375
unstructured data performance,
the answer is no,


00:08:41.375 --> 00:08:43.320
with some caveats, but let's dive


00:08:43.320 --> 00:08:46.390
more deeply into this
in the next video.
WEBVTT


00:00:00.240 --> 00:00:04.431
For a lot of machine learning problems,
training sets and


00:00:04.431 --> 00:00:09.406
dev and test set distribution
start at being reasonably similar.


00:00:09.406 --> 00:00:14.547
But, if you're using data augmentation,
you're adding to specific


00:00:14.547 --> 00:00:19.526
parts of the training set such as
adding lots of data with cafe noise.


00:00:19.526 --> 00:00:24.231
So now you're training set may come
from a very different distribution than


00:00:24.231 --> 00:00:26.153
the dev set and the test set.


00:00:26.153 --> 00:00:29.150
Is this going to hurt your
learning album's performance?


00:00:29.150 --> 00:00:33.670
Usually the answer is no with some caveats
when you're working on unstructured


00:00:33.670 --> 00:00:34.630
data problems.


00:00:34.630 --> 00:00:38.040
But let's take a deeper look
at what that really means.


00:00:38.040 --> 00:00:42.605
If you are working on
an unstructured data problem and


00:00:42.605 --> 00:00:48.590
if your model is large, such as a neural
network that is quite large and


00:00:48.590 --> 00:00:51.960
has a large capacity and does low bias.


00:00:51.960 --> 00:00:58.041
And if the mapping from x to y is clear
and by that I mean given only the input x,


00:00:58.041 --> 00:01:01.288
humans can make accurate predictions.


00:01:01.288 --> 00:01:08.350
Then it turns out adding accurately
labeled data rarely hurts accuracy.


00:01:08.350 --> 00:01:13.280
This is an important observation
because adding data through data


00:01:13.280 --> 00:01:17.242
augmentation or
collecting more of one type of data,


00:01:17.242 --> 00:01:22.449
can really change your input data
distribution to probability of x.


00:01:22.449 --> 00:01:29.840
Let's say at the start of your problem,
20% of your data had cafe noise.


00:01:29.840 --> 00:01:33.561
But using augmentation,
you added a lot of cafe noise.


00:01:33.561 --> 00:01:40.240
So now this is 50 of your data is
data of cafe noise in the background.


00:01:40.240 --> 00:01:44.765
It turns out that so
long as your model is sufficiently large,


00:01:44.765 --> 00:01:49.550
then it won't stop it from doing
a good job on the cafe noise data as


00:01:49.550 --> 00:01:52.970
well as doing a good job
on non cafe noise data.


00:01:52.970 --> 00:01:59.096
In contrast, if your model was small, then
changing your input data distribution this


00:01:59.096 --> 00:02:04.840
way may cause it to spend too much of its
resources modeling cafe noise settings.


00:02:04.840 --> 00:02:08.910
And this could hurt this
performance on non cafe noise data.


00:02:08.910 --> 00:02:13.740
But if your model is large enough,
then this isn't really an issue.


00:02:13.740 --> 00:02:20.143
The second problem that could arise is
if the mapping from x to y is not clear,


00:02:20.143 --> 00:02:25.440
meaning given x,
the true label of y is very ambiguous.


00:02:25.440 --> 00:02:29.694
This doesn't really happen much
in speech recognition, but


00:02:29.694 --> 00:02:34.440
let me illustrate this with
an example from computer vision.


00:02:34.440 --> 00:02:39.254
This is very rare, so it's not
something I would worry about the most


00:02:39.254 --> 00:02:43.500
practical problems, but
let's see why this is important.


00:02:43.500 --> 00:02:48.252
One of the systems I had worked
on many years ago use Google


00:02:48.252 --> 00:02:52.806
street view images to read
host numbers in order to more


00:02:52.806 --> 00:02:58.080
accurately clear locate buildings and
houses in Google maps.


00:02:58.080 --> 00:03:02.368
So one of the things that
system did was take us input


00:03:02.368 --> 00:03:06.880
pictures like this and
figure out what is this digit.


00:03:06.880 --> 00:03:14.397
So clearly this is a one and
this is a alphabet I.


00:03:14.397 --> 00:03:20.895
You don't see a lot of I's in street view
images, but there are some building.


00:03:20.895 --> 00:03:27.598
You may see a sign that says
navigate to house number 42 I,


00:03:27.598 --> 00:03:34.640
but house numbers really rarely
have an alphabet I in it.


00:03:34.640 --> 00:03:40.542
Now, if you find that your
algorithm has very high accuracy


00:03:40.542 --> 00:03:46.445
on recognizing ones, but
low accuracy on recognizing Is,


00:03:46.445 --> 00:03:53.790
one thing you might do is add a lot more
examples of Is in your training set.


00:03:53.790 --> 00:03:58.748
And the problem, and
this is a rare problem is there


00:03:58.748 --> 00:04:02.920
are some images that are truly ambiguous.


00:04:02.920 --> 00:04:07.940
Is this a one or is this an I?


00:04:07.940 --> 00:04:12.200
And if you were to add a lot of
new Is to your training set,


00:04:12.200 --> 00:04:15.646
especially ambiguous examples like these,


00:04:15.646 --> 00:04:21.284
then that may skew the data sets to
have a lot more Is and hurt performance.


00:04:21.284 --> 00:04:26.660
Because we know that there are a lot
more ones than Is on house numbers.


00:04:26.660 --> 00:04:29.505
If the Sees a picture like this,


00:04:29.505 --> 00:04:34.550
it would be safer to guess that this
is a one rather than that this is an I.


00:04:34.550 --> 00:04:39.314
But if data augmentation skews
the data set in the direction of


00:04:39.314 --> 00:04:42.793
having a lot more Is
rather than a lot of ones,


00:04:42.793 --> 00:04:49.540
that may cause the algorithm to guess
poorly on an ambiguous example like this.


00:04:49.540 --> 00:04:56.677
So this is one rare example where adding
more data could hurt performance and


00:04:56.677 --> 00:05:04.258
this example of one versus I is one that
contradicts the second bullet because for


00:05:04.258 --> 00:05:08.737
some images the mapping
from x to y is not clear.


00:05:08.737 --> 00:05:13.935
In particular given only
an image like this on the right,


00:05:13.935 --> 00:05:18.540
even a human can't really
tell what this is.


00:05:18.540 --> 00:05:23.241
Just to be clear, the example that we just
went through together is a pretty rare


00:05:23.241 --> 00:05:27.241
almost corner case and it's quite
unusual for data augmentation or


00:05:27.241 --> 00:05:31.410
adding more data to hurt the performance
of your learning algorithm.


00:05:31.410 --> 00:05:33.906
So long as your model is big enough,


00:05:33.906 --> 00:05:39.540
maybe a neural network is big enough to
learn from diverse set of data sources.


00:05:39.540 --> 00:05:44.347
But I hope that understanding this rare
case where it could hypothetically hurt


00:05:44.347 --> 00:05:49.082
gives you more comfort with using data
augmentation or collecting more data to


00:05:49.082 --> 00:05:53.887
improve the performance of your algorithm,
even if it causes your training set


00:05:53.887 --> 00:05:59.840
distribution to become different from
your dev set and test set distribution.


00:05:59.840 --> 00:06:04.530
So far, our discussion has focused
on unstructured data problems.


00:06:04.530 --> 00:06:07.120
How about structured data problems?


00:06:07.120 --> 00:06:11.354
It turns out there's a different
set of techniques that's useful for


00:06:11.354 --> 00:06:12.520
structured data.


00:06:12.520 --> 00:06:14.861
Let's take a look at
that in the next video.
WEBVTT


00:00:00.440 --> 00:00:02.710
For many structured data problems.


00:00:02.710 --> 00:00:06.999
It turns out that creating brand new
training examples is difficult, but


00:00:06.999 --> 00:00:11.912
there's something else you could do which
is to take existing training examples and


00:00:11.912 --> 00:00:16.540
figure out if there are additional
useful features you can add to it.


00:00:16.540 --> 00:00:18.470
Let's take a look at an example.


00:00:18.470 --> 00:00:23.463
Let me use an example of restaurant
recommendations where if you're


00:00:23.463 --> 00:00:27.938
running an app that has to
recommend restaurants to users that


00:00:27.938 --> 00:00:32.090
may be interested in checking
out certain restaurants.


00:00:32.090 --> 00:00:37.964
One way to do this would be to have a set
of features for each user of each person,


00:00:37.964 --> 00:00:42.948
and a set of features for each
restaurant that then get fed into some


00:00:42.948 --> 00:00:48.021
learning algorithms, say a neural
network and then your network,


00:00:48.021 --> 00:00:53.361
whose job it is to predict whether or
not this is a good recommendation,


00:00:53.361 --> 00:00:57.463
whether to recommend this
restaurant to that person.


00:00:57.463 --> 00:01:01.302
In this particular example,
which is a real example,


00:01:01.302 --> 00:01:06.143
error analysis showed that the system
was unfortunately frequently


00:01:06.143 --> 00:01:11.170
recommending to vegetarians restaurants
that only had meat options.


00:01:11.170 --> 00:01:12.302
There were users,


00:01:12.302 --> 00:01:16.901
they were pretty clearly vegetarian based
on what they had ordered before and


00:01:16.901 --> 00:01:21.924
the system was still sending to them maybe
a hot new restaurant that they recommended


00:01:21.924 --> 00:01:27.120
because there's a hot new restaurant, but
it didn't have good vegetarian options.


00:01:27.120 --> 00:01:30.108
So this wasn't a good experience for
anyone and


00:01:30.108 --> 00:01:32.740
there was a strong desire to change this.


00:01:32.740 --> 00:01:37.425
Now, I didn't know how to
synthesize new examples of users or


00:01:37.425 --> 00:01:41.841
new examples of restaurants
because this application had


00:01:41.841 --> 00:01:46.460
a fixed pool of users and
there are only so many restaurants.


00:01:46.460 --> 00:01:51.699
So rather than trying to use data
augmentation to create brand new people or


00:01:51.699 --> 00:01:57.269
restaurants to feed to the training set,
I thought it was more fruitful to see if


00:01:57.269 --> 00:02:03.120
there were features to add to either the
person inputs or to the restaurant inputs.


00:02:03.120 --> 00:02:07.550
Specifically one feature you can
consider adding is a feature that


00:02:07.550 --> 00:02:11.433
indicates whether this person
appears to be vegetarian.


00:02:11.433 --> 00:02:15.190
And this doesn't need to be
a binary value feature 0 1.


00:02:15.190 --> 00:02:20.098
It could be soft features such as
a percentage of fruit order that was


00:02:20.098 --> 00:02:25.630
vegetarian or some other measures of
how likely they seem to be vegetarian.


00:02:25.630 --> 00:02:29.370
And a feature to add on
the restaurant side would be.


00:02:29.370 --> 00:02:32.675
Does this restaurant have
vegetarian options or


00:02:32.675 --> 00:02:35.665
good vegetarian options based on the menu.


00:02:35.665 --> 00:02:40.537
For structure data problems,
usually you have a fixed set of users or


00:02:40.537 --> 00:02:44.233
a fixed set of restaurants or
fixed set of products,


00:02:44.233 --> 00:02:48.937
making it hard to use data augmentation or
collect new data from new


00:02:48.937 --> 00:02:54.000
users that you don't have yet
on restaurants that may or may not exist.


00:02:54.000 --> 00:02:58.966
Instead, adding features,
can be a more fruitful way to


00:02:58.966 --> 00:03:03.931
improve the performance of
the algorithm to fix problems


00:03:03.931 --> 00:03:08.400
like this one,
identify through error analysis.


00:03:08.400 --> 00:03:13.339
Additional features like these,
can be hand coded or they could in turn be


00:03:13.339 --> 00:03:18.757
generated by some learning algorithm,
such as having a learning average home,


00:03:18.757 --> 00:03:22.979
try to read the menu and
classify meals as vegetarian or not, or


00:03:22.979 --> 00:03:28.660
having people code this manually could
also work depending on your application.


00:03:28.660 --> 00:03:33.858
Some other food delivery examples,
we found that there were some users that


00:03:33.858 --> 00:03:39.240
would only ever order a tea and coffee and
some users are only ever order pizza.


00:03:39.240 --> 00:03:44.228
So if the product team wants to
improve the experience of these users,


00:03:44.228 --> 00:03:49.131
a machine learning team might ask
what are the additional features we


00:03:49.131 --> 00:03:53.776
can add to detect who are the people
that only order tea or coffee or


00:03:53.776 --> 00:03:59.140
who are the people that only ever or
the pizza and enrich the user features.


00:03:59.140 --> 00:04:03.494
So as the hope the learning algorithm
make better recommendations for


00:04:03.494 --> 00:04:07.240
restaurants that these
users may be interested in.


00:04:07.240 --> 00:04:09.208
Over the last several years,


00:04:09.208 --> 00:04:13.459
there's been a trend in product
recommendations of a shift from


00:04:13.459 --> 00:04:19.540
collaborative filtering approaches to
what content based filtering approaches.


00:04:19.540 --> 00:04:24.056
Collaborative filtering approaches
is loosely an approach that looks at


00:04:24.056 --> 00:04:27.692
the user, tries to figure out
who is similar to that user and


00:04:27.692 --> 00:04:31.590
then recommends things to you
that people like you also liked.


00:04:31.590 --> 00:04:36.444
In contrast, a content based filtering
approach will tend to look at you


00:04:36.444 --> 00:04:41.379
as a person and look at the description
of the restaurant or look at the menu


00:04:41.379 --> 00:04:46.236
of the restaurants and look at other
information about the restaurant,


00:04:46.236 --> 00:04:49.930
to see if that restaurant is
a good match for you or not.


00:04:49.930 --> 00:04:55.762
The advantage of content based filtering
is that even if there's a new restaurant


00:04:55.762 --> 00:05:00.832
or a new product that hardly anyone
else has liked by actually looking at


00:05:00.832 --> 00:05:05.987
the description of the restaurant,
rather than just looking at who else


00:05:05.987 --> 00:05:11.251
like the restaurants, you can more
quickly make good recommendations.


00:05:11.251 --> 00:05:15.560
This is sometimes also called
the Cold Start Problem.


00:05:15.560 --> 00:05:20.692
How do you recommend a brand new product
that almost no one else has purchased or


00:05:20.692 --> 00:05:22.270
like or dislike so far?


00:05:22.270 --> 00:05:28.323
And one of the ways to do that is to make
sure that you capture good features for


00:05:28.323 --> 00:05:31.940
the things that you
might want to recommend.


00:05:31.940 --> 00:05:34.467
Unlike collaborative filtering, which


00:05:34.467 --> 00:05:39.373
requires a bunch of people to look at the
product and decide if they like it or not,


00:05:39.373 --> 00:05:44.240
before it can decide whether a new user
should be recommended the same product.


00:05:44.240 --> 00:05:49.960
So data iteration for structured
data problems may look like this.


00:05:49.960 --> 00:05:57.240
You start out with some model, train the
model and then carry out error analysis.


00:05:57.240 --> 00:06:02.113
Error analysis can be harder on
structured data problems if there is no


00:06:02.113 --> 00:06:06.655
good baseline such as human level
performance to compare to, and


00:06:06.655 --> 00:06:11.694
human level performance is hard for
structured data because it's really


00:06:11.694 --> 00:06:16.760
difficult for people to recommend
good restaurants even to each other.


00:06:16.760 --> 00:06:22.244
But I found that error analysis can
discover ideas for improvement,


00:06:22.244 --> 00:06:27.180
so can user feedback and so
can benchmarking to competitors.


00:06:27.180 --> 00:06:31.564
But through these methods,
if you can identify a academy or


00:06:31.564 --> 00:06:37.353
a certain type of tag associated your
data that you want to drive improvement,


00:06:37.353 --> 00:06:41.824
then you may be able to go back
to select some features to add,


00:06:41.824 --> 00:06:47.350
such as features to figure out who's
vegetarian and what restaurants have


00:06:47.350 --> 00:06:52.390
good vegetarian options that would
help you to improve your model.


00:06:52.390 --> 00:06:57.228
And because the specific application may
have only a finite list of users and


00:06:57.228 --> 00:07:02.218
restaurants, the users and restaurants
you have maybe all the data you have,


00:07:02.218 --> 00:07:05.181
which is why adding
features to the examples.


00:07:05.181 --> 00:07:10.029
You have maybe a more fruitful approach
compared to trying to come up with


00:07:10.029 --> 00:07:12.110
new users or new restaurants.


00:07:12.110 --> 00:07:17.039
And of course I think features
are a form of data to which


00:07:17.039 --> 00:07:21.970
is why this form of data
iteration where error analysis


00:07:21.970 --> 00:07:26.540
helps you decide how to
modify the features.


00:07:26.540 --> 00:07:30.206
That can be an efficient way as
well of improving your learning


00:07:30.206 --> 00:07:32.240
algorithm's performance.


00:07:32.240 --> 00:07:36.812
I know that many years ago before the rise
of deep Learning, part of the hope for


00:07:36.812 --> 00:07:41.440
deep learning was that you don't have
to hand design features anymore.


00:07:41.440 --> 00:07:46.311
I think that has for the most part come
true for unstructured data problems.


00:07:46.311 --> 00:07:49.610
So I used to hand design features for
images.


00:07:49.610 --> 00:07:51.050
I just don't do that anymore.


00:07:51.050 --> 00:07:53.140
Let the learning I won't figure it out.


00:07:53.140 --> 00:07:56.071
But even with the rise
of modern deep learning,


00:07:56.071 --> 00:08:00.691
if your dataset size isn't massive,
there is still designing of features


00:08:00.691 --> 00:08:05.640
driven by error analysis that can be
useful for many applications today.


00:08:05.640 --> 00:08:09.314
The larger data set, the more
likely it is that a pure end-to-end


00:08:09.314 --> 00:08:11.540
deep learning algorithm can work.


00:08:11.540 --> 00:08:16.463
But for anyone other than the largest tech
companies and sometimes even them for


00:08:16.463 --> 00:08:21.315
some applications, designing features,
especially for structured data


00:08:21.315 --> 00:08:26.120
problems can still be a very important
driver of performance improvements.


00:08:26.120 --> 00:08:28.034
Maybe just don't do that for


00:08:28.034 --> 00:08:32.899
unstructured data nearly as much because
learning algorithms are very good at


00:08:32.899 --> 00:08:37.950
learning features automatically for
images, audio and for text maybe.


00:08:37.950 --> 00:08:42.351
But for structured data, it's okay to
go in and work on the features.
WEBVTT


00:00:00.000 --> 00:00:03.960
As you're working to iteratively
improve your algorithm.


00:00:03.960 --> 00:00:05.490
One thing, that'll help you be a


00:00:05.490 --> 00:00:07.020
bit more efficient is to make


00:00:07.020 --> 00:00:10.640
sure that you have robust
experiment tracking.


00:00:10.640 --> 00:00:13.065
Let's take a look at
some best practices.


00:00:13.065 --> 00:00:15.540
When you're running
dozens or hundreds


00:00:15.540 --> 00:00:17.280
or maybe even more experiments,


00:00:17.280 --> 00:00:18.885
it's easy to forget


00:00:18.885 --> 00:00:21.675
what experiments you
have already run.


00:00:21.675 --> 00:00:24.120
Having a system for tracking


00:00:24.120 --> 00:00:27.900
your experiments can help
you be more efficient


00:00:27.900 --> 00:00:31.740
in making the decisions on
the data or the model or


00:00:31.740 --> 00:00:33.210
hyperparameters to


00:00:33.210 --> 00:00:35.805
systematically improve your
algorithm's performance.


00:00:35.805 --> 00:00:38.775
When you are tracking the
experiments you've run,


00:00:38.775 --> 00:00:40.800
meaning the models
you've trained,


00:00:40.800 --> 00:00:44.370
here are some things I
would urge you to track.


00:00:44.370 --> 00:00:46.920
One, is to keep track of


00:00:46.920 --> 00:00:50.250
what algorithm you're using
and what version of code.


00:00:50.250 --> 00:00:52.560
Keeping a record of
this will make it


00:00:52.560 --> 00:00:54.600
much easier for
you to go back and


00:00:54.600 --> 00:00:57.660
replicate the experiment
you had run maybe


00:00:57.660 --> 00:00:59.490
two weeks ago and whose details


00:00:59.490 --> 00:01:01.215
you may not fully
remember anymore.


00:01:01.215 --> 00:01:04.395
Second, keep track of
the data set you use.


00:01:04.395 --> 00:01:07.810
Third, hyperparameters
and fourth,


00:01:07.810 --> 00:01:09.815
save the results somewhere.


00:01:09.815 --> 00:01:13.350
This should include at least
the high level metrics


00:01:13.350 --> 00:01:16.665
such as accuracy or F1 score
or the relevant metrics,


00:01:16.665 --> 00:01:18.870
but if possible, it'd
be useful to just


00:01:18.870 --> 00:01:21.555
save a copy of the trained model.


00:01:21.555 --> 00:01:24.735
How can you track these things?


00:01:24.735 --> 00:01:27.465
Here are some tracking
tools you might consider.


00:01:27.465 --> 00:01:29.640
A lot of individuals
and sometimes


00:01:29.640 --> 00:01:32.760
even teams will start
off with text files.


00:01:32.760 --> 00:01:35.145
When I'm running
experiment by myself,


00:01:35.145 --> 00:01:39.060
I might use a text file
to just make a note with


00:01:39.060 --> 00:01:40.560
a few lines of text per


00:01:40.560 --> 00:01:43.515
experiment to note
down what I was doing.


00:01:43.515 --> 00:01:45.270
This does not scale well,


00:01:45.270 --> 00:01:48.675
but it may be okay for
small experiments.


00:01:48.675 --> 00:01:50.700
A lot of teams then migrate from


00:01:50.700 --> 00:01:52.500
text files to spreadsheets,


00:01:52.500 --> 00:01:53.910
especially shared spreadsheets,


00:01:53.910 --> 00:01:56.130
if you're working in a team where


00:01:56.130 --> 00:01:58.410
different columns of a
spreadsheet could keep


00:01:58.410 --> 00:02:01.140
track of the different things


00:02:01.140 --> 00:02:02.370
you want to track for


00:02:02.370 --> 00:02:04.455
the different experiments
you're running.


00:02:04.455 --> 00:02:08.160
Spreadsheets actually
scale quite a bit further,


00:02:08.160 --> 00:02:10.320
especially shared
spreadsheets that


00:02:10.320 --> 00:02:12.855
multiple members of a team
may be able to look at.


00:02:12.855 --> 00:02:14.550
But beyond a certain point,


00:02:14.550 --> 00:02:16.935
some teams will also consider


00:02:16.935 --> 00:02:21.585
migrating to a more formal
experiment tracking system.


00:02:21.585 --> 00:02:24.090
The space of experiment
tracking systems


00:02:24.090 --> 00:02:26.460
is still evolving rapidly,


00:02:26.460 --> 00:02:30.225
and so does a growing
set of tools out there.


00:02:30.225 --> 00:02:33.375
But some examples
include Weight &Biases,


00:02:33.375 --> 00:02:36.525
Comet, MLflow, Sage Maker Studio.


00:02:36.525 --> 00:02:39.000
Landing.AI where I am CEO also has


00:02:39.000 --> 00:02:41.520
its own experiment
tracking tool focusing on


00:02:41.520 --> 00:02:44.370
computer vision and
manufacturing applications.


00:02:44.370 --> 00:02:48.540
When I'm trying to
use a tracking tool,


00:02:48.540 --> 00:02:49.560
whether a text file or


00:02:49.560 --> 00:02:51.930
a spreadsheet or
some larger system,


00:02:51.930 --> 00:02:54.945
here are some of
things I look at.


00:02:54.945 --> 00:02:57.930
First is, does it give me


00:02:57.930 --> 00:03:02.650
all the information needed
to replicate the results?


00:03:03.860 --> 00:03:07.165
In terms of replicability,


00:03:07.165 --> 00:03:09.510
one thing to watch out for is if


00:03:09.510 --> 00:03:13.110
your learning algorithm
pulls data off the Internet.


00:03:13.110 --> 00:03:15.690
Because data off the
Internet can change,


00:03:15.690 --> 00:03:18.195
that can decrease replicability


00:03:18.195 --> 00:03:22.095
unless you're careful in how
your system is implemented.


00:03:22.095 --> 00:03:25.950
Second, tools that
help you quickly


00:03:25.950 --> 00:03:28.290
understand the
experimental results


00:03:28.290 --> 00:03:29.955
of a specific training run,


00:03:29.955 --> 00:03:32.850
ideally with useful
summary metrics and


00:03:32.850 --> 00:03:36.195
maybe even a bit of
a in-depth analysis,


00:03:36.195 --> 00:03:38.520
can help you more quickly look


00:03:38.520 --> 00:03:40.620
at your most recent
experiments or even


00:03:40.620 --> 00:03:45.375
look at older experiments and
remember what had happened.


00:03:45.375 --> 00:03:49.185
Finally, some other
features to consider,


00:03:49.185 --> 00:03:50.800
resource monitoring,


00:03:50.800 --> 00:03:54.375
how much CPU/GPU memory
resources do it use?


00:03:54.375 --> 00:03:58.590
Or tools to help you
visualize the trained model


00:03:58.590 --> 00:04:00.690
or even tools to help you with


00:04:00.690 --> 00:04:03.060
a more in-depth error analysis.


00:04:03.060 --> 00:04:05.790
I've found all of
these to sometimes be


00:04:05.790 --> 00:04:09.840
useful features of experiment
tracking frameworks.


00:04:09.840 --> 00:04:12.510
Rather than worrying
too much about exactly


00:04:12.510 --> 00:04:15.255
which experiment tracking
framework to use though,


00:04:15.255 --> 00:04:17.040
the number one thing I


00:04:17.040 --> 00:04:19.155
hope you take away
from this video is,


00:04:19.155 --> 00:04:21.659
do try to have some system,


00:04:21.659 --> 00:04:23.880
even if it's just a text file or


00:04:23.880 --> 00:04:26.400
just a spreadsheet
for keeping track of


00:04:26.400 --> 00:04:28.230
your experiments and include


00:04:28.230 --> 00:04:31.350
as much information as is
convenient to include.


00:04:31.350 --> 00:04:33.630
Because later on, if
you try to look back,


00:04:33.630 --> 00:04:36.540
remember how you had
generated a certain model,


00:04:36.540 --> 00:04:38.580
having that information
would be really


00:04:38.580 --> 00:04:42.910
useful for helping you to
replicate your own results.
WEBVTT


00:00:00.540 --> 00:00:07.331
You've learned about taking a data
centric approach to AI development.


00:00:07.331 --> 00:00:09.920
In this last video for this week,


00:00:09.920 --> 00:00:16.239
I'd like to leave you with a thought
on shifting from big data to good data.


00:00:16.239 --> 00:00:19.915
Here's what I mean,
a lot of modern AI had grown up in


00:00:19.915 --> 00:00:24.680
large consumer internet companies
with maybe a billion users, and


00:00:24.680 --> 00:00:28.800
does companies like that have
a lot of data on their users.


00:00:28.800 --> 00:00:31.106
If you have big data like that,


00:00:31.106 --> 00:00:36.740
by all means it could help the performance
of your algorithm tremendously.


00:00:36.740 --> 00:00:41.676
But both software consumer internet but
equally importantly for


00:00:41.676 --> 00:00:47.100
many other industries,
there's just isn't a billion data points.


00:00:47.100 --> 00:00:50.521
And I think it may be
even more important for


00:00:50.521 --> 00:00:56.140
those applications to focus not
just on big data but on good data.


00:00:56.140 --> 00:01:01.379
I found that if you are able to ensure
consistently high quality data in all


00:01:01.379 --> 00:01:06.873
phases in the machine learning project
life cycle, that is key to making sure


00:01:06.873 --> 00:01:12.400
that you have a high performance and
reliable machine learning deployment.


00:01:12.400 --> 00:01:18.101
What I mean by good data, I think
good data covers the important cases,


00:01:18.101 --> 00:01:22.590
so you should have good
coverage of different inputs x.


00:01:22.590 --> 00:01:27.421
And if you find out that you don't
have enough data with speech,


00:01:27.421 --> 00:01:32.610
with cafe noise, data augmentation
can help you get more data,


00:01:32.610 --> 00:01:36.750
get more diverse inputs x,
to give you that coverage.


00:01:36.750 --> 00:01:42.840
So, we spent quite a bit of time talking
about this in this week's material.


00:01:42.840 --> 00:01:47.704
Good data is also defined
consistently with definition


00:01:47.704 --> 00:01:50.720
of labels y that's unambiguous.


00:01:50.720 --> 00:01:53.312
We haven't talked about this yet but


00:01:53.312 --> 00:01:57.010
we'll go into much greater
depth on this next week.


00:01:57.010 --> 00:02:02.040
Good data also has timely
feedback from production data.


00:02:02.040 --> 00:02:06.309
We actually talked about
this last week when we were


00:02:06.309 --> 00:02:10.581
covering the deployment
section in terms of having


00:02:10.581 --> 00:02:15.580
monitoring systems to track
concept drift and data drift.


00:02:15.580 --> 00:02:20.240
And finally,
you do need a reasonable size data set.


00:02:20.240 --> 00:02:25.415
So to summarize during the machine
learning project lifecycle,


00:02:25.415 --> 00:02:30.398
we've talked about during
the deployment phase last week how to


00:02:30.398 --> 00:02:34.081
make sure you have timely
feedback this week.


00:02:34.081 --> 00:02:38.778
As we talked about modeling,
we also included in our discussion how to


00:02:38.778 --> 00:02:43.740
make sure you have, hopefully
good coverage of important cases.


00:02:43.740 --> 00:02:48.787
Next week, when we dive into data
definition, we'll spend much more


00:02:48.787 --> 00:02:53.851
time to talk about how to make sure
your data is defined consistently.


00:02:53.851 --> 00:02:58.307
And I hope that with the ideas
conveyed last week, this week, and


00:02:58.307 --> 00:03:03.817
next week you'll be armed with the tools
you need to give your learning algorithm


00:03:03.817 --> 00:03:09.340
good data through all phases of
the machine learning project life cycle.


00:03:09.340 --> 00:03:10.710
So, that's it.


00:03:10.710 --> 00:03:15.510
Congratulations on getting to the end
of this week's videos on modeling.


00:03:15.510 --> 00:03:20.586
I look forward to diving more
deeply with you into the data


00:03:20.586 --> 00:03:25.540
part of the full cycle of
a machine learning project.


00:03:25.540 --> 00:03:26.508
And next week,


00:03:26.508 --> 00:03:31.670
we'll also have a short optional section
on scoping machine learning projects.


00:03:31.670 --> 00:03:33.960
I look forward to see you next week.
WEBVTT


00:00:00.000 --> 00:00:02.100
Welcome back. You're now in


00:00:02.100 --> 00:00:04.285
the 3rd and final
week of this course,


00:00:04.285 --> 00:00:05.640
just one more week,


00:00:05.640 --> 00:00:07.050
and then you'll be done with


00:00:07.050 --> 00:00:09.555
this 1st course of
the specialization.


00:00:09.555 --> 00:00:12.115
In this week we dive into data.


00:00:12.115 --> 00:00:16.020
How do you get data that
sets up your training,


00:00:16.020 --> 00:00:18.165
your modeling for success?


00:00:18.165 --> 00:00:22.350
But first, why is defining
what data to use even hard?


00:00:22.350 --> 00:00:23.695
Let's look at an example.


00:00:23.695 --> 00:00:27.930
I'm going to use the example
of detecting Iguanas.


00:00:27.930 --> 00:00:30.570
One of my friends, [inaudible]
really likes Iguanas,


00:00:30.570 --> 00:00:33.280
so I have a bunch of iguana
pictures floating around.


00:00:33.280 --> 00:00:35.190
Let's say that you've gone into


00:00:35.190 --> 00:00:37.440
the forest and
collected hundreds of


00:00:37.440 --> 00:00:39.870
pictures like these and you send


00:00:39.870 --> 00:00:42.374
these pictures to labelers
with the instructions,


00:00:42.374 --> 00:00:43.770
"Please use bounding boxes to


00:00:43.770 --> 00:00:45.810
indicate the position
of Iguanas.''


00:00:45.810 --> 00:00:48.900
One labeler, may label
it like this and say,


00:00:48.900 --> 00:00:50.790
one iguana, two Iguanas.


00:00:50.790 --> 00:00:52.170
This labeler did a good job.


00:00:52.170 --> 00:00:54.630
A 2nd labeler that is
equally hard working,


00:00:54.630 --> 00:00:56.505
equally diligent may say,


00:00:56.505 --> 00:00:58.740
look, the iguana on the left has


00:00:58.740 --> 00:01:01.590
a tail that goes all the way
to the right to this image.


00:01:01.590 --> 00:01:05.955
The 2nd labeler may say
one iguana, two iguanas.


00:01:05.955 --> 00:01:08.925
Good job, labeler. Hard to
follow this labor either.


00:01:08.925 --> 00:01:10.830
A 3rd labeler may say, well,


00:01:10.830 --> 00:01:12.480
I'm going to look
through all hundreds of


00:01:12.480 --> 00:01:14.325
images and label them all,


00:01:14.325 --> 00:01:16.980
and I'm going to
use bounding boxes,


00:01:16.980 --> 00:01:18.870
and so let me indicate


00:01:18.870 --> 00:01:21.450
the position iguanas and draw
a bounding box like that.


00:01:21.450 --> 00:01:24.450
Three diligent, hard
working labelers can


00:01:24.450 --> 00:01:26.670
come up with these three
very different ways


00:01:26.670 --> 00:01:28.290
of labeling iguanas,


00:01:28.290 --> 00:01:30.900
and maybe any of these
is actually fine.


00:01:30.900 --> 00:01:34.020
I would prefer the top two
rather than the 3rd one.


00:01:34.020 --> 00:01:36.390
But any of these
labeling conventions


00:01:36.390 --> 00:01:38.445
could result in your
learning algorithm


00:01:38.445 --> 00:01:41.115
learning a pretty
good iguana detector.


00:01:41.115 --> 00:01:43.500
But what is not fine is if 1/3 of


00:01:43.500 --> 00:01:46.710
your labelers use the 1st
and 1/3 the 2nd, and 1/3,


00:01:46.710 --> 00:01:48.300
the 3rd labeling convention,


00:01:48.300 --> 00:01:51.120
because then your labels
are inconsistent,


00:01:51.120 --> 00:01:53.460
and this is confusing to
the learning algorithm.


00:01:53.460 --> 00:01:56.220
While, the iguana
example was a fun one.


00:01:56.220 --> 00:01:58.080
You see this type of effect in


00:01:58.080 --> 00:02:01.170
many practical computer
vision problems as well.


00:02:01.170 --> 00:02:04.270
Let's use the phone
defect detection example.


00:02:04.270 --> 00:02:06.060
If you ask a labeler to use


00:02:06.060 --> 00:02:09.195
bounding boxes to indicate
significant defects,


00:02:09.195 --> 00:02:11.625
maybe one labeler will
look and then go, ''Well,


00:02:11.625 --> 00:02:13.920
clearly the scratch is the
most significant defect.


00:02:13.920 --> 00:02:16.365
Let me draw a bounding
box on that.''


00:02:16.365 --> 00:02:18.960
A 2nd labeler may look
at his phone and say,


00:02:18.960 --> 00:02:21.090
"There are actually two
significant defects.


00:02:21.090 --> 00:02:22.530
There's a big scratch,


00:02:22.530 --> 00:02:24.300
and then there's that small mark


00:02:24.300 --> 00:02:26.010
there,'' it's called a pit mark,


00:02:26.010 --> 00:02:29.910
like if someone poked a phone
with a sharp screwdriver.


00:02:29.910 --> 00:02:33.180
I think the 2nd labeler
probably did a better job.


00:02:33.180 --> 00:02:36.360
But then a 3rd labeler
may look at this and say,


00:02:36.360 --> 00:02:38.010
well, here's a bounding box


00:02:38.010 --> 00:02:40.095
that shows you where
the defects are.


00:02:40.095 --> 00:02:42.360
Between these three labels,


00:02:42.360 --> 00:02:45.540
probably the one in the
middle would work the best.


00:02:45.540 --> 00:02:48.060
But this is a very
typical example


00:02:48.060 --> 00:02:51.090
of inconsistence labeling
that you will get


00:02:51.090 --> 00:02:53.820
back from a labeling process with


00:02:53.820 --> 00:02:57.375
even slightly ambiguous
labeling instructions,


00:02:57.375 --> 00:02:59.160
and if you can


00:02:59.160 --> 00:03:02.430
consistently label the
data with one convention,


00:03:02.430 --> 00:03:03.660
maybe the one in middle,


00:03:03.660 --> 00:03:05.640
you're learning algorithm
will do better.


00:03:05.640 --> 00:03:10.620
What we would do in this week
is dive into best practices


00:03:10.620 --> 00:03:12.389
for the data stage


00:03:12.389 --> 00:03:16.170
of the full cycle of a
machine learning project.


00:03:16.170 --> 00:03:18.930
Specifically, we'll
talk about how


00:03:18.930 --> 00:03:21.900
to define what is the data,


00:03:21.900 --> 00:03:23.970
what should be x
and what should be


00:03:23.970 --> 00:03:26.715
y and establish a baseline


00:03:26.715 --> 00:03:30.090
and doing that well will set you


00:03:30.090 --> 00:03:33.420
up to label and
organize the data well,


00:03:33.420 --> 00:03:35.775
which would give
you a good data set


00:03:35.775 --> 00:03:38.955
for when you move into
the modeling phase,


00:03:38.955 --> 00:03:40.530
which you already saw last week.


00:03:40.530 --> 00:03:42.540
Many machine learning
researchers and


00:03:42.540 --> 00:03:44.850
many machine learning
engineers had started


00:03:44.850 --> 00:03:46.890
off downloading data off


00:03:46.890 --> 00:03:49.230
the Internet to
experiment with models,


00:03:49.230 --> 00:03:52.230
so using data prepared
by someone else.


00:03:52.230 --> 00:03:53.955
Nothing at all wrong with that,


00:03:53.955 --> 00:03:57.180
and for many practical
applications,


00:03:57.180 --> 00:03:59.520
the way you prepare your
data sets will have


00:03:59.520 --> 00:04:00.540
a huge impact on


00:04:00.540 --> 00:04:02.940
the success of your
machine learning projects.


00:04:02.940 --> 00:04:04.560
In the next video,


00:04:04.560 --> 00:04:06.840
we'll take a look at
some more examples


00:04:06.840 --> 00:04:09.315
of how data can be ambiguous,


00:04:09.315 --> 00:04:11.790
so that this will set us
up later this week for


00:04:11.790 --> 00:04:15.060
some techniques for improving
the quality of your data.


00:04:15.060 --> 00:04:17.890
Let's go on to the next video.
WEBVTT


00:00:00.000 --> 00:00:02.340
In the last video, you saw how


00:00:02.340 --> 00:00:05.835
the right bounding boxes for
an image can be ambiguous.


00:00:05.835 --> 00:00:10.665
Let's take a look at some more
label ambiguity examples.


00:00:10.665 --> 00:00:13.170
We briefly touched on


00:00:13.170 --> 00:00:16.530
speech recognition in the
first week of this course.


00:00:16.530 --> 00:00:18.345
Here's another example.


00:00:18.345 --> 00:00:21.340
Given this audio clip,


00:00:24.460 --> 00:00:26.960
sounds like someone
was standing on


00:00:26.960 --> 00:00:28.820
a busy road side asking for


00:00:28.820 --> 00:00:32.520
the nearest gas station
and then a car drove past.


00:00:32.520 --> 00:00:34.415
Did they say something
right after that?


00:00:34.415 --> 00:00:37.020
I don't know. One way to


00:00:37.020 --> 00:00:40.590
transcribe this would be
"Um, nearest gas station."


00:00:40.590 --> 00:00:45.120
In some places, people
spell "um" with two m's.


00:00:45.120 --> 00:00:47.395
That would be a different
way to spell it.


00:00:47.395 --> 00:00:49.820
We could have used dot-dot-dot or


00:00:49.820 --> 00:00:52.040
ellipses instead of
the comma as well,


00:00:52.040 --> 00:00:53.920
which would be another ambiguity.


00:00:53.920 --> 00:00:58.715
Or given the audio had
noise after the last words.


00:00:58.715 --> 00:01:00.990
Nearest gas station.


00:01:01.780 --> 00:01:05.315
Did they say something
after nearest gas station?


00:01:05.315 --> 00:01:07.280
I'm not sure actually.


00:01:07.280 --> 00:01:11.090
Would you transcribe
it like this instead?


00:01:11.090 --> 00:01:15.380
There are combinatorially
many ways to transcribe this.


00:01:15.380 --> 00:01:18.604
With one M or two M's,
comma or ellipses,


00:01:18.604 --> 00:01:22.360
whether to write unintelligible
at the end of this.


00:01:22.360 --> 00:01:25.460
Being able to standardize
on one convention will


00:01:25.460 --> 00:01:28.340
help your speech
recognition algorithm.


00:01:28.340 --> 00:01:32.075
Let's also look an example
of structured data.


00:01:32.075 --> 00:01:35.495
A common application in


00:01:35.495 --> 00:01:39.255
many large companies
is user ID merge.


00:01:39.255 --> 00:01:40.625
That's when you have


00:01:40.625 --> 00:01:44.900
multiple data records that
you think correspond to


00:01:44.900 --> 00:01:47.150
the same person and you want to


00:01:47.150 --> 00:01:50.310
merge these user data
records together.


00:01:50.310 --> 00:01:53.090
For example, say you
run a website that


00:01:53.090 --> 00:01:56.270
offers online listings of jobs.


00:01:56.270 --> 00:01:59.585
This may be one data record that


00:01:59.585 --> 00:02:00.620
you have from one of


00:02:00.620 --> 00:02:03.110
your registered users
with the email,


00:02:03.110 --> 00:02:05.705
first name, last
name and address.


00:02:05.705 --> 00:02:09.050
Now, say your company acquires


00:02:09.050 --> 00:02:11.870
a second company that runs


00:02:11.870 --> 00:02:15.035
a mobile app that
allows people to login,


00:02:15.035 --> 00:02:16.790
to chat and get advice from each


00:02:16.790 --> 00:02:19.205
other about their resumes.


00:02:19.205 --> 00:02:21.560
It seems synergistic
for your business.


00:02:21.560 --> 00:02:24.185
If you run a listing
of online jobs,


00:02:24.185 --> 00:02:27.890
maybe you merge or acquire
a second company that runs


00:02:27.890 --> 00:02:30.470
a mobile app that lets
people chat about


00:02:30.470 --> 00:02:34.220
their resumes and
from this mobile app,


00:02:34.220 --> 00:02:37.760
you have a different
database of users.


00:02:37.760 --> 00:02:44.725
Given this data
record and this one,


00:02:44.725 --> 00:02:48.490
do you think these two
are the same person?


00:02:48.490 --> 00:02:52.550
One approach to the
User ID merge problem,


00:02:52.550 --> 00:02:55.790
is the use of supervised
learning algorithm that takes as


00:02:55.790 --> 00:02:59.600
inputs to user data
records and tries to


00:02:59.600 --> 00:03:03.500
outputs either one or
zero based on whether it


00:03:03.500 --> 00:03:05.330
thinks these two are actually


00:03:05.330 --> 00:03:07.885
the same physical human being.


00:03:07.885 --> 00:03:11.030
If you have a way to get
ground true data records,


00:03:11.030 --> 00:03:12.920
such as if a handful
of users are willing


00:03:12.920 --> 00:03:14.975
to explicitly link
the two accounts,


00:03:14.975 --> 00:03:17.600
then that could be a good set of


00:03:17.600 --> 00:03:20.720
labeled examples to
train an algorithm.


00:03:20.720 --> 00:03:24.545
But if you don't have such
a ground true set of data,


00:03:24.545 --> 00:03:30.050
what many companies have
done is ask human labors,


00:03:30.050 --> 00:03:31.880
sometimes a product
management team


00:03:31.880 --> 00:03:34.190
to just manually look
at some pairs of


00:03:34.190 --> 00:03:36.440
records that have
been filtered to have


00:03:36.440 --> 00:03:40.790
maybe similar names
or similar ZIP codes,


00:03:40.790 --> 00:03:43.895
and then to just use human
judgment to determine


00:03:43.895 --> 00:03:48.235
if these two records appear
to be the same person.


00:03:48.235 --> 00:03:51.190
Because whether these
two records really


00:03:51.190 --> 00:03:54.220
is the same person, is
genuinely ambiguous.


00:03:54.220 --> 00:03:56.440
They may and they may not be


00:03:56.440 --> 00:04:01.655
different people will label
these records inconsistently.


00:04:01.655 --> 00:04:05.005
If there's a way to just get them


00:04:05.005 --> 00:04:08.245
to label the data a
little more consistently,


00:04:08.245 --> 00:04:11.260
you see some examples
of how to do this later


00:04:11.260 --> 00:04:14.455
even when the ground
truth is ambiguous,


00:04:14.455 --> 00:04:15.940
then that can help


00:04:15.940 --> 00:04:18.130
the performance of your
learning algorithm.


00:04:18.130 --> 00:04:19.960
User ID merging is


00:04:19.960 --> 00:04:22.335
a very common function
in many companies.


00:04:22.335 --> 00:04:24.190
Let me just ask you to please do


00:04:24.190 --> 00:04:28.820
this only in ways
that are respectful


00:04:28.820 --> 00:04:31.070
of the users data and


00:04:31.070 --> 00:04:35.780
their privacy and only if
you're using the data in a way,


00:04:35.780 --> 00:04:39.520
consistence with what they
have given you permission for.


00:04:39.520 --> 00:04:42.535
User privacy is really important.


00:04:42.535 --> 00:04:46.645
A few other examples
from structured data.


00:04:46.645 --> 00:04:48.710
If you are trying to use


00:04:48.710 --> 00:04:52.760
the learning algorithm to
look at the user account like


00:04:52.760 --> 00:05:02.790
this and predict is it a
bot or a spam account?


00:05:04.970 --> 00:05:08.820
Sometimes that can be ambiguous.


00:05:08.820 --> 00:05:13.145
Or if you look at
a online purchase,


00:05:13.145 --> 00:05:17.890
is this a 40-length transaction?


00:05:17.890 --> 00:05:21.290
Has someone stolen
accounts and is using


00:05:21.290 --> 00:05:23.270
stolen accounts to interact


00:05:23.270 --> 00:05:25.895
with your websites or
to make purchases?


00:05:25.895 --> 00:05:29.530
Sometimes that too is ambiguous.


00:05:29.530 --> 00:05:32.225
Or if you look at


00:05:32.225 --> 00:05:34.895
someone's interactions
with your website


00:05:34.895 --> 00:05:36.725
and you want to know,


00:05:36.725 --> 00:05:38.750
are they looking for a new job at


00:05:38.750 --> 00:05:40.850
this moment in time based on


00:05:40.850 --> 00:05:42.200
how someone behaves on


00:05:42.200 --> 00:05:44.935
a job board website
or a resume chat app,


00:05:44.935 --> 00:05:47.840
you can sometimes guess if
they're looking for a job,


00:05:47.840 --> 00:05:49.295
but it's hard to be sure.


00:05:49.295 --> 00:05:52.100
That's also a little
bit ambiguous.


00:05:52.100 --> 00:05:53.720
In the face of


00:05:53.720 --> 00:05:55.910
potentially very important and


00:05:55.910 --> 00:05:58.205
valuable prediction
tasks like these,


00:05:58.205 --> 00:06:01.200
the ground truth
can be ambiguous.


00:06:01.200 --> 00:06:03.920
If you ask people to take


00:06:03.920 --> 00:06:05.240
their best guess at


00:06:05.240 --> 00:06:08.305
the ground truth label
for tasks like these,


00:06:08.305 --> 00:06:11.420
giving labeling instructions
that results in


00:06:11.420 --> 00:06:16.220
more consistent and less
noisy and less random labels


00:06:16.220 --> 00:06:19.220
will improve the performance
of your learning algorithm.


00:06:19.220 --> 00:06:24.769
When defining the data for
your learning algorithm,


00:06:24.769 --> 00:06:29.385
here are some
important questions.


00:06:29.385 --> 00:06:32.295
First, what is the input x?


00:06:32.295 --> 00:06:34.460
For example, if you are trying to


00:06:34.460 --> 00:06:36.875
detect defects on smart phones,


00:06:36.875 --> 00:06:38.900
for the pictures you're taking,


00:06:38.900 --> 00:06:40.505
is the lighting good enough?


00:06:40.505 --> 00:06:42.605
Is the camera
contrast good enough?


00:06:42.605 --> 00:06:45.140
Is the camera
resolution good enough?


00:06:45.140 --> 00:06:49.235
If you find that you have a
bunch of pictures like these,


00:06:49.235 --> 00:06:50.360
which are so dark,


00:06:50.360 --> 00:06:54.110
it's hard even for a person
to see what's going on.


00:06:54.110 --> 00:06:56.780
The right thing to
do may not be to


00:06:56.780 --> 00:06:59.570
take this input x
and just label it.


00:06:59.570 --> 00:07:02.600
It may be to go to
the factory and


00:07:02.600 --> 00:07:06.000
politely request
improving the lighting


00:07:06.000 --> 00:07:07.370
because it is only with


00:07:07.370 --> 00:07:11.075
this better image quality
that the labor can then


00:07:11.075 --> 00:07:16.900
more easily see scratches
like this and label them.


00:07:16.900 --> 00:07:21.050
Sometimes if your sensor or


00:07:21.050 --> 00:07:22.550
your imaging solution or


00:07:22.550 --> 00:07:26.255
your audio recording
solution is not good enough,


00:07:26.255 --> 00:07:30.320
the best thing you
could do is recognize


00:07:30.320 --> 00:07:32.470
that if even a person can't


00:07:32.470 --> 00:07:35.330
look at the input and
tell us what's going on,


00:07:35.330 --> 00:07:39.065
then improving the quality of


00:07:39.065 --> 00:07:45.945
your sensor or improving
the quality of the input x,


00:07:45.945 --> 00:07:49.315
that can be an
important first step


00:07:49.315 --> 00:07:51.160
to ensuring your
learning algorithm


00:07:51.160 --> 00:07:53.315
can have reasonable performance.


00:07:53.315 --> 00:07:56.215
For structured data problems,


00:07:56.215 --> 00:07:58.990
defining whether the
features to include can


00:07:58.990 --> 00:08:00.070
have a huge impact on


00:08:00.070 --> 00:08:01.825
your learning
algorithm's performance.


00:08:01.825 --> 00:08:04.600
For example, for user ID merge,


00:08:04.600 --> 00:08:08.065
if you have a way of getting
the user's location,


00:08:08.065 --> 00:08:10.190
even a rough GPS location.


00:08:10.190 --> 00:08:13.150
If you have permission
from the user to use that,


00:08:13.150 --> 00:08:15.940
can be a very useful tool
for deciding whether


00:08:15.940 --> 00:08:19.745
two user accounts actually
belong to the same person.


00:08:19.745 --> 00:08:22.690
Of course, please do this type
of thing only if you have


00:08:22.690 --> 00:08:26.260
permission from the user to
use their data this way.


00:08:26.260 --> 00:08:29.720
In addition to
defining the input x,


00:08:29.720 --> 00:08:31.730
you also have to figure out what


00:08:31.730 --> 00:08:34.070
should be the target label y.


00:08:34.070 --> 00:08:37.220
As you've seen from the
preceding examples,


00:08:37.220 --> 00:08:39.185
one key question is,


00:08:39.185 --> 00:08:43.150
how can we ensure labels
give consistent labels?


00:08:43.150 --> 00:08:44.810
In the last video and this video,


00:08:44.810 --> 00:08:47.180
you saw a variety
of problems with


00:08:47.180 --> 00:08:49.820
the labels being ambiguous
or in some cases,


00:08:49.820 --> 00:08:53.705
the input x not being
sufficiently informative,


00:08:53.705 --> 00:08:55.715
such as an image is too dark.


00:08:55.715 --> 00:08:57.890
Let's take these
data issues and put


00:08:57.890 --> 00:09:00.275
them into more
systematic framework.


00:09:00.275 --> 00:09:02.210
That will allow us to devise


00:09:02.210 --> 00:09:04.295
solutions in a more
systematic way.


00:09:04.295 --> 00:09:07.830
Let's go on to the next
video to take a look.
WEBVTT


00:00:00.540 --> 00:00:02.925
I'd like to share with you
a useful framework for


00:00:02.925 --> 00:00:06.373
thinking about different major
types of machine learning projects.


00:00:06.373 --> 00:00:11.047
It turns out that the best practices for
organizing data for one type can be quite


00:00:11.047 --> 00:00:14.906
different than the best practices for
totally different types.


00:00:14.906 --> 00:00:18.738
Let's take a look at whether these major
types of machine learning projects.


00:00:18.738 --> 00:00:21.222
Let's fall in this two by two grid.


00:00:21.222 --> 00:00:26.631
One axis will be whether your machine
learning problem uses unstructured data or


00:00:26.631 --> 00:00:27.912
structured data.


00:00:27.912 --> 00:00:33.592
I found that the best practices for these
are very different, mainly because humans


00:00:33.592 --> 00:00:38.712
are great at processing unstructured data,
the images and audio and text,


00:00:38.712 --> 00:00:43.442
and not as good at processing
structured data like database records.


00:00:43.442 --> 00:00:47.251
The second axis is
the size of your data set.


00:00:47.251 --> 00:00:50.786
Do you have a relatively small data set?


00:00:50.786 --> 00:00:52.147
or do you have a very large data set?


00:00:52.147 --> 00:00:58.448
There is no precise definition of what
exactly is small and what is large?


00:00:58.448 --> 00:01:03.373
But I'm going to use as
a slightly arbitrary threshold,


00:01:03.373 --> 00:01:08.197
whether you have over 10,000 examples or
not.


00:01:08.197 --> 00:01:12.148
And clearly this boundary
is a little bit fuzzy and


00:01:12.148 --> 00:01:16.957
the transitions from small to
big data sets is a gradual one.


00:01:16.957 --> 00:01:23.059
But I found that best practices if
you have, say 100 or 1000 examples,


00:01:23.059 --> 00:01:28.711
smaller data sets is pretty different
than we have a very large data set.


00:01:28.711 --> 00:01:34.941
And the reason I chose the number 10,000
is that's roughly the size beyond


00:01:34.941 --> 00:01:40.723
which it becomes quite painful to
examine every single example yourself.


00:01:40.723 --> 00:01:45.325
If you have 1000 examples, you could
probably examine every example yourself.


00:01:45.325 --> 00:01:51.071
But when you have, 10,000,
100,000, million examples,


00:01:51.071 --> 00:01:55.448
it becomes very time consuming for
you as an individual or


00:01:55.448 --> 00:02:01.394
maybe a couple of machinery and engineers
to manually look at every example.


00:02:01.394 --> 00:02:05.703
So that affects the best
practices as well.


00:02:05.703 --> 00:02:07.809
Let's look at some examples.


00:02:07.809 --> 00:02:13.096
If you are training a manufacturing
visual inspection from just 100 examples


00:02:13.096 --> 00:02:18.149
of stretch phones, that's unstructured
data because this is image data and


00:02:18.149 --> 00:02:20.107
it's pretty small data set.


00:02:20.107 --> 00:02:25.342
If you are trying to predict housing
prices based on the size of the halls and


00:02:25.342 --> 00:02:29.247
other features of the house,
from just 52 examples,


00:02:29.247 --> 00:02:31.923
then there's a structured data set.


00:02:31.923 --> 00:02:37.931
We've just real value features and
a relatively small data sets.


00:02:37.931 --> 00:02:43.257
If you are carrying out speech recognition
from 50 million train examples,


00:02:43.257 --> 00:02:45.282
that's unstructured data.


00:02:45.282 --> 00:02:51.041
But you have a lot of data or
if you are trying to recommend products.


00:02:51.041 --> 00:02:56.132
So online shopping recommendations and
you have a million users in your database,


00:02:56.132 --> 00:03:00.657
then that's a structured problem with
relatively large amount of data.


00:03:00.657 --> 00:03:03.790
For a lot of unstructured data problems,


00:03:03.790 --> 00:03:10.043
people, Can help you to label data and


00:03:10.043 --> 00:03:16.000
data augmentation such as synthesizing
new images or synthesizing new audio.


00:03:16.000 --> 00:03:20.881
And there's some emerging techniques for
synthesizing new text as well,


00:03:20.881 --> 00:03:23.059
but data augmentation can help.


00:03:23.059 --> 00:03:27.742
So for manufacturing vision inspection,
you can use data augmentation


00:03:27.742 --> 00:03:32.589
to maybe generate more pictures of
smart films or for speech recognition.


00:03:32.589 --> 00:03:35.822
Data augmentation can help
you synthesize audio clips


00:03:35.822 --> 00:03:37.938
with different background noise.


00:03:37.938 --> 00:03:41.305
In contrast for structured data problems,


00:03:41.305 --> 00:03:47.220
it can be harder to obtain more data and
also harder to use data augmentation,


00:03:47.220 --> 00:03:51.865
if only 50 houses have been so
recently in that geography.


00:03:51.865 --> 00:03:55.588
Well, it's hard to synthesize
new houses that don't exist or


00:03:55.588 --> 00:03:58.747
if you have a million users
in your database, again,


00:03:58.747 --> 00:04:02.416
it's hard to synthesize new
users that don't really exist.


00:04:02.416 --> 00:04:06.931
And it's also harder not impossible,
still worth trying, but it may or


00:04:06.931 --> 00:04:10.126
may not be possible to get
humans to label the data.


00:04:10.126 --> 00:04:12.204
So I find that the best practices for


00:04:12.204 --> 00:04:15.742
unstructured versus structured
data are quite different.


00:04:15.742 --> 00:04:18.139
The second axis is
the size of your data set.


00:04:18.139 --> 00:04:25.215
When you have a relatively small data set,
having clean labels is critical.


00:04:25.215 --> 00:04:27.904
If you have 100 training examples,


00:04:27.904 --> 00:04:33.285
then if just one of the examples is
mislabeled, that's 1% of your data set.


00:04:33.285 --> 00:04:38.476
And because the data set is small enough
for you or a small team to go through it


00:04:38.476 --> 00:04:44.256
efficiently, it may well be aware of your
while to go through that 100 examples.


00:04:44.256 --> 00:04:48.605
And make sure that every one of those
examples is labelled in a clean and


00:04:48.605 --> 00:04:53.193
consistent way, meaning according
to a consistent labeling standard.


00:04:53.193 --> 00:05:01.695
In contrast, if you have a million
data points, it can be harder.


00:05:01.695 --> 00:05:03.300
Maybe impossible for


00:05:03.300 --> 00:05:08.481
a small machine learning team to
manually go through every example.


00:05:08.481 --> 00:05:12.055
Having clean labels is still very helpful,
don't get me wrong.


00:05:12.055 --> 00:05:16.624
Even when you have a lot of data, clean
labels is better than non clean ones.


00:05:16.624 --> 00:05:21.626
But because of the difficulty of
having the machine learning and


00:05:21.626 --> 00:05:27.193
jointly go through every example,
the emphasis is on data processes.


00:05:27.193 --> 00:05:30.330
In terms of how you collect,
install the data,


00:05:30.330 --> 00:05:35.831
the labeling instructions you may write
for a large team of crowdsource labelers.


00:05:35.831 --> 00:05:40.607
And once you have executed some
data process, such as asked a large


00:05:40.607 --> 00:05:45.638
team of laborers to label a large set
of audio clips, it can also be much


00:05:45.638 --> 00:05:50.607
harder to go back and change your mind and
get everything relabeled.


00:05:50.607 --> 00:05:54.748
So let's summarize or
unstructured data problems.


00:05:54.748 --> 00:05:58.782
You may or may not have a huge
collection of unlabeled examples x.


00:05:58.782 --> 00:06:03.831
Maybe in your factory, you actually took
many thousands of images of smartphones,


00:06:03.831 --> 00:06:07.187
but you just haven't bothered
to label all of them yet.


00:06:07.187 --> 00:06:11.102
This is also common in
the self driving car industry,


00:06:11.102 --> 00:06:16.235
where many self driving car companies
have collected tons of images of


00:06:16.235 --> 00:06:21.560
cars driving around, but just have not yet
caught in that data labeled.


00:06:21.560 --> 00:06:26.467
For these structured data problems,
you can sometimes get more data by


00:06:26.467 --> 00:06:31.552
taking your unlabeled data x, and
asking humans to just label more of it.


00:06:31.552 --> 00:06:34.113
This doesn't apply to every problem,
but for


00:06:34.113 --> 00:06:38.590
the problems where you do have tons of
unlabeled data, this can be very helpful.


00:06:38.590 --> 00:06:43.201
And as we have already mentioned,
data augmentation can also be helpful.


00:06:43.201 --> 00:06:45.322
For structured data problems,


00:06:45.322 --> 00:06:50.190
is usually harder to obtain more data
because you only have so many users or


00:06:50.190 --> 00:06:54.056
only so many houses were so
that you can collect data from.


00:06:54.056 --> 00:06:59.307
And human labeling on average is
also harder, although there are some


00:06:59.307 --> 00:07:04.113
exceptions, such as in the Lost Video
where you saw that we could


00:07:04.113 --> 00:07:09.011
try to ask people to label examples for
the user ID merge problem.


00:07:09.011 --> 00:07:13.454
But in many cases where we ask
humans to label structure data,


00:07:13.454 --> 00:07:19.009
even when there's a completely worthwhile
to ask people to try to label if two


00:07:19.009 --> 00:07:25.178
records are the same person, there's more
likely to be a little bit more ambiguity.


00:07:25.178 --> 00:07:29.946
But even the human labor sometimes
finds it hard to be sure what is


00:07:29.946 --> 00:07:31.461
the correct label.


00:07:31.461 --> 00:07:36.634
Lastly, let's look at small
versus big data where I used to


00:07:36.634 --> 00:07:41.603
slightly arbitrary threshold
of whether you have more or


00:07:41.603 --> 00:07:46.792
less than say 10,000,
they put training examples.


00:07:46.792 --> 00:07:50.289
For small data sets,
clean labels are critical and


00:07:50.289 --> 00:07:52.647
the data set may be small enough for


00:07:52.647 --> 00:07:58.279
you to manually look through the entire
data set and fix any inconsistent labels.


00:07:58.279 --> 00:08:04.019
Further, the labeling team is probably
not that large, it maybe one or


00:08:04.019 --> 00:08:08.813
two or just a handful of people
that created all the labels.


00:08:08.813 --> 00:08:13.931
So if you discover an inconsistency
in the labels, say one person label


00:08:13.931 --> 00:08:19.319
Iguanas one way and the different
person labeled Iguanas a different way.


00:08:19.319 --> 00:08:22.184
You can just get the two or
three labels together and


00:08:22.184 --> 00:08:26.586
have them talk to each other and hash out
and agree on one labeling convention.


00:08:26.586 --> 00:08:32.723
For the very large data sets,
the emphasis has to be on data process.


00:08:32.723 --> 00:08:36.905
And if you have a 100 labelers or
even more, it's just harder to


00:08:36.905 --> 00:08:41.849
get 100 people into a room to all talk
to each other and hash out the process.


00:08:41.849 --> 00:08:46.831
And so you might have to rely on
a smaller team to establish a consistent


00:08:46.831 --> 00:08:51.983
label definition and then share that
definition with all, say 100 or


00:08:51.983 --> 00:08:56.386
more labelers and ask them to
all implement the same process.


00:08:56.386 --> 00:09:01.845
I want to leave you with one last thought,
which is that I found this categorization


00:09:01.845 --> 00:09:06.926
of problems into unstructured versus
structured, small versus big data.


00:09:06.926 --> 00:09:11.788
I found this to be helpful for
predicting not just whether


00:09:11.788 --> 00:09:16.954
data processes generalize from
one to another problem, but


00:09:16.954 --> 00:09:23.657
also whether other machine learning
idea is generalized from one to another.


00:09:23.657 --> 00:09:29.289
So one tip, if you are working on a
problem from one of these four quadrants,


00:09:29.289 --> 00:09:34.129
then on average advice from someone
that has worked on problems in


00:09:34.129 --> 00:09:38.881
the same quadrants will probably
be more useful than advice from


00:09:38.881 --> 00:09:42.582
someone that's worked in
a different quadrant.


00:09:42.582 --> 00:09:47.682
I found also in hiring machine learning
engineers, someone that's worked


00:09:47.682 --> 00:09:52.383
in the same quadrant as the problem
I'm trying to solve will usually be


00:09:52.383 --> 00:09:57.344
able to adapt more quickly to working
on other problems in that quadrant.


00:09:57.344 --> 00:10:02.252
Because the instincts and decisions
are more similar within one quadrant


00:10:02.252 --> 00:10:06.619
than if you shift to a totally
different quadrants in discharge.


00:10:06.619 --> 00:10:11.440
I've sometimes heard people give advice
like if you are building a computer


00:10:11.440 --> 00:10:15.290
vision system always get at
least 1000 labor examples.


00:10:15.290 --> 00:10:19.236
And I think people that give advice
like that are well meaning and


00:10:19.236 --> 00:10:22.891
I appreciate that they're
trying to give good advice, but


00:10:22.891 --> 00:10:26.636
I found that advice to not really
be useful for all problems.


00:10:26.636 --> 00:10:29.550
Machine learning is very diverse and


00:10:29.550 --> 00:10:33.787
it's hard to find one size
fits all advice like that.


00:10:33.787 --> 00:10:37.999
I've seen computer vision problems
built with 100 examples or


00:10:37.999 --> 00:10:43.277
100 examples for a class, screen systems
built with 100 million examples.


00:10:43.277 --> 00:10:48.884
And so if you are looking for advice on
a machine learning project, try to find


00:10:48.884 --> 00:10:54.944
someone that's worked in the same quadrant
as the problem you are trying to solve.


00:10:54.944 --> 00:10:59.363
Now we talked about one
formulation of different types of


00:10:59.363 --> 00:11:01.673
machine learning problems.


00:11:01.673 --> 00:11:06.203
There's one aspect I would like to
dive into with you in the next video,


00:11:06.203 --> 00:11:11.505
which is how for small data problems,
having clean data is especially important.


00:11:11.505 --> 00:11:15.061
Let's take a look at the next
video of why it is true.
1
00:00:00,640 --> 00:00:02,720
In problems of a small dataset.


2
00:00:02,720 --> 00:00:06,580
Having clean and
consistent labels is especially important.


3
00:00:06,580 --> 00:00:08,740
Let's start with an example.


4
00:00:08,740 --> 00:00:13,470
One of the things I used to do is use
machine learning to fly helicopters.


5
00:00:13,470 --> 00:00:19,488
One things you might want to do is take us
input the voltage apply to the motor or


6
00:00:19,488 --> 00:00:24,790
to the helicopter rotor and
predict what's the speed of the rotor.


7
00:00:24,790 --> 00:00:29,042
You can have this type of problem,
not just to find helicopters before


8
00:00:29,042 --> 00:00:33,030
other control problems with
controlling the speed of the motor.


9
00:00:33,030 --> 00:00:37,529
So let's say you have a data
set that looks like this where


10
00:00:37,529 --> 00:00:39,520
you have five examples.


11
00:00:39,520 --> 00:00:44,182
So a pretty small data
set because this data


12
00:00:44,182 --> 00:00:47,584
set that is the output Y is pretty noisy,


13
00:00:47,584 --> 00:00:49,000
It is difficult to know what is the


14
00:00:49,000 --> 00:00:51,000
the function you should use to


15
00:00:51,000 --> 00:00:55,716
map voltage to the rotor speed in rpm.


16
00:00:56,716 --> 00:01:02,516
Maybe it should be a straight line,
something like that.


17
00:01:02,516 --> 00:01:04,246
Or maybe something like that.


18
00:01:04,246 --> 00:01:08,376
Or maybe it should go up and
then be flat like that.


19
00:01:08,376 --> 00:01:10,276
Or maybe it should be a curve like that.


20
00:01:10,276 --> 00:01:13,866
Really hard to tell when
you have a small data set.


21
00:01:13,866 --> 00:01:15,757
five examples in noisy labels.


22
00:01:15,757 --> 00:01:19,616
It's difficult to fit
a function confidently.


23
00:01:19,616 --> 00:01:24,382
Now, if you had a ton of data, this
data set is equally noisy as the one on


24
00:01:24,382 --> 00:01:27,916
the left, but
you just have a lot more data.


25
00:01:27,916 --> 00:01:32,422
Then the learning algorithm can
average over the noisy data sets and


26
00:01:32,422 --> 00:01:34,336
you can now fill a function.


27
00:01:34,336 --> 00:01:38,116
You're pretty confidently looks like
curve should be something like that.


28
00:01:38,116 --> 00:01:43,070
A lot of AI had recently grown up in large
consumer Internet companies which may


29
00:01:43,070 --> 00:01:48,316
have 100 million users or billion
users and does very large data sets.


30
00:01:48,316 --> 00:01:51,556
And so, I think some of the practices for


31
00:01:51,556 --> 00:01:56,606
how to deal with small data sets
have not been emphasized as much


32
00:01:56,606 --> 00:02:01,276
as would be needed to tackle
problems where you don't have


33
00:02:01,276 --> 00:02:06,176
100 million examples, but
only 1000 or even fewer.


34
00:02:06,176 --> 00:02:10,986
So to me, the interesting case is what
if you still have a small data set?


35
00:02:10,986 --> 00:02:14,116
Five examples same as
the example on the left.


36
00:02:14,116 --> 00:02:17,546
But you now have clean and
consistent labels.


37
00:02:17,546 --> 00:02:22,227
In this case you can pretty
confidently fit a function


38
00:02:22,227 --> 00:02:26,396
through your data and
with only five examples.


39
00:02:26,396 --> 00:02:31,616
You can build a pretty good model for
predicting speed


40
00:02:31,616 --> 00:02:36,836
as a function of the input
voltage of trained computer


41
00:02:36,836 --> 00:02:42,989
vision systems with just 30 images and
had to work just fine.


42
00:02:42,989 --> 00:02:48,816
And the key is usually to make sure that
the labels are clean and consistent.


43
00:02:48,816 --> 00:02:53,520
Let's take a look at another
example of phone defect inspection,


44
00:02:53,520 --> 00:02:57,552
the tosses, the tickets,
input pictures like these and


45
00:02:57,552 --> 00:03:02,086
to decide whether there is a defect or
not on the phone.


46
00:03:02,086 --> 00:03:06,903
Now, if labeling instructions
are initially unclear,


47
00:03:06,903 --> 00:03:11,516
then labors will label
images inconsistently.


48
00:03:11,516 --> 00:03:14,848
It may be that when
there's a giant scratch,


49
00:03:14,848 --> 00:03:19,674
sufficiently large one that everyone
will agree as a defect, and


50
00:03:19,674 --> 00:03:24,430
if there's a tiny little thing
that inspectors will ignore it.


51
00:03:24,430 --> 00:03:29,844
But there's this region of ambiguity
where different inspectors will label


52
00:03:29,844 --> 00:03:36,816
different scratches with a length between
0.2 and 0.4 in slightly inconsistent ways.


53
00:03:36,816 --> 00:03:39,227
So one solution to this would be to say,


54
00:03:39,227 --> 00:03:43,405
why don't we try to get a lot more
pictures of phones and scratches.


55
00:03:43,405 --> 00:03:45,711
And then see what the inspectors do and


56
00:03:45,711 --> 00:03:48,967
then maybe eventually we
can train a neural network.


57
00:03:48,967 --> 00:03:53,689
They can figure out from the image what
is and what isn't a scratch on average.


58
00:03:53,689 --> 00:03:57,288
Maybe that approach could work,
but it'd be a lot of work and


59
00:03:57,288 --> 00:03:59,496
require collecting a lot of images.


60
00:03:59,496 --> 00:04:04,587
I found that it can be more fruitful
to ask the inspectors to sit down and


61
00:04:04,587 --> 00:04:08,834
just try to reach agreement on
what is the size of scratch.


62
00:04:08,834 --> 00:04:13,417
That would cause them to label
a scratcher of a bounding box versus


63
00:04:13,417 --> 00:04:17,276
decide is too small and
not worth bothering labeling.


64
00:04:17,276 --> 00:04:22,783
So in this example,
if the labelers can agree that the point


65
00:04:22,783 --> 00:04:27,973
of transition from where
little ding becomes a defect.


66
00:04:27,973 --> 00:04:29,208
Is a length of 0.3,


67
00:04:29,208 --> 00:04:32,792
then the way they label the images
becomes much more consistent.


68
00:04:32,792 --> 00:04:38,251
And it becomes much easier for
learning algorithm to take as input images


69
00:04:38,251 --> 00:04:45,016
like this and consistently decide whether
something is a scratch of the effect..


70
00:04:45,016 --> 00:04:46,256
Just to be clear.


71
00:04:46,256 --> 00:04:52,140
In this example, the input to the learning
algorithm is images like that on the left,


72
00:04:52,140 --> 00:04:55,516
not the stretched length
like that on the right.


73
00:04:55,516 --> 00:05:00,178
But the point is, if you can get
inspectors to agree what is a scratch and


74
00:05:00,178 --> 00:05:01,756
what is in the scratch.


75
00:05:01,756 --> 00:05:06,334
And to define The task as
putting bounding boxes around


76
00:05:06,334 --> 00:05:09,527
defects are over 0.3 mm in length.


77
00:05:10,616 --> 00:05:14,108
Then that will cause your images to
be labeled more consistently and


78
00:05:14,108 --> 00:05:16,997
allow your learning album
to achieve higher accuracy.


79
00:05:16,997 --> 00:05:19,516
Even when your data set isn't that big.


80
00:05:19,516 --> 00:05:22,952
So you see the couple
examples now of how label


81
00:05:22,952 --> 00:05:26,226
consistency helps a learning algorithm.


82
00:05:26,226 --> 00:05:30,595
I want to wrap up this video
with one more thought,


83
00:05:30,595 --> 00:05:36,541
which is that big data problems can
have small data challenges too.


84
00:05:36,541 --> 00:05:41,765
Specifically problems of the large
data set, but where there's a long


85
00:05:41,765 --> 00:05:47,416
tail of rare events in the input
will have small data challenges too.


86
00:05:47,416 --> 00:05:52,310
For example, the large web search engine
companies all have very large data


87
00:05:52,310 --> 00:05:57,616
sets of web search queries, but
many web queries actually very rare.


88
00:05:57,616 --> 00:06:00,395
And so the amount of click stream data for


89
00:06:00,395 --> 00:06:04,831
the rare queries is actually small or
take self-driving cars.


90
00:06:04,831 --> 00:06:09,194
Self-driving car companies tend
to have very large data sets,


91
00:06:09,194 --> 00:06:14,385
collected from driving hundreds of
thousands or millions of hours or more.


92
00:06:14,385 --> 00:06:18,761
But there are rare occurrences that
are critical to get right to make sure


93
00:06:18,761 --> 00:06:20,566
a self-driving car is safe.


94
00:06:20,566 --> 00:06:26,451
Such as that very rare occurrence of
a young child running across the highway,


95
00:06:26,451 --> 00:06:31,466
or that very rare occurrence of
a truck parked across the highway.


96
00:06:31,466 --> 00:06:34,870
So even if a self driving car
has a very large data set,


97
00:06:34,870 --> 00:06:40,516
the number of examples that may have of
these rare events is actually very small.


98
00:06:40,516 --> 00:06:45,383
And so ensuring label consistency
in terms of how these rare


99
00:06:45,383 --> 00:06:50,056
events are detective and
labels is still very helpful for


100
00:06:50,056 --> 00:06:56,016
improving self-driving cars or
product recommended systems.


101
00:06:56,016 --> 00:07:00,371
If you have a catalog of hundreds
of thousands, or millions or


102
00:07:00,371 --> 00:07:03,821
more items or
product recommendation systems.


103
00:07:03,821 --> 00:07:09,026
If you have an online catalog of anywhere
from thousands to hundreds of thousands


104
00:07:09,026 --> 00:07:14,017
to sometimes even millions of catalogs
to sometimes even millions of items.


105
00:07:14,017 --> 00:07:18,984
Then you will have a lot of
products where the number sold


106
00:07:18,984 --> 00:07:22,116
of that item is quite small.


107
00:07:22,116 --> 00:07:27,003
And so the amount of data you
have of users interacting with


108
00:07:27,003 --> 00:07:30,816
the items in the long
tail is actually small.


109
00:07:30,816 --> 00:07:35,407
And if there's a way which is not easy,
but there's a way to make sure that data


110
00:07:35,407 --> 00:07:39,806
is clean and consistent, then that
too will help you learning algorithm.


111
00:07:39,806 --> 00:07:43,241
In terms of how it recommends or
doesn't recommend items


112
00:07:43,241 --> 00:07:48,016
in the long tail where the amount of
data per item will tend to be low.


113
00:07:48,016 --> 00:07:52,476
So when you have a small dataset
label consistency is critical.


114
00:07:52,476 --> 00:07:57,416
Even when you have a big data set,
label consistency can be very important.


115
00:07:57,416 --> 00:07:59,587
It's just that found it easier,


116
00:07:59,587 --> 00:08:05,716
on average to get to label consistency on
smaller data sets than on very large ones.


117
00:08:05,716 --> 00:08:10,452
In the next video, we'll look at some
concrete ideas and best practices for


118
00:08:10,452 --> 00:08:13,506
improving your data,
says label consistency.


119
00:08:13,506 --> 00:08:15,337
Let's go on to the next video.
1
00:00:00,000 --> 00:00:02,400
Let's take a look at
some ways to improve


2
00:00:02,400 --> 00:00:04,730
the consistency of your labels.


3
00:00:04,730 --> 00:00:08,020
Here's a general
process you can use.


4
00:00:08,020 --> 00:00:12,395
If you are worried about
labels being inconsistent,


5
00:00:12,395 --> 00:00:14,420
find a few examples and have


6
00:00:14,420 --> 00:00:17,495
multiple labelers label
the same example.


7
00:00:17,495 --> 00:00:20,150
In some cases you can also have


8
00:00:20,150 --> 00:00:22,955
the same labeler
label an example,


9
00:00:22,955 --> 00:00:25,310
wait a while until
they have hopefully


10
00:00:25,310 --> 00:00:28,610
forgotten or technical
term is wash out,


11
00:00:28,610 --> 00:00:31,640
but have them take a break
and then come back and


12
00:00:31,640 --> 00:00:33,110
re-label it and see if they're


13
00:00:33,110 --> 00:00:35,370
even consistent with themselves.


14
00:00:35,370 --> 00:00:38,305
When you find that
there's disagreements,


15
00:00:38,305 --> 00:00:42,030
have the people
responsible for labeling,


16
00:00:42,030 --> 00:00:44,015
this could be the
machine label engineer,


17
00:00:44,015 --> 00:00:46,345
it could be the
subject matter expert,


18
00:00:46,345 --> 00:00:49,210
such as the manufacturing
expert that is responsible


19
00:00:49,210 --> 00:00:52,120
for labeling what is a stretch
and what isn't a stretch,


20
00:00:52,120 --> 00:00:54,940
and/or the dedicated labelers,


21
00:00:54,940 --> 00:00:57,670
discuss together what
they think should


22
00:00:57,670 --> 00:01:01,270
be a more consistent
definition of a label y,


23
00:01:01,270 --> 00:01:05,305
and try to have them
reach an agreement.


24
00:01:05,305 --> 00:01:10,495
Ideally, also document and
write down that agreement,


25
00:01:10,495 --> 00:01:13,375
and this definition of y


26
00:01:13,375 --> 00:01:16,790
can then become an updated set of


27
00:01:16,790 --> 00:01:19,625
labeling instructions
that they can go back to


28
00:01:19,625 --> 00:01:24,220
label new data or to
relabel old data.


29
00:01:24,220 --> 00:01:28,520
During this discussion, in
some cases the labelers will


30
00:01:28,520 --> 00:01:30,530
come back and say
they don't think


31
00:01:30,530 --> 00:01:32,885
the input x has
enough information.


32
00:01:32,885 --> 00:01:34,295
If that's the case,


33
00:01:34,295 --> 00:01:37,510
consider changing the input x.


34
00:01:37,510 --> 00:01:41,480
For example, when we saw
the pictures of phones,


35
00:01:41,480 --> 00:01:42,970
they were was so dark that we


36
00:01:42,970 --> 00:01:44,955
couldn't even tell
what was going on,


37
00:01:44,955 --> 00:01:47,215
that was a sign that we should


38
00:01:47,215 --> 00:01:49,740
consider increasing
the illumination,


39
00:01:49,740 --> 00:01:52,305
the lighting with which
the pictures were taken.


40
00:01:52,305 --> 00:01:55,269
But of course, I know this
isn't always possible,


41
00:01:55,269 --> 00:01:57,115
but sometimes this
can be a big help.


42
00:01:57,115 --> 00:02:00,310
Then all this is an
iterative process.


43
00:02:00,310 --> 00:02:02,260
So after improve x or


44
00:02:02,260 --> 00:02:04,685
after improving the
label instructions,


45
00:02:04,685 --> 00:02:07,965
you will ask the team
to label more data.


46
00:02:07,965 --> 00:02:11,350
If you think there are
still disagreements,


47
00:02:11,350 --> 00:02:13,990
then repeat the whole process of


48
00:02:13,990 --> 00:02:17,035
having multiple labelers
label the same example,


49
00:02:17,035 --> 00:02:19,505
major disagreement and so on.


50
00:02:19,505 --> 00:02:21,290
Let's look at some examples.


51
00:02:21,290 --> 00:02:24,520
One common outcome of
this type of exercise


52
00:02:24,520 --> 00:02:28,405
is to standardize the
definition of labels.


53
00:02:28,405 --> 00:02:31,060
Between these ways of labeling


54
00:02:31,060 --> 00:02:33,955
the audio clip you heard
on the earlier video,


55
00:02:33,955 --> 00:02:36,190
perhaps the labelers
will standardize


56
00:02:36,190 --> 00:02:39,190
on this as the convention,


57
00:02:39,190 --> 00:02:41,115
or maybe they'll
pick a different one


58
00:02:41,115 --> 00:02:42,700
and that could be okay too.


59
00:02:42,700 --> 00:02:45,945
But at least this makes
the data more consistent.


60
00:02:45,945 --> 00:02:49,300
Another common decision
that I've seen come out


61
00:02:49,300 --> 00:02:52,855
of a process like this
is merging classes.


62
00:02:52,855 --> 00:02:56,560
If in your labeling
guidelines you asked labelers


63
00:02:56,560 --> 00:02:59,890
to label deep scratches on
the surface of the phone,


64
00:02:59,890 --> 00:03:03,035
as well as shallow scratches
on the surface of the phone,


65
00:03:03,035 --> 00:03:05,710
but if the definition
between what constitutes


66
00:03:05,710 --> 00:03:11,189
a deep scratch versus
a shallow scratch,


67
00:03:11,189 --> 00:03:14,565
barely visible here
I know, is unclear,


68
00:03:14,565 --> 00:03:17,940
then you end up with
labelers very inconsistently


69
00:03:17,940 --> 00:03:21,930
labeling things as deep
versus shallow scratches.


70
00:03:21,930 --> 00:03:24,270
Sometimes the factory
does really need to


71
00:03:24,270 --> 00:03:28,200
distinguish between deep
versus shallow scratches.


72
00:03:28,200 --> 00:03:30,420
Sometimes factories
need to do this to


73
00:03:30,420 --> 00:03:32,875
figure out what was the
cause of the defect.


74
00:03:32,875 --> 00:03:36,270
But sometimes I found that you


75
00:03:36,270 --> 00:03:37,470
don't really need to


76
00:03:37,470 --> 00:03:39,780
distinguish between
these two classes,


77
00:03:39,780 --> 00:03:41,940
and you can instead merge


78
00:03:41,940 --> 00:03:45,885
the two classes into
a single class, say,


79
00:03:45,885 --> 00:03:49,260
the scratch class, and
this gets rid of all of


80
00:03:49,260 --> 00:03:51,840
the inconsistencies
with different


81
00:03:51,840 --> 00:03:55,095
labelers labeling the same
thing deep versus shallow.


82
00:03:55,095 --> 00:03:58,080
Merging classes isn't
always applicable,


83
00:03:58,080 --> 00:03:59,160
but when it is,


84
00:03:59,160 --> 00:04:01,980
it simplifies the task for
the learning algorithm.


85
00:04:01,980 --> 00:04:05,100
One of the technique I've used
is to create a new class,


86
00:04:05,100 --> 00:04:09,840
or create a new label
to capture uncertainty.


87
00:04:09,840 --> 00:04:14,330
For example, let's say you
asked labelers to label


88
00:04:14,330 --> 00:04:16,085
phones as defective or not


89
00:04:16,085 --> 00:04:18,710
based on the length
of the scratch.


90
00:04:18,710 --> 00:04:20,810
Here's a sequence of smartphones


91
00:04:20,810 --> 00:04:23,155
with larger and larger scratches.


92
00:04:23,155 --> 00:04:25,220
Not sure if you can see
these on your display,


93
00:04:25,220 --> 00:04:28,120
but let me just make them a
little bit more visible here.


94
00:04:28,120 --> 00:04:29,960
I know that all of these are


95
00:04:29,960 --> 00:04:31,280
really large scratches if


96
00:04:31,280 --> 00:04:32,800
this is a real phone
you're buying.


97
00:04:32,800 --> 00:04:35,035
This is just for
illustrative purposes.


98
00:04:35,035 --> 00:04:38,825
Maybe everyone agrees that the
giant scratch is a defect,


99
00:04:38,825 --> 00:04:41,315
a tiny scratch is not a defect,


100
00:04:41,315 --> 00:04:43,830
but they don't agree
on what's in between.


101
00:04:43,830 --> 00:04:46,880
If it was possible to
get them to agree,


102
00:04:46,880 --> 00:04:51,525
then that would be one way
to reduce label ambiguity.


103
00:04:51,525 --> 00:04:53,600
But if that turns
out to be difficult,


104
00:04:53,600 --> 00:04:56,225
then here's another option;


105
00:04:56,225 --> 00:04:58,130
which is to create


106
00:04:58,130 --> 00:05:01,400
a new class where you
now have three labels.


107
00:05:01,400 --> 00:05:03,925
You can say, it's
clearly not a defect,


108
00:05:03,925 --> 00:05:05,395
or clearly a defect,


109
00:05:05,395 --> 00:05:08,210
or just acknowledge
there's some examples are


110
00:05:08,210 --> 00:05:12,205
ambiguous and put them in
a new borderline class.


111
00:05:12,205 --> 00:05:14,900
If it becomes easier
to come up with


112
00:05:14,900 --> 00:05:17,900
consistent instructions for
this three class problem,


113
00:05:17,900 --> 00:05:20,990
because maybe some examples
are genuinely borderline,


114
00:05:20,990 --> 00:05:26,060
then that could potentially
improve labeling consistency.


115
00:05:26,060 --> 00:05:28,220
Let me use speech illustration


116
00:05:28,220 --> 00:05:31,320
to illustrate this further.


117
00:05:31,790 --> 00:05:34,260
Given this audio clip,


118
00:05:34,260 --> 00:05:39,630
[inaudible] I really can't
tell what they said.


119
00:05:39,630 --> 00:05:43,480
[inaudible] If you were


120
00:05:43,480 --> 00:05:46,060
to force everyone
to transcribe it,


121
00:05:46,060 --> 00:05:49,110
some labelers would
transcribe, "Nearly go."


122
00:05:49,110 --> 00:05:50,550
Some maybe they'll say,


123
00:05:50,550 --> 00:05:54,460
"Nearest grocery," and it's
very difficult to get to


124
00:05:54,460 --> 00:05:56,200
consistency because


125
00:05:56,200 --> 00:05:59,840
the audio clip is
genuinely ambiguous.


126
00:05:59,840 --> 00:06:02,250
To improve labeling consistency,


127
00:06:02,250 --> 00:06:04,840
it may be better to
create a new tag,


128
00:06:04,840 --> 00:06:06,580
the unintelligible tag, and


129
00:06:06,580 --> 00:06:08,545
just ask everyone to label this


130
00:06:08,545 --> 00:06:13,370
as nearest [inaudible]
unintelligible.


131
00:06:14,090 --> 00:06:16,300
This can result in


132
00:06:16,300 --> 00:06:19,480
more consistent labels than
if we were to ask everyone to


133
00:06:19,480 --> 00:06:24,435
guess what they heard when
it really is unintelligible.


134
00:06:24,435 --> 00:06:28,660
Let me wrap up with some
suggestions for working with


135
00:06:28,660 --> 00:06:33,780
small versus big datasets to
improve label consistency.


136
00:06:33,780 --> 00:06:36,610
We've just been talking
about unstructured data or


137
00:06:36,610 --> 00:06:39,880
problems where we can count
on people to label the data.


138
00:06:39,880 --> 00:06:41,380
For small datasets there's


139
00:06:41,380 --> 00:06:43,690
usually a small
number of labelers.


140
00:06:43,690 --> 00:06:46,105
So when you find
an inconsistency,


141
00:06:46,105 --> 00:06:48,400
you can ask the labelers
to sit down and


142
00:06:48,400 --> 00:06:51,335
discuss a specific image
or a specific audio clip,


143
00:06:51,335 --> 00:06:54,240
and try to drive to an agreement.


144
00:06:54,240 --> 00:06:57,550
For big datasets,
it would be more


145
00:06:57,550 --> 00:06:59,260
common to try to get to


146
00:06:59,260 --> 00:07:01,960
consistent definition
with a small group,


147
00:07:01,960 --> 00:07:04,510
and then send the
labeling instructions


148
00:07:04,510 --> 00:07:06,580
to a larger group of labelers.


149
00:07:06,580 --> 00:07:09,800
One other technique
that is commonly used,


150
00:07:09,800 --> 00:07:12,220
but I think overused
in my opinion,


151
00:07:12,220 --> 00:07:13,720
is that you can have


152
00:07:13,720 --> 00:07:17,170
multiple labelers
label every example


153
00:07:17,170 --> 00:07:18,700
and then let them vote.


154
00:07:18,700 --> 00:07:22,570
Voting is sometimes called
consensus labeling,


155
00:07:22,570 --> 00:07:24,840
in order to increase accuracy.


156
00:07:24,840 --> 00:07:28,355
I find that this type of
voting mechanism technique,


157
00:07:28,355 --> 00:07:30,100
it can work, but it's


158
00:07:30,100 --> 00:07:33,610
probably over used in
machine learning today.


159
00:07:33,610 --> 00:07:36,520
Where what I've
seen a lot of teams


160
00:07:36,520 --> 00:07:39,880
do is have inconsistent
labeling instructions,


161
00:07:39,880 --> 00:07:42,805
and then try to have a lot
of labelers and then voting,


162
00:07:42,805 --> 00:07:44,920
to try to make it
more consistent.


163
00:07:44,920 --> 00:07:47,080
But before resorting to this,


164
00:07:47,080 --> 00:07:48,310
which I do use,


165
00:07:48,310 --> 00:07:49,945
but more of a last resort,


166
00:07:49,945 --> 00:07:51,970
I would use the first,


167
00:07:51,970 --> 00:07:56,530
try to get to more consistent
label definitions,


168
00:07:56,530 --> 00:07:59,620
to try to make the
individual labelers choices


169
00:07:59,620 --> 00:08:01,780
less noisy in the first place,


170
00:08:01,780 --> 00:08:04,180
rather than take a lot
of noisy data and then


171
00:08:04,180 --> 00:08:06,955
try to use voting to
reduce the noise.


172
00:08:06,955 --> 00:08:09,730
I hope that the tools you
just learnt for improving


173
00:08:09,730 --> 00:08:12,010
label consistency
will help you to get


174
00:08:12,010 --> 00:08:14,860
better data for your
machine learning task.


175
00:08:14,860 --> 00:08:17,950
One of the gaps I see in
the machine learning world


176
00:08:17,950 --> 00:08:20,930
today is that there's
still a lack of tools,


177
00:08:20,930 --> 00:08:22,750
and there are also
machine learning ops


178
00:08:22,750 --> 00:08:24,940
tools for helping teams


179
00:08:24,940 --> 00:08:26,230
to carry out this type of


180
00:08:26,230 --> 00:08:29,130
process more consistently
and repeatedly.


181
00:08:29,130 --> 00:08:30,790
It's not us trying to


182
00:08:30,790 --> 00:08:33,160
figure this out in
the Jupyter Notebook,


183
00:08:33,160 --> 00:08:35,050
but instead to have tools help


184
00:08:35,050 --> 00:08:37,460
us to detect when labels are


185
00:08:37,460 --> 00:08:39,800
inconsistent and
to help facilitate


186
00:08:39,800 --> 00:08:43,805
the process in improving
the quality of the data.


187
00:08:43,805 --> 00:08:46,360
This is something I look
forward to hopefully


188
00:08:46,360 --> 00:08:50,075
our community working
on and developing.


189
00:08:50,075 --> 00:08:53,070
In terms of improving
label quality,


190
00:08:53,070 --> 00:08:55,910
one of the questions
that often comes up is:


191
00:08:55,910 --> 00:08:59,040
What is human level
performance on the task?


192
00:08:59,040 --> 00:09:00,930
I find human level
performance to be


193
00:09:00,930 --> 00:09:03,755
important and sometimes
misused concept.


194
00:09:03,755 --> 00:09:07,450
Let's take a deeper look
at this in the next video.
1
00:00:00,440 --> 00:00:06,028
Some machine learning tasks are trying to
predict an inherently ambiguous output and


2
00:00:06,028 --> 00:00:10,918
Human Level Performance can establish
a useful baseline of performance as


3
00:00:10,918 --> 00:00:12,240
a reference.


4
00:00:12,240 --> 00:00:16,170
But Human Level Performance
is also sometimes misuse.


5
00:00:16,170 --> 00:00:17,430
Let's take a look.


6
00:00:17,430 --> 00:00:22,305
One of the most important users of
measuring Human Level Performance or


7
00:00:22,305 --> 00:00:25,980
HLP is to estimate based error or
irreducible error.


8
00:00:25,980 --> 00:00:31,432
Especially on unstructured data tasks in
order to help with their analysis and


9
00:00:31,432 --> 00:00:36,040
prioritization and
just establish what might be possible.


10
00:00:36,040 --> 00:00:38,140
Take a visual inspection tasks.


11
00:00:38,140 --> 00:00:40,800
This may have happened to you before, but


12
00:00:40,800 --> 00:00:45,056
I have gotten requests from
business owners saying, hey Andrew,


13
00:00:45,056 --> 00:00:50,540
can you please build a system that's
99% accurate or maybe 99.9% accurate.


14
00:00:50,540 --> 00:00:55,329
So one way to establish what might be
possible would be to take a data set and


15
00:00:55,329 --> 00:00:57,840
look at the Ground Truth Data.


16
00:00:57,840 --> 00:01:02,630
Say you have six examples where
the Ground Truth Label is these, and


17
00:01:02,630 --> 00:01:07,085
then to answer human inspector to
label the same data blinded to


18
00:01:07,085 --> 00:01:11,570
the Ground Truth Label of course and
see what they come up with.


19
00:01:11,570 --> 00:01:16,355
And if they come up with these you
would say this inspector agreed to


20
00:01:16,355 --> 00:01:21,760
the ground truth on four other six
examples and disagreed on two out of six.


21
00:01:21,760 --> 00:01:26,551
And so Human Level Performance is 66.7%.


22
00:01:28,040 --> 00:01:32,480
And so this would let you go back
to the business owner and say look,


23
00:01:32,480 --> 00:01:35,910
even your inspector is
only 66.7% accuracy.


24
00:01:35,910 --> 00:01:39,230
How can you expect me to Get 99% accuracy?


25
00:01:39,230 --> 00:01:40,786
So HLP is useful for


26
00:01:40,786 --> 00:01:46,340
establishing a baseline in terms
of what might be possible.


27
00:01:46,340 --> 00:01:50,371
There's one question
that is often not asked,


28
00:01:50,371 --> 00:01:54,620
which is what exactly is
this Ground Truth Label?


29
00:01:54,620 --> 00:01:59,471
Because rather than just
measuring how well we can


30
00:01:59,471 --> 00:02:03,400
do compared to some Ground Truth Label,


31
00:02:03,400 --> 00:02:08,840
which was probably written
by some other human.


32
00:02:08,840 --> 00:02:13,321
Are we really measuring what is
possible or are we just measuring how


33
00:02:13,321 --> 00:02:17,840
well two different people happen
to agree with each other?


34
00:02:17,840 --> 00:02:24,140
When the Ground Truth Label is
itself determined by a person.


35
00:02:24,140 --> 00:02:30,163
There's a very different approach to
thinking about Human Level Performance


36
00:02:30,163 --> 00:02:34,282
which I want to share of you in this and
the next video.


37
00:02:34,282 --> 00:02:39,266
Beyond this purpose of estimating Bayes
error and establishing what's possible


38
00:02:39,266 --> 00:02:43,340
using that to help with their analysis and
prioritization.


39
00:02:43,340 --> 00:02:47,271
Here are some other users
of Human Level Performance.


40
00:02:47,271 --> 00:02:52,240
In academia, HLP is often used
as a respectable benchmark.


41
00:02:52,240 --> 00:02:56,776
And so when you establish that
people are only 92% accurate or


42
00:02:56,776 --> 00:03:00,490
some of the number on a speech
recognition data set.


43
00:03:00,490 --> 00:03:03,721
And if you can beat
human level performance,


44
00:03:03,721 --> 00:03:09,332
then that establishes then that helps
you to quote proof that you're learning


45
00:03:09,332 --> 00:03:14,370
algorithm is doing something hard and
helps get the paper published.


46
00:03:14,370 --> 00:03:18,158
I'm not saying this is a great use of HLP,
but


47
00:03:18,158 --> 00:03:21,949
in academia showing you
can beat HLP maybe for


48
00:03:21,949 --> 00:03:26,138
the first time has been a tried and
true formula for


49
00:03:26,138 --> 00:03:31,328
establishing the academic
significance of a piece of work and


50
00:03:31,328 --> 00:03:35,740
helps with getting something published.


51
00:03:35,740 --> 00:03:40,649
We discussed briefly on the last slide
what to do if a business of product owner


52
00:03:40,649 --> 00:03:44,802
asked for 99% accuracy and
if you think that's unrealistic,


53
00:03:44,802 --> 00:03:49,740
then measuring HLP may help you to
establish a more reasonable target.


54
00:03:49,740 --> 00:03:53,240
That's one of the use of HLP
that you might hear about.


55
00:03:53,240 --> 00:03:57,669
Do not be cautious about which is,
I've seen many projects


56
00:03:57,669 --> 00:04:02,560
with the machine learning team,
wants to use HLP or beating HLP.


57
00:04:02,560 --> 00:04:07,540
To prove that the Machine Learning System
is superior to the human is doing the job.


58
00:04:07,540 --> 00:04:11,903
And as tempting as it is to go to
someone and says look, I've proved that


59
00:04:11,903 --> 00:04:16,416
my machinery system is more accurate
than humans inspecting the phones or


60
00:04:16,416 --> 00:04:19,430
the radiologist reading X-rays or
something.


61
00:04:19,430 --> 00:04:23,757
And now that I've mathematically proved
the superiority of my learning album,


62
00:04:23,757 --> 00:04:25,110
you have to use it right?


63
00:04:25,110 --> 00:04:28,583
I know the logic of that is tempting, but


64
00:04:28,583 --> 00:04:33,200
as a practical matter,
this approach rarely works.


65
00:04:33,200 --> 00:04:37,171
And you also saw last week
that businesses need systems


66
00:04:37,171 --> 00:04:42,140
that do more than just doing well
on average test set accuracy.


67
00:04:42,140 --> 00:04:45,847
So if you ever find
yourself in this situation,


68
00:04:45,847 --> 00:04:50,480
I would urge you to just use this
type of logic with caution or


69
00:04:50,480 --> 00:04:55,320
maybe even more preferably just
don't use these arguments.


70
00:04:55,320 --> 00:04:59,547
I've usually found other arguments
than this to be more effective that


71
00:04:59,547 --> 00:05:05,040
working with the business to see if they
should adopt a Machine Learning System.


72
00:05:05,040 --> 00:05:09,846
The problem with beating
Human Level Performance as proof of


73
00:05:09,846 --> 00:05:13,531
machine learning
superiority is multi fold.


74
00:05:13,531 --> 00:05:18,330
Beyond the fact that most
applications require more than just


75
00:05:18,330 --> 00:05:23,037
high average tested accuracy,
one of the problems with this


76
00:05:23,037 --> 00:05:28,021
metric is that it sometimes gives
a learning algorithm an unfair


77
00:05:28,021 --> 00:05:33,040
advantage when labeling
instructions are inconsistent.


78
00:05:33,040 --> 00:05:34,710
Let me show you what I mean.


79
00:05:34,710 --> 00:05:39,541
If you have inconsistent
labeling instructions so


80
00:05:39,541 --> 00:05:44,258
that when an audio clip
says nearest gas station,


81
00:05:44,258 --> 00:05:49,762
let's say 70% of labelers,
uses label convention and


82
00:05:49,762 --> 00:05:54,510
30 percent of labelers
uses label convention.


83
00:05:54,510 --> 00:05:59,070
Neither one is the superiors transcript
to the other both seemed completely fine.


84
00:05:59,070 --> 00:06:04,114
But just by luck of the draw,
70% of labelers choose the first one,


85
00:06:04,114 --> 00:06:06,360
30% choose the second one.


86
00:06:06,360 --> 00:06:10,038
So if the ground truth is
established by a labelers,


87
00:06:10,038 --> 00:06:16,240
maybe just a laborer with a slightly
bigger title, but really by one labelers.


88
00:06:16,240 --> 00:06:21,910
Then the chance that two
random labeler will agree


89
00:06:21,910 --> 00:06:29,350
will be 0.7 squared plus 0.3 squares,
which is 0.58.


90
00:06:29,350 --> 00:06:32,678
So if you had two labelers
use the first convention,


91
00:06:32,678 --> 00:06:35,340
there's a 0.7 square chance of that.


92
00:06:35,340 --> 00:06:39,057
Or if both of your random labelers
use the second convention,


93
00:06:39,057 --> 00:06:41,940
there's a 0.3 square chance of that.


94
00:06:41,940 --> 00:06:44,040
Then the two of them will agree.


95
00:06:44,040 --> 00:06:47,320
So the chances to labelers agreeing 0.58.


96
00:06:47,320 --> 00:06:51,009
And in the usual way of measuring
Human Level Performance,


97
00:06:51,009 --> 00:06:55,340
you will conclude that
Human Level Performance is 0.58.


98
00:06:55,340 --> 00:06:59,752
But what you're really measuring is the
chance of two random labelers agreeing.


99
00:06:59,752 --> 00:07:04,640
This is where the machine learning
our room has an unfair advantage.


100
00:07:04,640 --> 00:07:08,250
I think either of these labeling
conventions is completely fine.


101
00:07:08,250 --> 00:07:13,849
But the learning algorithm is a little
bit better at gathering statistics


102
00:07:13,849 --> 00:07:19,808
of how often ellipses versus commas are
used in such a context than the learning


103
00:07:19,808 --> 00:07:24,970
algorithm may be able to always
use the first labeling convention.


104
00:07:24,970 --> 00:07:27,712
Because it knows that statistically,


105
00:07:27,712 --> 00:07:32,953
it has a 70% chance of getting it right
if it uses ellipses or dot dot dot.


106
00:07:32,953 --> 00:07:38,666
So a learning algorithm will agree
with humans 70% of the time,


107
00:07:38,666 --> 00:07:43,340
just by choosing the first
lebeling convention.


108
00:07:43,340 --> 00:07:47,330
But this 12% improvement in performance,


109
00:07:47,330 --> 00:07:51,321
whereas Human Level Performance is 58%


110
00:07:51,321 --> 00:07:56,142
and your learning algorithm
is 12% better is 0.70.


111
00:07:56,142 --> 00:07:59,960
This 12 better performance is
not actually important for


112
00:07:59,960 --> 00:08:02,000
anything between these two equally


113
00:08:02,000 --> 00:08:04,890
good, slightly arbitrary choices.


114
00:08:04,890 --> 00:08:09,667
The learning algorithm just
consistently picks the first one so


115
00:08:09,667 --> 00:08:14,798
it gains what seems like a 12%
advantage on this type of query, but


116
00:08:14,798 --> 00:08:21,020
it's not actually outperforming any human
in any way that a user would care about.


117
00:08:21,020 --> 00:08:24,041
And one side effect of this is that,


118
00:08:24,041 --> 00:08:29,600
if you're speech recognition tool
has multiple types of audio.


119
00:08:29,600 --> 00:08:34,531
For some, there's this dot dot dot or
ellipses versus common ambiguity and


120
00:08:34,531 --> 00:08:37,391
learning album does 12% better on this.


121
00:08:41,040 --> 00:08:46,845
If you're learning algorithm makes some
more significant errors on other types


122
00:08:46,845 --> 00:08:52,562
of input audio, then when its performance
where it actually does worse could be


123
00:08:52,562 --> 00:08:58,565
averaged out by queries like these where
kind of fake looks like it's doing better.


124
00:08:58,565 --> 00:09:04,044
And this will therefore mask or hide
the fact that you're learning algorithm


125
00:09:04,044 --> 00:09:09,340
is actually creating worse
transcripts than humans actually are.


126
00:09:09,340 --> 00:09:14,047
And what this means is that
a machine learning system can look


127
00:09:14,047 --> 00:09:16,733
like it's doing better than HLP.


128
00:09:16,733 --> 00:09:21,931
But actually be producing worse
transcripts than people because it's just


129
00:09:21,931 --> 00:09:26,882
doing better on this type of problem
which is not important to do better on


130
00:09:26,882 --> 00:09:32,101
while potentially actually doing worse
on some other types of input audio.


131
00:09:32,101 --> 00:09:37,063
Given these problems with Human Level
Performance, what are we supposed to do?


132
00:09:37,063 --> 00:09:40,000
Measuring Human Level Performance
is useful for


133
00:09:40,000 --> 00:09:45,340
establishing a baseline using that to
drive error analysis and prioritization.


134
00:09:45,340 --> 00:09:47,836
But using it to benchmark machines and


135
00:09:47,836 --> 00:09:52,240
humans sometimes runs into
problematic cases like this.


136
00:09:52,240 --> 00:09:55,362
I found that when my goal is
to build a useful application,


137
00:09:55,362 --> 00:09:57,654
not publish a paper, you publish a paper,


138
00:09:57,654 --> 00:10:01,307
let's prove we can outperform
people that helps published paper.


139
00:10:01,307 --> 00:10:06,297
But found that when my goal is to build
a useful application rather than trying to


140
00:10:06,297 --> 00:10:08,308
beat Human Level Performance,


141
00:10:08,308 --> 00:10:12,924
I found it's often useful to instead
try to raise Human Level Performance


142
00:10:12,924 --> 00:10:17,766
because we raise Human Level Performance
by improving label consistency and


143
00:10:17,766 --> 00:10:23,040
that ultimately results in better
learning outcomes performance as well.


144
00:10:23,040 --> 00:10:25,560
Let's take a deeper look
at this in the next video
1
00:00:00,000 --> 00:00:03,070
I think the use of HLP in


2
00:00:03,070 --> 00:00:05,260
machine learning has
taken off partly


3
00:00:05,260 --> 00:00:06,940
because it helped people get


4
00:00:06,940 --> 00:00:10,285
papers published to
show they can beat HLP.


5
00:00:10,285 --> 00:00:13,480
There's also been a bit
misused in settings where


6
00:00:13,480 --> 00:00:16,810
the goal was to build a
valuable application,


7
00:00:16,810 --> 00:00:19,240
not just to publish a paper.


8
00:00:19,240 --> 00:00:23,035
When the ground truth
is externally defined,


9
00:00:23,035 --> 00:00:25,900
then there are
fewer problems with


10
00:00:25,900 --> 00:00:30,040
HLP when the drought really
is some real drought-proof.


11
00:00:30,040 --> 00:00:34,510
For example, I've done a lot
of work on medical imaging,


12
00:00:34,510 --> 00:00:37,090
working on your AI for


13
00:00:37,090 --> 00:00:40,075
diagnosing from X-rays
or things like these.


14
00:00:40,075 --> 00:00:42,220
Given an X-ray image,


15
00:00:42,220 --> 00:00:45,010
if you want to
predict a diagnosis,


16
00:00:45,010 --> 00:00:50,080
if the diagnosis is defined
according to, say, a biopsy,


17
00:00:50,080 --> 00:00:52,915
so your biological
or medical tests,


18
00:00:52,915 --> 00:00:57,370
then HLP helps you measure
how well does a doctor versus


19
00:00:57,370 --> 00:01:00,460
a learning algorithm
predict the outcome


20
00:01:00,460 --> 00:01:04,120
of a biopsy or a
biological medical tests.


21
00:01:04,120 --> 00:01:06,625
I find that to be really useful.


22
00:01:06,625 --> 00:01:10,210
But when the ground truth
is defined by a human,


23
00:01:10,210 --> 00:01:12,595
maybe even a doctor
labeled an X-ray image,


24
00:01:12,595 --> 00:01:15,085
then HLP is just measuring


25
00:01:15,085 --> 00:01:17,500
how well can one doctor predict


26
00:01:17,500 --> 00:01:20,200
another doctor's label
versus how well can


27
00:01:20,200 --> 00:01:24,100
one learning algorithm predict
another doctor's label.


28
00:01:24,100 --> 00:01:26,184
That too is useful,


29
00:01:26,184 --> 00:01:28,240
but it's different than if you're


30
00:01:28,240 --> 00:01:30,610
measuring how well you versus


31
00:01:30,610 --> 00:01:32,440
a doctor are predicting


32
00:01:32,440 --> 00:01:35,845
some ground truth outcome
from a medical biopsy.


33
00:01:35,845 --> 00:01:37,220
To summarize, when


34
00:01:37,220 --> 00:01:40,179
the ground truth label
is externally defined,


35
00:01:40,179 --> 00:01:43,285
such as the medical biopsy,


36
00:01:43,285 --> 00:01:46,720
then HLP gives an estimate
for base error and


37
00:01:46,720 --> 00:01:49,090
irreducible error in
terms of predicting


38
00:01:49,090 --> 00:01:52,355
the outcome of that
medical test, the biopsy.


39
00:01:52,355 --> 00:01:54,640
But there are also a
lot of problems with


40
00:01:54,640 --> 00:01:57,370
the ground truth is just
another human label.


41
00:01:57,370 --> 00:02:00,400
The visual inspection
example we had from


42
00:02:00,400 --> 00:02:03,745
the previous video showed this,


43
00:02:03,745 --> 00:02:09,670
where the inspector had
66.7 percent accuracy.


44
00:02:09,670 --> 00:02:16,255
Rather than just aspiring to
beat the human inspector,


45
00:02:16,255 --> 00:02:21,190
it may be more useful to
see why the ground truth,


46
00:02:21,190 --> 00:02:23,620
which is just some other
inspector compared


47
00:02:23,620 --> 00:02:26,535
to this inspector don't agree.


48
00:02:26,535 --> 00:02:28,840
For example, if we look at


49
00:02:28,840 --> 00:02:31,930
the length of the different
scratches that they labeled,


50
00:02:31,930 --> 00:02:34,060
say, on these six examples,


51
00:02:34,060 --> 00:02:37,285
these were the length
of the scratches.


52
00:02:37,285 --> 00:02:41,980
If we speak of the inspectors
and have them agree that


53
00:02:41,980 --> 00:02:46,405
0.3 mm is the threshold


54
00:02:46,405 --> 00:02:49,329
above which a stretch
becomes a defect,


55
00:02:49,329 --> 00:02:54,010
then what we realize is
that for the first example,


56
00:02:54,010 --> 00:02:57,160
both label that one
totally appropriately.


57
00:02:57,160 --> 00:02:59,020
For the second example,


58
00:02:59,020 --> 00:03:03,160
the ground truth here is
one but is less than 0.3,


59
00:03:03,160 --> 00:03:06,670
so we really should
change this to zero,


60
00:03:06,670 --> 00:03:10,960
then 0.5 guess 1 1, 0.2 000.1.


61
00:03:10,960 --> 00:03:14,500
This example has
a stretch of 0.1,


62
00:03:14,500 --> 00:03:18,325
but really this should
have been a zero.


63
00:03:18,325 --> 00:03:21,595
If we go through this
exercise of getting


64
00:03:21,595 --> 00:03:26,755
the ground truth label and
this inspector to agree,


65
00:03:26,755 --> 00:03:30,400
then we actually just raise
human-level performance


66
00:03:30,400 --> 00:03:34,090
from 66.7 percent to 100 percent,


67
00:03:34,090 --> 00:03:36,805
at least as measured
on these six examples.


68
00:03:36,805 --> 00:03:39,820
But notice what we've done,


69
00:03:39,820 --> 00:03:43,660
by raising HLP to 100
percent we've made


70
00:03:43,660 --> 00:03:45,365
it pretty much impossible for


71
00:03:45,365 --> 00:03:47,695
learning algorithm to beat HLP,


72
00:03:47,695 --> 00:03:49,150
so that seems terrible.


73
00:03:49,150 --> 00:03:51,580
You can't tell the
business owner anymore,


74
00:03:51,580 --> 00:03:54,730
you beat HLP, and thus
they must use your system.


75
00:03:54,730 --> 00:03:58,480
But the benefit of this is
you now have much cleaner,


76
00:03:58,480 --> 00:04:01,060
more consistent data, and that


77
00:04:01,060 --> 00:04:02,500
ultimately will allow


78
00:04:02,500 --> 00:04:04,690
your learning algorithm
to do better.


79
00:04:04,690 --> 00:04:06,850
When you go is to come up with


80
00:04:06,850 --> 00:04:09,685
a learning algorithm
that actually generates


81
00:04:09,685 --> 00:04:11,980
accurate predictions rather than


82
00:04:11,980 --> 00:04:15,795
just proof for some reason
that you can beat HLP.


83
00:04:15,795 --> 00:04:18,790
I find this approach
of working to


84
00:04:18,790 --> 00:04:22,045
raise HLP to be more useful.


85
00:04:22,045 --> 00:04:23,455
To summarize,


86
00:04:23,455 --> 00:04:27,310
when the ground truth label
y comes from a human,


87
00:04:27,310 --> 00:04:29,155
HLP being


88
00:04:29,155 --> 00:04:32,470
quite a bit less than 100
percent may just indicate


89
00:04:32,470 --> 00:04:34,600
that the labeling instructions or


90
00:04:34,600 --> 00:04:38,095
labeling convention is ambiguous.


91
00:04:38,095 --> 00:04:40,390
On the last slide,
you saw an example of


92
00:04:40,390 --> 00:04:42,715
this in visual inspection.


93
00:04:42,715 --> 00:04:45,965
You also see this in
speech recognition where


94
00:04:45,965 --> 00:04:51,030
the camera versus ellipses...,


95
00:04:51,570 --> 00:04:55,610
that type of ambiguous
labeling convention will


96
00:04:55,610 --> 00:04:58,925
also cause HLP to be less
than 100 hundred percent.


97
00:04:58,925 --> 00:05:01,445
Improving label consistency


98
00:05:01,445 --> 00:05:05,080
will raise human-level
performance.


99
00:05:05,080 --> 00:05:06,830
This makes it harder,


100
00:05:06,830 --> 00:05:09,530
unfortunately for your
learning algorithm to beat


101
00:05:09,530 --> 00:05:12,230
HLP by the more consistent labels


102
00:05:12,230 --> 00:05:14,750
who raise your machine
learning album performance,


103
00:05:14,750 --> 00:05:16,430
which is ultimately likely to


104
00:05:16,430 --> 00:05:18,765
benefit the actual application.


105
00:05:18,765 --> 00:05:23,165
Far we've been discussing
HLP on unstructured data,


106
00:05:23,165 --> 00:05:27,480
but some of these issues apply
to structure data as well.


107
00:05:27,480 --> 00:05:29,270
You already know that
structured data problems are


108
00:05:29,270 --> 00:05:31,550
less likely to
involve human labors


109
00:05:31,550 --> 00:05:34,100
and thus HLP is less
frequently use.


110
00:05:34,100 --> 00:05:35,830
But there are exceptions.


111
00:05:35,830 --> 00:05:39,740
You saw previously the
user ID emerging example,


112
00:05:39,740 --> 00:05:42,020
where you might
have a human label,


113
00:05:42,020 --> 00:05:45,380
where the two records
belong to the same person.


114
00:05:45,400 --> 00:05:48,920
I've worked on projects
where we will look at


115
00:05:48,920 --> 00:05:51,545
network traffic into a computer


116
00:05:51,545 --> 00:05:54,005
to try to figure out if
the computer was hacked,


117
00:05:54,005 --> 00:05:57,665
and we as human IT experts
to provide labels for us.


118
00:05:57,665 --> 00:06:00,350
Sometimes it's hard to
know if a transaction is


119
00:06:00,350 --> 00:06:03,915
fraudulent and you just
ask a human to label that.


120
00:06:03,915 --> 00:06:09,010
Or is this a spam account or
a bot-generated accounts?


121
00:06:09,010 --> 00:06:11,545
Or from GPS,


122
00:06:11,545 --> 00:06:13,825
what is the mode
of transportation


123
00:06:13,825 --> 00:06:15,180
is this person on foot,


124
00:06:15,180 --> 00:06:19,090
or on a bike, or in
the car, or the bus.


125
00:06:19,090 --> 00:06:21,765
It turns out buses
stop at bus stops,


126
00:06:21,765 --> 00:06:24,465
and so you can actually
tell if someone is


127
00:06:24,465 --> 00:06:27,405
in a bus or in a car
based on the GPS trace.


128
00:06:27,405 --> 00:06:29,815
For problems like these,


129
00:06:29,815 --> 00:06:32,040
it would be quite
reasonable to ask


130
00:06:32,040 --> 00:06:35,234
a human to label the data,


131
00:06:35,234 --> 00:06:36,900
at least on the first pass for


132
00:06:36,900 --> 00:06:40,140
a learning algorithm to make
such predictions as these.


133
00:06:40,140 --> 00:06:42,210
When the ground truth
label you're trying


134
00:06:42,210 --> 00:06:44,340
to predict comes from one human,


135
00:06:44,340 --> 00:06:48,490
the same questions of
what does HLP mean?


136
00:06:48,490 --> 00:06:52,440
It is a useful baseline to
figure out what is possible.


137
00:06:52,440 --> 00:06:55,620
But sometimes when measuring HLP,


138
00:06:55,620 --> 00:07:01,545
you realize that low HLP stems
from inconsistent labels,


139
00:07:01,545 --> 00:07:06,150
and working to improve
HLP by coming up with


140
00:07:06,150 --> 00:07:10,320
a more consistent labeling
standard will both raise


141
00:07:10,320 --> 00:07:11,850
HLP and give you


142
00:07:11,850 --> 00:07:13,710
cleaner data with which to


143
00:07:13,710 --> 00:07:15,780
improve your learning
experience performance.


144
00:07:15,780 --> 00:07:18,300
Here's what I hope you
take away from this video.


145
00:07:18,300 --> 00:07:22,200
First, HLP is
important for problems


146
00:07:22,200 --> 00:07:24,330
where human-level performance can


147
00:07:24,330 --> 00:07:25,920
provide a useful reference.


148
00:07:25,920 --> 00:07:29,670
I do measure it and use it as
a reference for what might


149
00:07:29,670 --> 00:07:33,885
be possible and to drive air
analysis and prioritization.


150
00:07:33,885 --> 00:07:37,355
Having said that, when
you're measuring HLP,


151
00:07:37,355 --> 00:07:41,300
if you find the HLP is much
less than 100 percent,


152
00:07:41,300 --> 00:07:46,260
also ask yourself if some
of the gap between HLP and


153
00:07:46,260 --> 00:07:48,480
complete consistency is due


154
00:07:48,480 --> 00:07:51,540
to inconsistent
labeling instructions.


155
00:07:51,540 --> 00:07:53,745
Because if that turns
out to be the case,


156
00:07:53,745 --> 00:07:57,475
then improving labeling
consistency will raise


157
00:07:57,475 --> 00:07:59,610
HLP and also give


158
00:07:59,610 --> 00:08:02,085
cleaner data for your
learning algorithm,


159
00:08:02,085 --> 00:08:03,780
which will ultimately result in


160
00:08:03,780 --> 00:08:06,885
better machine-learning
algorithm performance.


161
00:08:06,885 --> 00:08:09,665
Guess what I hope you take
away from this video.


162
00:08:09,665 --> 00:08:13,805
HLP is useful and important
for many applications.


163
00:08:13,805 --> 00:08:16,070
For problems where
I think how well


164
00:08:16,070 --> 00:08:18,490
humans perform is a
useful reference,


165
00:08:18,490 --> 00:08:20,810
I do measure HLP and I use


166
00:08:20,810 --> 00:08:23,390
that to get a sense of
what might be possible,


167
00:08:23,390 --> 00:08:25,370
and also use HLP to


168
00:08:25,370 --> 00:08:28,535
drive error analysis
and preservation.


169
00:08:28,535 --> 00:08:30,290
Having said that, if,


170
00:08:30,290 --> 00:08:32,345
in the process of measuring HLP,


171
00:08:32,345 --> 00:08:35,975
you find that HLP is much less
than perfect performance,


172
00:08:35,975 --> 00:08:37,510
much lower than 100 percent.


173
00:08:37,510 --> 00:08:39,460
This is also worth
asking yourself,


174
00:08:39,460 --> 00:08:42,430
if that gap between HLP and


175
00:08:42,430 --> 00:08:44,780
100 percent accuracy may be due


176
00:08:44,780 --> 00:08:47,645
to inconsistent
labeling instructions.


177
00:08:47,645 --> 00:08:48,920
Because if that's the case,


178
00:08:48,920 --> 00:08:53,390
then improving labeling
consistency will both raise HLP,


179
00:08:53,390 --> 00:08:55,790
but more importantly help you get


180
00:08:55,790 --> 00:08:58,190
cleaner and more
consistent labels which


181
00:08:58,190 --> 00:09:01,000
will improve your learning
algorithm's performance
1
00:00:00,000 --> 00:00:01,500
You've learned about how to


2
00:00:01,500 --> 00:00:04,170
define what should be the data,


3
00:00:04,170 --> 00:00:06,479
what should be the
definition of y,


4
00:00:06,479 --> 00:00:10,005
what should be the
definition of the input x.


5
00:00:10,005 --> 00:00:12,420
But how do you actually go about


6
00:00:12,420 --> 00:00:15,045
obtaining data for your task?


7
00:00:15,045 --> 00:00:17,750
Let's take a look at
some best practices.


8
00:00:17,750 --> 00:00:21,120
One key question I would
urge you to think about


9
00:00:21,120 --> 00:00:25,350
is how much time should
you spend obtaining data?


10
00:00:25,350 --> 00:00:28,350
You know that machine learning is


11
00:00:28,350 --> 00:00:30,390
a highly iterative
process where you


12
00:00:30,390 --> 00:00:32,609
need to pick a model,
hyperparameters,


13
00:00:32,609 --> 00:00:35,730
have a data set, then
training to carry out


14
00:00:35,730 --> 00:00:37,920
our analysis and go
around this loop multiple


15
00:00:37,920 --> 00:00:40,210
times to get to a good model.


16
00:00:40,210 --> 00:00:42,240
Let's say for the
sake of argument that


17
00:00:42,240 --> 00:00:43,380
training your model for


18
00:00:43,380 --> 00:00:45,465
the first time takes
a couple of days,


19
00:00:45,465 --> 00:00:48,535
maybe much shorter
or maybe longer.


20
00:00:48,535 --> 00:00:50,340
Let's say just for the sake of


21
00:00:50,340 --> 00:00:52,515
arguments that carrying out


22
00:00:52,515 --> 00:00:55,830
error analysis for your project


23
00:00:55,830 --> 00:00:58,155
for the first time may
take a couple of days.


24
00:00:58,155 --> 00:00:59,940
If this is the case,


25
00:00:59,940 --> 00:01:06,734
I would urge you not to spend
30 days collecting data,


26
00:01:06,734 --> 00:01:08,520
because that will delay by


27
00:01:08,520 --> 00:01:13,075
a whole month your getting
into this iteration.


28
00:01:13,075 --> 00:01:15,230
Instead, I urge you to get in


29
00:01:15,230 --> 00:01:18,330
this iteration loop as
quickly as possible.


30
00:01:18,330 --> 00:01:20,130
Training a model and


31
00:01:20,130 --> 00:01:22,790
error analysis might take
just a couple of days.


32
00:01:22,790 --> 00:01:24,840
I would urge you to ask yourself,


33
00:01:24,840 --> 00:01:26,790
what if you were to give yourself


34
00:01:26,790 --> 00:01:28,965
only two days to collect data?


35
00:01:28,965 --> 00:01:35,400
Would that help get you into
this loop much more quickly?


36
00:01:35,400 --> 00:01:37,265
Maybe two days is too short,


37
00:01:37,265 --> 00:01:41,360
but I've seen far too
many teams take too


38
00:01:41,360 --> 00:01:43,640
long to collect
their initial data


39
00:01:43,640 --> 00:01:46,720
set before they train
the initial model.


40
00:01:46,720 --> 00:01:50,660
Whereas I've rarely come
across a team where I said,


41
00:01:50,660 --> 00:01:52,280
"Hey, you really
should have spent


42
00:01:52,280 --> 00:01:54,535
more time collecting data."


43
00:01:54,535 --> 00:01:56,865
After you've trained
your initial model,


44
00:01:56,865 --> 00:01:59,210
carry out error analysis
there's plenty of time


45
00:01:59,210 --> 00:02:02,210
to go back and collect more data.


46
00:02:02,210 --> 00:02:04,580
I found for a lot of projects


47
00:02:04,580 --> 00:02:07,220
I've led when I go
to the team and say,


48
00:02:07,220 --> 00:02:09,110
"Hey everyone, we're
going to spend


49
00:02:09,110 --> 00:02:11,380
at most seven days
collecting data,


50
00:02:11,380 --> 00:02:13,160
so what can we do?"


51
00:02:13,160 --> 00:02:15,770
I found that posing
the question that way


52
00:02:15,770 --> 00:02:18,950
often leads to much
more creative ways,


53
00:02:18,950 --> 00:02:21,650
our ways are still
100 percent respect


54
00:02:21,650 --> 00:02:25,910
user privacy and follow
regulatory considerations if any,


55
00:02:25,910 --> 00:02:27,860
but much more creative,


56
00:02:27,860 --> 00:02:31,100
scrappy ways to get a
lot of data quickly.


57
00:02:31,100 --> 00:02:35,270
That allows you to enter this
iteration loop much more


58
00:02:35,270 --> 00:02:39,515
quickly and let the project
make faster progress.


59
00:02:39,515 --> 00:02:42,170
One exception to this guideline


60
00:02:42,170 --> 00:02:45,605
is if you have worked
on this problem before,


61
00:02:45,605 --> 00:02:48,035
and if from experience you know


62
00:02:48,035 --> 00:02:50,840
you need at least a
certain training set size.


63
00:02:50,840 --> 00:02:53,000
Then it might be okay to invest


64
00:02:53,000 --> 00:02:56,655
more effort up front to
collect that much data.


65
00:02:56,655 --> 00:02:58,610
Because I've worked on


66
00:02:58,610 --> 00:03:01,010
speech recognition I
have a good sense of


67
00:03:01,010 --> 00:03:03,230
how much data I'll need
to do certain things


68
00:03:03,230 --> 00:03:05,540
and I know it's just
not worth trying,


69
00:03:05,540 --> 00:03:07,460
to train certain
models if I have less


70
00:03:07,460 --> 00:03:10,305
than certain number
of hours of data.


71
00:03:10,305 --> 00:03:12,500
But a lot of the time if you're


72
00:03:12,500 --> 00:03:15,170
working on a brand
new problem and


73
00:03:15,170 --> 00:03:17,660
if you are not sure and is


74
00:03:17,660 --> 00:03:20,525
often hard to tell even
from the literature,


75
00:03:20,525 --> 00:03:24,950
but if you're not sure just
how much data is needed,


76
00:03:24,950 --> 00:03:26,690
then is much better to


77
00:03:26,690 --> 00:03:28,940
quickly collect a
small amount of data,


78
00:03:28,940 --> 00:03:32,540
train a model and then use
error analysis to tell you


79
00:03:32,540 --> 00:03:36,450
if is worth your while to go
out to collect more data.


80
00:03:36,450 --> 00:03:40,745
In terms of getting
the data you need,


81
00:03:40,745 --> 00:03:45,335
one other step I
often carry out is to


82
00:03:45,335 --> 00:03:50,180
take inventory of
possible data sources.


83
00:03:50,180 --> 00:03:53,700
Let's continue to use speech
recognition as an example.


84
00:03:53,700 --> 00:03:58,280
If you were to brainstorm
a list of data sources,


85
00:03:58,280 --> 00:04:01,635
this is maybe what you
might come up with.


86
00:04:01,635 --> 00:04:06,475
Maybe I already own 100 hours
of transcribed speech data,


87
00:04:06,475 --> 00:04:09,875
and because you already own
it the cost of that is zero.


88
00:04:09,875 --> 00:04:11,870
Or you may be able to use


89
00:04:11,870 --> 00:04:16,640
a crowdsourcing platform and
pay people to read text.


90
00:04:16,640 --> 00:04:19,160
You provide them a piece of text


91
00:04:19,160 --> 00:04:21,845
and ask them to read
it out loud and just


92
00:04:21,845 --> 00:04:24,770
create text data where
you already have


93
00:04:24,770 --> 00:04:26,330
the transcript because
they were reading


94
00:04:26,330 --> 00:04:28,590
a piece of text that you have.


95
00:04:28,590 --> 00:04:31,070
Or you may decide to take


96
00:04:31,070 --> 00:04:33,935
audio that you have that
hasn't been labeled yet,


97
00:04:33,935 --> 00:04:36,895
and to pay for it
to be transcribed.


98
00:04:36,895 --> 00:04:39,530
It turns out this is
more expensive on


99
00:04:39,530 --> 00:04:42,950
a per hour basis than paying
people to read texts,


100
00:04:42,950 --> 00:04:45,290
but this results in
audio that sounds more


101
00:04:45,290 --> 00:04:48,045
natural because people
aren't reading.


102
00:04:48,045 --> 00:04:51,020
For 100 hours of data it may cost


103
00:04:51,020 --> 00:04:54,440
$6,000 to get high
quality transcripts.


104
00:04:54,440 --> 00:04:56,120
Or you may find


105
00:04:56,120 --> 00:04:59,545
some commercial organizations
that could sell you data.


106
00:04:59,545 --> 00:05:01,320
Through an exercise like this,


107
00:05:01,320 --> 00:05:02,200
you can brainstorm what


108
00:05:02,200 --> 00:05:04,240
are the different types
of data you might use


109
00:05:04,240 --> 00:05:06,630
as well as their
associated costs.


110
00:05:06,630 --> 00:05:09,900
One column that's
missing from this that I


111
00:05:09,900 --> 00:05:13,085
find very important
is the time costs,


112
00:05:13,085 --> 00:05:15,430
so how long will it
take you to execute


113
00:05:15,430 --> 00:05:18,505
a project to get these
different types of data?


114
00:05:18,505 --> 00:05:22,630
For the owned data you could
get that instantaneously.


115
00:05:22,630 --> 00:05:24,640
For crowdsourced reading you


116
00:05:24,640 --> 00:05:26,945
may need to implement
a bunch of software,


117
00:05:26,945 --> 00:05:29,200
find the right
crowdsourcing platform,


118
00:05:29,200 --> 00:05:31,930
carry out software integration
so you might estimate


119
00:05:31,930 --> 00:05:34,780
that that's two weeks
of engineering work.


120
00:05:34,780 --> 00:05:37,390
Paying for data to be
labeled is simpler but


121
00:05:37,390 --> 00:05:40,360
still is work to
organize and manage,


122
00:05:40,360 --> 00:05:43,210
whereas purchasing
data maybe there's


123
00:05:43,210 --> 00:05:46,050
a purchase order process
that may be much quicker.


124
00:05:46,050 --> 00:05:48,640
I find it some teams
won't go through


125
00:05:48,640 --> 00:05:50,380
an inventory process like


126
00:05:50,380 --> 00:05:52,810
this and would just
pick a random idea,


127
00:05:52,810 --> 00:05:56,230
and maybe decide to


128
00:05:56,230 --> 00:05:59,140
use crowdsourcing and
reading to collect data.


129
00:05:59,140 --> 00:06:01,015
But if you can sit down,


130
00:06:01,015 --> 00:06:03,460
write to all the
different data sources


131
00:06:03,460 --> 00:06:05,740
and think through the trade-offs,


132
00:06:05,740 --> 00:06:08,395
including costs and time,


133
00:06:08,395 --> 00:06:10,390
then that can help you to make


134
00:06:10,390 --> 00:06:15,150
often better decisions about
what sources of data to use.


135
00:06:15,150 --> 00:06:18,230
If you are especially
pressed with time,


136
00:06:18,230 --> 00:06:20,590
based on this analysis
you may decide


137
00:06:20,590 --> 00:06:22,960
to use the data you
already own and


138
00:06:22,960 --> 00:06:27,580
maybe purchase some
data and use that


139
00:06:27,580 --> 00:06:30,440
over the middle two options


140
00:06:30,440 --> 00:06:32,510
in order to get
going more quickly.


141
00:06:32,510 --> 00:06:35,160
In addition to the
amount of data you can


142
00:06:35,160 --> 00:06:38,595
acquire and the financial
costs and the time costs,


143
00:06:38,595 --> 00:06:41,480
other important factors that's


144
00:06:41,480 --> 00:06:45,365
application dependent will
include data quality.


145
00:06:45,365 --> 00:06:47,430
Where you may decide for example,


146
00:06:47,430 --> 00:06:49,200
that paying for
labels actually gives


147
00:06:49,200 --> 00:06:50,640
more natural audio than


148
00:06:50,640 --> 00:06:53,505
having people sound
like they're reading,


149
00:06:53,505 --> 00:06:56,685
as well as really important the


150
00:06:56,685 --> 00:06:59,895
privacy and regulatory
constraints.


151
00:06:59,895 --> 00:07:02,670
If you decide to
get data labeled,


152
00:07:02,670 --> 00:07:07,425
here are some options you
might think through as well.


153
00:07:07,425 --> 00:07:11,400
The three most common ways
to get data labeled are,


154
00:07:11,400 --> 00:07:15,270
in house where you have your
own team label the data,


155
00:07:15,270 --> 00:07:18,360
versus outsource
where you might find


156
00:07:18,360 --> 00:07:19,620
some company that labels


157
00:07:19,620 --> 00:07:22,245
data and have them do it for you,


158
00:07:22,245 --> 00:07:24,990
versus crowdsource
where you might use


159
00:07:24,990 --> 00:07:27,570
a crowdsourcing platform to have


160
00:07:27,570 --> 00:07:31,325
a large group collectively
label the data.


161
00:07:31,325 --> 00:07:33,150
The difference between outsource


162
00:07:33,150 --> 00:07:34,905
versus crowdsource is that,


163
00:07:34,905 --> 00:07:37,320
depending on what type
of data you have,


164
00:07:37,320 --> 00:07:39,600
there may be specialized
companies that


165
00:07:39,600 --> 00:07:42,245
could help you get the
label quite efficiently.


166
00:07:42,245 --> 00:07:45,390
Some of the trade offs
between these options,


167
00:07:45,390 --> 00:07:47,760
having machine learning
engineers label


168
00:07:47,760 --> 00:07:51,000
data is often expensive.


169
00:07:51,000 --> 00:07:53,700
But I find that to get a
project going quickly,


170
00:07:53,700 --> 00:07:56,040
having machine learning
engineers do this just


171
00:07:56,040 --> 00:07:58,455
for a few days is usually
fine and in fact,


172
00:07:58,455 --> 00:08:00,090
this can help build


173
00:08:00,090 --> 00:08:03,745
the machine learning engineers
intuition about the data.


174
00:08:03,745 --> 00:08:06,250
When I'm working
on a new project,


175
00:08:06,250 --> 00:08:08,460
I often don't mind spending


176
00:08:08,460 --> 00:08:11,280
a few hours or maybe a day or two


177
00:08:11,280 --> 00:08:14,130
labeling data myself
if that helps me


178
00:08:14,130 --> 00:08:17,425
to build my intuition
about the project.


179
00:08:17,425 --> 00:08:20,880
But beyond a certain
point you may not want


180
00:08:20,880 --> 00:08:22,680
to spend all your time as


181
00:08:22,680 --> 00:08:24,825
a machine learning
engineer labeling data,


182
00:08:24,825 --> 00:08:26,880
and you might want to shift to


183
00:08:26,880 --> 00:08:30,415
a more scalable
labeling processes.


184
00:08:30,415 --> 00:08:32,495
Depending on your application,


185
00:08:32,495 --> 00:08:33,869
there may be also


186
00:08:33,869 --> 00:08:37,170
different groups or subgroups
of individuals that


187
00:08:37,170 --> 00:08:42,585
are going to be more qualified
to provide the labels, y.


188
00:08:42,585 --> 00:08:45,840
If you're working on
speech recognition,


189
00:08:45,840 --> 00:08:49,755
then maybe almost any
reasonably fluent speaker


190
00:08:49,755 --> 00:08:52,615
can listen to audio
and transcribe it.


191
00:08:52,615 --> 00:08:55,170
Speech recognition, because of


192
00:08:55,170 --> 00:08:58,215
the number of people that
speak a certain language,


193
00:08:58,215 --> 00:09:01,800
has a very large pool
of potential labels,


194
00:09:01,800 --> 00:09:05,065
well, hopefully they will
be careful and diligent.


195
00:09:05,065 --> 00:09:08,610
For more specialized applications


196
00:09:08,610 --> 00:09:12,810
like factory inspection or
medical image diagnosis,


197
00:09:12,810 --> 00:09:16,170
a typical person off the
street probably can't look at


198
00:09:16,170 --> 00:09:19,130
a medical X-ray image
and diagnose from it,


199
00:09:19,130 --> 00:09:22,010
or look at a smartphone and


200
00:09:22,010 --> 00:09:25,905
determine what is and
what isn't a defect.


201
00:09:25,905 --> 00:09:28,550
More specialized tasks
like these usually


202
00:09:28,550 --> 00:09:31,670
require an SME or
subject matter expert,


203
00:09:31,670 --> 00:09:35,025
in order to provide
accurate labels.


204
00:09:35,025 --> 00:09:38,400
Finally there are some
applications was very


205
00:09:38,400 --> 00:09:41,985
difficult to get anyone
to give good labels.


206
00:09:41,985 --> 00:09:45,000
Take product
recommendations, there are


207
00:09:45,000 --> 00:09:48,120
probably product recommendation
systems that are


208
00:09:48,120 --> 00:09:51,180
giving better
recommendations to you than


209
00:09:51,180 --> 00:09:55,085
even your best friends or
maybe your significant other.


210
00:09:55,085 --> 00:10:00,165
For this, you may
just have to rely on


211
00:10:00,165 --> 00:10:02,545
purchase data by the user as


212
00:10:02,545 --> 00:10:06,840
a label rather than get
humans to label this.


213
00:10:06,840 --> 00:10:09,300
When you're working
on an application,


214
00:10:09,300 --> 00:10:10,530
figuring out which of


215
00:10:10,530 --> 00:10:13,500
these categories of application
you're working on and


216
00:10:13,500 --> 00:10:16,020
identifying the right
type of person or


217
00:10:16,020 --> 00:10:18,840
persons to help you label,


218
00:10:18,840 --> 00:10:21,210
will be an important
step to making


219
00:10:21,210 --> 00:10:24,900
sure your labels
are high quality.


220
00:10:24,900 --> 00:10:31,290
One last tip. Let's say
you have 1,000 examples,


221
00:10:31,290 --> 00:10:34,780
and you've decided you
need a bigger data set.


222
00:10:34,780 --> 00:10:38,255
How much bigger should
you make your data set?


223
00:10:38,255 --> 00:10:41,430
One tip I've given
a lot of teams is


224
00:10:41,430 --> 00:10:45,360
don't increase your data by
more than 10x at a time.


225
00:10:45,360 --> 00:10:48,300
If you have a 1,000 examples


226
00:10:48,300 --> 00:10:51,675
and you've trained your
model on 1,000 examples,


227
00:10:51,675 --> 00:10:53,550
maybe it's worth investing


228
00:10:53,550 --> 00:10:55,470
to try to increase
your dataset to


229
00:10:55,470 --> 00:10:59,505
3,000 examples or maybe
at most 10,000 examples.


230
00:10:59,505 --> 00:11:01,920
But I would first do


231
00:11:01,920 --> 00:11:05,865
a 10x or less than
10x increase first,


232
00:11:05,865 --> 00:11:07,620
train another model,


233
00:11:07,620 --> 00:11:10,020
carry out error
analysis and only then


234
00:11:10,020 --> 00:11:12,090
figure out if it's worth


235
00:11:12,090 --> 00:11:14,595
increasing it
substantially beyond that.


236
00:11:14,595 --> 00:11:18,780
Because once you increase
your dataset size by 10x,


237
00:11:18,780 --> 00:11:21,220
so many things change
is really difficult.


238
00:11:21,220 --> 00:11:24,030
I found this really hard to
predict what will happen


239
00:11:24,030 --> 00:11:27,835
when your data set size
increases even beyond that.


240
00:11:27,835 --> 00:11:31,380
Is also fine to increase
your dataset size 10 percent


241
00:11:31,380 --> 00:11:35,280
or 50 percent or
just 2x at a time,


242
00:11:35,280 --> 00:11:38,340
so this is only an upper
bound for how much you


243
00:11:38,340 --> 00:11:42,180
might invest to increase
your dataset size.


244
00:11:42,180 --> 00:11:45,660
This guideline
hopefully will help


245
00:11:45,660 --> 00:11:49,755
teams avoid over investing
in tons of data,


246
00:11:49,755 --> 00:11:51,960
only to realize that collecting


247
00:11:51,960 --> 00:11:54,030
quite that much data wasn't


248
00:11:54,030 --> 00:11:56,760
the most useful thing
they could have done.


249
00:11:56,760 --> 00:11:59,610
I hope the tips in this video
will help you to be more


250
00:11:59,610 --> 00:12:03,665
efficient in how you go
about collecting your data.


251
00:12:03,665 --> 00:12:06,180
Now, when you collect your data,


252
00:12:06,180 --> 00:12:08,430
one of the things you
might run into is


253
00:12:08,430 --> 00:12:11,115
the need to build
a data pipeline.


254
00:12:11,115 --> 00:12:14,070
Where your data doesn't come
all at once but there are


255
00:12:14,070 --> 00:12:16,020
most complete processing steps


256
00:12:16,020 --> 00:12:18,110
that your data has to go through.


257
00:12:18,110 --> 00:12:20,700
Let's go on to the next
video to take a look at


258
00:12:20,700 --> 00:12:25,240
some best practices for
building data pipelines.
1
00:00:00,000 --> 00:00:02,580
Data pipelines, sometimes also


2
00:00:02,580 --> 00:00:05,175
called Data Cascades refers to


3
00:00:05,175 --> 00:00:07,590
when your data has
multiple steps of


4
00:00:07,590 --> 00:00:10,945
processing before getting
to the final output.


5
00:00:10,945 --> 00:00:12,630
There's some best practices


6
00:00:12,630 --> 00:00:15,810
relevant for managing
such data pipelines.


7
00:00:15,810 --> 00:00:17,430
Let's start with an example.


8
00:00:17,430 --> 00:00:21,570
Let's say that given
some user information


9
00:00:21,570 --> 00:00:23,190
you would like to predict


10
00:00:23,190 --> 00:00:26,730
if a given user is
looking for a job,


11
00:00:26,730 --> 00:00:28,200
because if they're
looking for a job


12
00:00:28,200 --> 00:00:29,460
at this moment in time,


13
00:00:29,460 --> 00:00:31,920
you may want to surface job ads


14
00:00:31,920 --> 00:00:36,090
or other pieces of perfectly
useful information to them.


15
00:00:36,090 --> 00:00:40,785
Given raw data such
as the data on top,


16
00:00:40,785 --> 00:00:46,350
there's often some pre-processing
or data cleaning before


17
00:00:46,350 --> 00:00:49,590
the data is fed to
learning algorithm that


18
00:00:49,590 --> 00:00:53,805
then tries to predict, why?


19
00:00:53,805 --> 00:00:55,935
Are they looking for a job?


20
00:00:55,935 --> 00:00:59,795
The data cleaning may include
things like spam cleanup,


21
00:00:59,795 --> 00:01:02,330
such as removing
the spam accounts,


22
00:01:02,330 --> 00:01:05,090
and maybe also user ID merge


23
00:01:05,090 --> 00:01:07,685
which we talked about
in an earlier video.


24
00:01:07,685 --> 00:01:10,820
For the sake of this example,


25
00:01:10,820 --> 00:01:12,800
let's say that spam clean-up and


26
00:01:12,800 --> 00:01:17,220
user ID merge are done
with just scripting.


27
00:01:17,590 --> 00:01:22,849
Explicit sequences of
instructions that tells your code


28
00:01:22,849 --> 00:01:24,590
when is an account
to be considered


29
00:01:24,590 --> 00:01:28,025
spammy and when should
two user IDs be merged.


30
00:01:28,025 --> 00:01:29,870
These systems could be built


31
00:01:29,870 --> 00:01:31,550
using machine learning
algorithms as


32
00:01:31,550 --> 00:01:33,350
well which makes them even


33
00:01:33,350 --> 00:01:35,315
a little bit more
complex to manage.


34
00:01:35,315 --> 00:01:39,245
Now, when you have strips
for the data cleaning,


35
00:01:39,245 --> 00:01:42,905
one of the issues you run into is


36
00:01:42,905 --> 00:01:45,440
replicability when you take


37
00:01:45,440 --> 00:01:48,745
these systems into
production deployment.


38
00:01:48,745 --> 00:01:51,410
Let's say during the
development of the system,


39
00:01:51,410 --> 00:01:55,864
you have input data fed through
pre-processing scripts,


40
00:01:55,864 --> 00:01:57,890
and the pre-processed data


41
00:01:57,890 --> 00:02:01,144
is fed to a machine
learning algorithm,


42
00:02:01,144 --> 00:02:04,280
and after some amount of work,


43
00:02:04,280 --> 00:02:07,990
your learning algorithm
does well on the test set.


44
00:02:07,990 --> 00:02:10,815
Printed development
phase, you may have seen


45
00:02:10,815 --> 00:02:13,590
that pre-processing scripts
can be quite messy.


46
00:02:13,590 --> 00:02:16,175
It may be you hacking
something up,


47
00:02:16,175 --> 00:02:17,570
processing data,


48
00:02:17,570 --> 00:02:20,480
mailing a file to a different
member of your team,


49
00:02:20,480 --> 00:02:24,530
having them have a few
incantations in Python or


50
00:02:24,530 --> 00:02:26,390
some scripting language to


51
00:02:26,390 --> 00:02:28,700
process the data and having them,


52
00:02:28,700 --> 00:02:30,950
mail the process
data back to you.


53
00:02:30,950 --> 00:02:34,370
When you take this
system to production,


54
00:02:34,370 --> 00:02:37,055
you then have new data


55
00:02:37,055 --> 00:02:40,010
which has to be fed
through a similar set of


56
00:02:40,010 --> 00:02:42,980
scripts because this data is


57
00:02:42,980 --> 00:02:47,050
going to be fed to the same
machine learning algorithm.


58
00:02:47,050 --> 00:02:49,550
Your machine-learning
algorithm on


59
00:02:49,550 --> 00:02:52,700
this data is what will
run in your product.


60
00:02:52,700 --> 00:02:55,120
The key question is,


61
00:02:55,120 --> 00:03:00,179
if you're pre-processing was
done with a bunch of strips,


62
00:03:00,179 --> 00:03:01,440
spread out on a bunch of


63
00:03:01,440 --> 00:03:03,795
different people's
computers and laptops,


64
00:03:03,795 --> 00:03:06,180
how do you replicate
the strips to


65
00:03:06,180 --> 00:03:08,640
make sure that the
input distribution to


66
00:03:08,640 --> 00:03:10,770
a machine learning
algorithm was the


67
00:03:10,770 --> 00:03:14,715
same for the development data
and the production data?


68
00:03:14,715 --> 00:03:17,130
I find that the amount of effort


69
00:03:17,130 --> 00:03:19,680
that you should
invest to make sure


70
00:03:19,680 --> 00:03:22,740
that the pre-processing
scripts are highly


71
00:03:22,740 --> 00:03:24,900
replicable can depend a little


72
00:03:24,900 --> 00:03:27,645
bit on the face of the project.


73
00:03:27,645 --> 00:03:30,450
I know that it may be
fashionable to say that


74
00:03:30,450 --> 00:03:32,984
everything you do should
be 100 percent replicable,


75
00:03:32,984 --> 00:03:35,310
and I'll probably
get some criticism


76
00:03:35,310 --> 00:03:38,420
for not hewing to that line,


77
00:03:38,420 --> 00:03:40,860
but I find it a
lot of projects do


78
00:03:40,860 --> 00:03:43,800
go through a proof of
concept or POC phase,


79
00:03:43,800 --> 00:03:45,525
and then a production phase


80
00:03:45,525 --> 00:03:47,775
where during the proof
of concept phase,


81
00:03:47,775 --> 00:03:51,090
the primary goal is
just to decide if


82
00:03:51,090 --> 00:03:53,144
the application is workable


83
00:03:53,144 --> 00:03:55,655
and worth building and deploying.


84
00:03:55,655 --> 00:03:58,860
My advice to most teams
is during the proof of


85
00:03:58,860 --> 00:04:03,240
concept phase focus on getting
the prototype to work.


86
00:04:03,240 --> 00:04:08,910
It's okay if some of the data
pre-processing is manual.


87
00:04:08,910 --> 00:04:10,709
If the project succeeds,


88
00:04:10,709 --> 00:04:14,160
you need to replicate all of
this pre-processing later.


89
00:04:14,160 --> 00:04:17,010
My advice would be
take extensive notes,


90
00:04:17,010 --> 00:04:19,560
write extensive
comments to increase


91
00:04:19,560 --> 00:04:21,270
the odds that you can replicate


92
00:04:21,270 --> 00:04:23,395
all this pre-processing later,


93
00:04:23,395 --> 00:04:26,070
but this is also not
the time to get bogged


94
00:04:26,070 --> 00:04:28,740
down in tons of process just to


95
00:04:28,740 --> 00:04:32,040
ensure replicability
when the focus


96
00:04:32,040 --> 00:04:34,020
is really to just decide


97
00:04:34,020 --> 00:04:36,845
if the application is


98
00:04:36,845 --> 00:04:40,160
workable and is worth
taking to the next phase.


99
00:04:40,160 --> 00:04:41,660
Once you decided that


100
00:04:41,660 --> 00:04:44,430
this project is worth
taking to production,


101
00:04:44,430 --> 00:04:46,910
then you know it's
going to be really


102
00:04:46,910 --> 00:04:51,075
important to do the replica
any pre-processing strips.


103
00:04:51,075 --> 00:04:54,530
In this phase, that's
when I would use


104
00:04:54,530 --> 00:04:56,750
more sophisticated
tools to make sure


105
00:04:56,750 --> 00:04:59,825
the entire data
pipeline is replicable.


106
00:04:59,825 --> 00:05:02,195
This is when tools


107
00:05:02,195 --> 00:05:04,339
which can be a little
bit more heavyweight,


108
00:05:04,339 --> 00:05:07,445
but tools like TensorFlow
Transform, Apache beam,


109
00:05:07,445 --> 00:05:10,925
Airflow, and so on
become very valuable.


110
00:05:10,925 --> 00:05:13,050
In fact, you will
learn more about


111
00:05:13,050 --> 00:05:16,835
TensorFlow Transform later
into specialization as well.


112
00:05:16,835 --> 00:05:20,000
In this video, you learned
about data pipelines,


113
00:05:20,000 --> 00:05:23,435
and when to invest in
their replicability.


114
00:05:23,435 --> 00:05:25,970
It turns out many
applications have


115
00:05:25,970 --> 00:05:28,610
significantly more
complex data pipelines


116
00:05:28,610 --> 00:05:30,830
than what we saw in this video.


117
00:05:30,830 --> 00:05:33,170
For those settings, you also have


118
00:05:33,170 --> 00:05:35,450
to think about what metadata you


119
00:05:35,450 --> 00:05:38,120
want and perhaps also keep track


120
00:05:38,120 --> 00:05:41,565
and take care of data
provenance and lineage.


121
00:05:41,565 --> 00:05:45,810
Let's go on to the next video
to look at these topics.
1
00:00:00,000 --> 00:00:04,890
For some applications, having
and tracking metadata,


2
00:00:04,890 --> 00:00:08,565
data provenance, and data
lineage can be a big help.


3
00:00:08,565 --> 00:00:10,605
What do these words even mean?


4
00:00:10,605 --> 00:00:12,525
Let's look at an example.


5
00:00:12,525 --> 00:00:16,590
Here's a more complex
example of a data pipeline,


6
00:00:16,590 --> 00:00:20,310
building on our previous
example of using


7
00:00:20,310 --> 00:00:22,650
user records to
predict if someone


8
00:00:22,650 --> 00:00:25,590
is looking for a job at
a given moment in time.


9
00:00:25,590 --> 00:00:28,259
Let's say you start off
with a spam dataset.


10
00:00:28,259 --> 00:00:30,270
This may include a list of


11
00:00:30,270 --> 00:00:33,735
known spam accounts
as well as features


12
00:00:33,735 --> 00:00:35,760
such as a list of


13
00:00:35,760 --> 00:00:39,900
blacklisted IP addresses that
spammers are known to use.


14
00:00:39,900 --> 00:00:43,250
You might also implement
a learning algorithm or


15
00:00:43,250 --> 00:00:44,840
a piece of machine learning code


16
00:00:44,840 --> 00:00:47,015
and train your
learning algorithm,


17
00:00:47,015 --> 00:00:51,460
understand dataset, thus
giving you an anti-spam model.


18
00:00:51,460 --> 00:00:54,240
These arrows indicate flow


19
00:00:54,240 --> 00:00:57,225
of information or
flow of computation,


20
00:00:57,225 --> 00:00:59,070
where training your ML code on


21
00:00:59,070 --> 00:01:02,200
the spam dataset gives
you your anti-spam model.


22
00:01:02,200 --> 00:01:03,950
You then take your user data


23
00:01:03,950 --> 00:01:06,110
and apply the anti-spam model


24
00:01:06,110 --> 00:01:10,645
to it to get the
disband user data.


25
00:01:10,645 --> 00:01:14,180
We're following our usual
convention that things with


26
00:01:14,180 --> 00:01:18,085
the purple rectangle around
it represent pieces of code.


27
00:01:18,085 --> 00:01:21,235
Now, taking your
de-spammed user data.


28
00:01:21,235 --> 00:01:24,575
Next, you may want to
carry out user ID merge.


29
00:01:24,575 --> 00:01:28,380
To do that, you might start
off with some ID merged data.


30
00:01:28,380 --> 00:01:30,050
This would be labeled data


31
00:01:30,050 --> 00:01:32,660
telling you some pairs of
accounts that actually


32
00:01:32,660 --> 00:01:34,660
correspond to the same person


33
00:01:34,660 --> 00:01:37,640
have a machine learning
algorithm implementation,


34
00:01:37,640 --> 00:01:39,245
train the model on that,


35
00:01:39,245 --> 00:01:40,790
and this gives you a learned


36
00:01:40,790 --> 00:01:42,770
ID merge model that tells you


37
00:01:42,770 --> 00:01:47,000
when to combine two accounts
into a single user ID.


38
00:01:47,000 --> 00:01:50,375
You take your ID merge model,


39
00:01:50,375 --> 00:01:53,270
apply it to the
de-spammed user data.


40
00:01:53,270 --> 00:01:56,335
This gives you your
cleaned up user data.


41
00:01:56,335 --> 00:01:59,210
Then finally, based on
the clean user data,


42
00:01:59,210 --> 00:02:01,190
hopefully, some of this labels


43
00:02:01,190 --> 00:02:03,245
with whether someone's
looking for a job,


44
00:02:03,245 --> 00:02:06,709
you'll then have another
machine learning model,


45
00:02:06,709 --> 00:02:09,440
train on it to give
you a model to


46
00:02:09,440 --> 00:02:12,590
predict if a given user is
looking for a job or not.


47
00:02:12,590 --> 00:02:15,545
This is then used
to make predictions


48
00:02:15,545 --> 00:02:17,330
on other users or maybe


49
00:02:17,330 --> 00:02:19,850
across your whole
database of users.


50
00:02:19,850 --> 00:02:23,660
This level of complexity
of a data pipeline is


51
00:02:23,660 --> 00:02:27,320
not atypical in large
commercial systems.


52
00:02:27,320 --> 00:02:30,700
I've seen data pipelines
or data cascades that


53
00:02:30,700 --> 00:02:33,905
are even far more
complicated than this.


54
00:02:33,905 --> 00:02:35,840
One of the challenges of


55
00:02:35,840 --> 00:02:38,480
working with data
pipelines like this is,


56
00:02:38,480 --> 00:02:41,195
what if after running
this system for months,


57
00:02:41,195 --> 00:02:44,690
you discover that, oops,


58
00:02:44,690 --> 00:02:46,730
the IP address blacklists


59
00:02:46,730 --> 00:02:49,415
you're using has
some mistakes in it.


60
00:02:49,415 --> 00:02:54,050
In particular, what if you
discover that there was


61
00:02:54,050 --> 00:03:01,785
some IP addresses that were
incorrectly blacklisted.


62
00:03:01,785 --> 00:03:07,160
Maybe because the provider
from whom you had purchased


63
00:03:07,160 --> 00:03:10,820
a blacklisted IP
addresses found out that


64
00:03:10,820 --> 00:03:15,255
there were some IP addresses
that multiple users use,


65
00:03:15,255 --> 00:03:17,310
such as multiple users on


66
00:03:17,310 --> 00:03:19,715
a corporate campus or
university campus,


67
00:03:19,715 --> 00:03:22,955
sharing IP address
for security reasons.


68
00:03:22,955 --> 00:03:26,675
But the organization
creating the blacklist


69
00:03:26,675 --> 00:03:28,790
IP address thought it
was spammy because


70
00:03:28,790 --> 00:03:31,115
so many people shared
an IP address.


71
00:03:31,115 --> 00:03:32,810
This has happened before.


72
00:03:32,810 --> 00:03:38,345
The question is, having built
up this big complex system,


73
00:03:38,345 --> 00:03:41,690
if you were to update
your spam dataset,


74
00:03:41,690 --> 00:03:44,285
won't that change
your spam model,


75
00:03:44,285 --> 00:03:46,820
and therefore that,
and therefore that,


76
00:03:46,820 --> 00:03:49,465
and therefore that,
and therefore that.


77
00:03:49,465 --> 00:03:54,420
How do you go back
and fix this problem?


78
00:03:54,420 --> 00:03:56,510
Especially if each of


79
00:03:56,510 --> 00:03:59,389
these systems was developed
by different engineers,


80
00:03:59,389 --> 00:04:02,300
and you have files spread
across the laptops of


81
00:04:02,300 --> 00:04:05,755
your machine learning
engineering development team.


82
00:04:05,755 --> 00:04:09,640
To make sure your
system is maintainable,


83
00:04:09,640 --> 00:04:11,740
especially when a piece of


84
00:04:11,740 --> 00:04:15,100
data upstream ends up
needing to be changed,


85
00:04:15,100 --> 00:04:17,770
it can be very
helpful to keep track


86
00:04:17,770 --> 00:04:21,985
of data provenance
as well as lineage.


87
00:04:21,985 --> 00:04:26,775
Data provenance refers to
where the data came from.


88
00:04:26,775 --> 00:04:31,125
Who did you purchase the
spam IP address from?


89
00:04:31,125 --> 00:04:34,720
Lineage refers to the sequence of


90
00:04:34,720 --> 00:04:39,970
steps needed to get to
the end of the pipeline.


91
00:04:39,970 --> 00:04:41,635
At the very least,


92
00:04:41,635 --> 00:04:45,430
having extensive
documentation could


93
00:04:45,430 --> 00:04:48,805
help you reconstruct data
provenance and lineage,


94
00:04:48,805 --> 00:04:50,919
but to build robust,


95
00:04:50,919 --> 00:04:54,170
maintainable systems, not in
the proof of concept stage,


96
00:04:54,170 --> 00:04:55,610
but in the production stage.


97
00:04:55,610 --> 00:04:58,250
There are more
sophisticated tools to help


98
00:04:58,250 --> 00:05:00,980
you keep track of what
happens so that you


99
00:05:00,980 --> 00:05:03,110
can change part of the system and


100
00:05:03,110 --> 00:05:05,300
hopefully replicate the rest of


101
00:05:05,300 --> 00:05:07,280
the data pipeline without


102
00:05:07,280 --> 00:05:10,155
too much unnecessary complexity.


103
00:05:10,155 --> 00:05:12,140
To be honest, the tools for


104
00:05:12,140 --> 00:05:14,240
keeping track of
data provenance and


105
00:05:14,240 --> 00:05:16,100
lineage are still immature


106
00:05:16,100 --> 00:05:17,930
into this machine learning world.


107
00:05:17,930 --> 00:05:20,540
I find that extensive
documentation can


108
00:05:20,540 --> 00:05:22,850
help and some formal tools


109
00:05:22,850 --> 00:05:24,860
like TensorFlow
Transform can also


110
00:05:24,860 --> 00:05:28,435
help but solving this
type of problem is


111
00:05:28,435 --> 00:05:30,680
still not something that we are


112
00:05:30,680 --> 00:05:33,605
great at as a community yet.


113
00:05:33,605 --> 00:05:35,675
To make life easier,


114
00:05:35,675 --> 00:05:39,650
both for managing data
pipelines as well as


115
00:05:39,650 --> 00:05:41,450
for error analysis and


116
00:05:41,450 --> 00:05:43,850
driving machine
learning development,


117
00:05:43,850 --> 00:05:46,445
there's one tip I want to share,


118
00:05:46,445 --> 00:05:50,895
which is to make extensive
use of metadata.


119
00:05:50,895 --> 00:05:55,395
Metadata is data about data.


120
00:05:55,395 --> 00:05:59,359
For example, in manufacturing
visual inspection,


121
00:05:59,359 --> 00:06:04,235
the data would be the pictures
of phones and the labels


122
00:06:04,235 --> 00:06:06,800
but if you have
metadata that tells


123
00:06:06,800 --> 00:06:10,210
you what time was this
picture of a phone taken,


124
00:06:10,210 --> 00:06:12,540
what factory was
this picture from,


125
00:06:12,540 --> 00:06:14,055
what's the line number,


126
00:06:14,055 --> 00:06:16,115
what were the camera
settings such as


127
00:06:16,115 --> 00:06:19,459
camera exposure time
and camera aperture,


128
00:06:19,459 --> 00:06:22,129
what's the number of the
phone you're inspecting,


129
00:06:22,129 --> 00:06:25,360
what's the ID of the inspector
that provided this label.


130
00:06:25,360 --> 00:06:32,715
These are examples of data
about your dataset X and Y.


131
00:06:32,715 --> 00:06:36,230
This type of metadata can
turn out to be really


132
00:06:36,230 --> 00:06:39,925
useful because if you discover,


133
00:06:39,925 --> 00:06:42,115
during machine
learning development,


134
00:06:42,115 --> 00:06:44,689
that for some strange reason,


135
00:06:44,689 --> 00:06:48,560
line number 17 in factory


136
00:06:48,560 --> 00:06:52,610
2 generates images that


137
00:06:52,610 --> 00:06:54,995
produce a lot more
errors for some reason.


138
00:06:54,995 --> 00:06:57,860
Then this allows you
to go back to see what


139
00:06:57,860 --> 00:07:01,210
was funny about line
17 and factory 2.


140
00:07:01,210 --> 00:07:03,815
But if you had not stored


141
00:07:03,815 --> 00:07:07,295
the factory in line number
metadata in the first place,


142
00:07:07,295 --> 00:07:09,290
then it would have been
really difficult to


143
00:07:09,290 --> 00:07:12,295
discover this during
error analysis.


144
00:07:12,295 --> 00:07:15,530
I found many times
when I happened


145
00:07:15,530 --> 00:07:19,400
to maybe get lucky and
store the right metadata,


146
00:07:19,400 --> 00:07:23,885
only to discover a month
later that that metadata


147
00:07:23,885 --> 00:07:25,975
helped generate a key insight


148
00:07:25,975 --> 00:07:28,890
that helped the
project move forward.


149
00:07:28,890 --> 00:07:31,970
My tip is if you have
a framework or a set


150
00:07:31,970 --> 00:07:34,835
of MLOps tools for
storing metadata,


151
00:07:34,835 --> 00:07:36,425
that will definitely make life


152
00:07:36,425 --> 00:07:39,005
easier but even if you don't,


153
00:07:39,005 --> 00:07:42,050
just like you rarely regret
commenting your code,


154
00:07:42,050 --> 00:07:44,340
I think you will
really regret storing


155
00:07:44,340 --> 00:07:47,925
metadata that could then
turn out to be useful later.


156
00:07:47,925 --> 00:07:51,440
Just like if you don't comment
your code in a timely way,


157
00:07:51,440 --> 00:07:54,670
it's much harder to go
back to comment it later.


158
00:07:54,670 --> 00:07:56,000
In the same way,


159
00:07:56,000 --> 00:07:59,300
if you don't store the
metadata in a timely way,


160
00:07:59,300 --> 00:08:01,580
it can be much
harder to go back to


161
00:08:01,580 --> 00:08:04,355
recapture and organize that data.


162
00:08:04,355 --> 00:08:08,050
One more example for
speech recognition.


163
00:08:08,050 --> 00:08:10,654
If you have audio recorded


164
00:08:10,654 --> 00:08:13,970
from different brands
of smartphones,


165
00:08:13,970 --> 00:08:15,875
let's say that in advance,


166
00:08:15,875 --> 00:08:19,445
or if you have different
labelers labeling your speech,


167
00:08:19,445 --> 00:08:23,780
or if you use a voice
activity detection model,


168
00:08:23,780 --> 00:08:25,820
then just keep track of what was


169
00:08:25,820 --> 00:08:27,260
their version number of


170
00:08:27,260 --> 00:08:30,625
the voice activity detection
model that you use.


171
00:08:30,625 --> 00:08:34,970
All of these means that
in case for some reason,


172
00:08:34,970 --> 00:08:37,100
one version of the VAD,


173
00:08:37,100 --> 00:08:38,810
voice activity detection system


174
00:08:38,810 --> 00:08:40,670
results in much larger errors,


175
00:08:40,670 --> 00:08:44,030
this significantly increases
the odds of you discovering


176
00:08:44,030 --> 00:08:45,910
that and really use


177
00:08:45,910 --> 00:08:48,970
that to improve your learning
algorithm performance.


178
00:08:48,970 --> 00:08:51,890
To summarize, metadata can be


179
00:08:51,890 --> 00:08:55,220
very useful for error
analysis and spotting


180
00:08:55,220 --> 00:08:58,565
unexpected effects or tags
or categories of data


181
00:08:58,565 --> 00:09:02,300
that have some unusually
poor performance


182
00:09:02,300 --> 00:09:03,740
or something else,


183
00:09:03,740 --> 00:09:07,120
to suggest how to
improve your system.


184
00:09:07,120 --> 00:09:09,740
Of course, maybe
not surprisingly,


185
00:09:09,740 --> 00:09:13,580
this type of metadata is
also very useful for keeping


186
00:09:13,580 --> 00:09:17,440
track of where the data came
from or data provenance.


187
00:09:17,440 --> 00:09:20,150
The takeaway from this
video is that for


188
00:09:20,150 --> 00:09:22,010
large complex machine learning


189
00:09:22,010 --> 00:09:24,244
systems that you might
need to maintain,


190
00:09:24,244 --> 00:09:26,660
keeping track of
data provenance and


191
00:09:26,660 --> 00:09:29,675
lineage can make your
life much easier.


192
00:09:29,675 --> 00:09:32,750
As part of building
out the systems,


193
00:09:32,750 --> 00:09:35,690
consider keeping
track of metadata,


194
00:09:35,690 --> 00:09:38,975
which can help you with
tracking data provenance,


195
00:09:38,975 --> 00:09:41,870
but also error analysis.


196
00:09:41,870 --> 00:09:44,270
Before we wrap up this section,


197
00:09:44,270 --> 00:09:46,935
there's just one more tip
I hope to share with you,


198
00:09:46,935 --> 00:09:48,560
which is the importance of


199
00:09:48,560 --> 00:09:50,815
balanced train dev-test splits.


200
00:09:50,815 --> 00:09:53,720
Let's go on to the next video.
1
00:00:00,440 --> 00:00:03,330
Many of us are used to
taking a data set and


2
00:00:03,330 --> 00:00:06,830
randomly splitting it into train dev and
tests.


3
00:00:06,830 --> 00:00:11,248
It turns out when your data set is
small having balanced train dev and


4
00:00:11,248 --> 00:00:16,620
test set can significantly improve your
machine learning development process.


5
00:00:16,620 --> 00:00:17,640
Let's take a look.


6
00:00:17,640 --> 00:00:21,240
Let's use our manufacturing
visual inspection example.


7
00:00:21,240 --> 00:00:25,938
Say your training set has 100 images,
so pretty small data set and


8
00:00:25,938 --> 00:00:31,840
with 30 positive examples, so
30 defective phones and 70 non defective.


9
00:00:31,840 --> 00:00:38,960
If you were to use a train dev test split
of 60% of the data in the training set,


10
00:00:38,960 --> 00:00:44,933
20% in the dev or a validation set and
20% in the test set say.


11
00:00:44,933 --> 00:00:49,689
Then if you were to use
a random split just by chance


12
00:00:49,689 --> 00:00:54,334
is not inconceivable that
you may end up with 21


13
00:00:54,334 --> 00:01:00,540
positive examples in train,
2 in dev and 7 in tests.


14
00:01:00,540 --> 00:01:07,950
This would be quite likely
just by random chance.


15
00:01:07,950 --> 00:01:14,550
And this means the training
set is 35% positive,


16
00:01:14,550 --> 00:01:21,900
not that far from 30% positive
in the overall dataset,


17
00:01:21,900 --> 00:01:26,700
but your dev set is 10% positive and


18
00:01:26,700 --> 00:01:31,051
your test set is 35% positive.


19
00:01:31,051 --> 00:01:37,040
So 2 out of 20 is 10%, 7 out of 20 is 35%.


20
00:01:37,040 --> 00:01:42,232
And this makes your dev set
quite non representative


21
00:01:42,232 --> 00:01:46,244
because in your dev set you have only 2 or


22
00:01:46,244 --> 00:01:53,640
10% positive examples rather
than 30% positive examples.


23
00:01:53,640 --> 00:01:58,470
But when your data set is small than
all of your 20 dev set examples,


24
00:01:58,470 --> 00:02:03,830
it's just a higher chance of this,
slightly less representative split.


25
00:02:03,830 --> 00:02:09,163
So what we would really want is for
the training


26
00:02:09,163 --> 00:02:14,361
set to have exactly 18 positive examples,


27
00:02:14,361 --> 00:02:19,832
dev set to have exactly
6 positive examples and


28
00:02:19,832 --> 00:02:25,747
the test set to have exactly
6 positive examples.


29
00:02:25,747 --> 00:02:29,840
And this would be 30%, 30% 30%.


30
00:02:29,840 --> 00:02:36,061
And if you could get this type of split,
this would be called a balanced split.


31
00:02:37,440 --> 00:02:43,560
Where each of your train, dev and tests
has exactly 30% positive examples and this


32
00:02:43,560 --> 00:02:48,751
makes your data set more representative
of the true data distribution.


33
00:02:50,140 --> 00:02:55,055
There's no need to worry about this
effect when you have a large data set.


34
00:02:55,055 --> 00:02:57,252
If you have a very large data set,


35
00:02:57,252 --> 00:03:01,572
a random split will very likely
be representative, meaning that


36
00:03:01,572 --> 00:03:07,020
the percentage of positive examples will
be quite close to your overall data set.


37
00:03:07,020 --> 00:03:12,635
But when you have a small data set
with just 20 dev set examples and


38
00:03:12,635 --> 00:03:17,448
20 test set examples,
then explicitly making sure you


39
00:03:17,448 --> 00:03:21,258
have a balanced split can
make your dev set and


40
00:03:21,258 --> 00:03:27,700
test set more reliable measures of
your learning algorithms performance.


41
00:03:27,700 --> 00:03:32,254
This is one of those little techniques
that turns out to make a big difference to


42
00:03:32,254 --> 00:03:35,981
your performance when you're
working on a small data problem,


43
00:03:35,981 --> 00:03:40,740
but that you don't really need to worry
about if you have a very large data set.


44
00:03:40,740 --> 00:03:45,400
So when you have a smaller data set,
I hope you consider using a balanced


45
00:03:45,400 --> 00:03:50,340
train dev test split as well in terms
of how you set up your data set.


46
00:03:50,340 --> 00:03:54,756
So when you're working on a smaller data
problem I hope that using a balanced


47
00:03:54,756 --> 00:03:58,430
change test split will help you
with your learning algorithm.


48
00:03:58,430 --> 00:03:59,950
And so that's it.


49
00:03:59,950 --> 00:04:04,248
Congratulations on getting to
this point in this course.


50
00:04:04,248 --> 00:04:07,748
You've finished the data
section of videos and


51
00:04:07,748 --> 00:04:12,930
in the last two weeks you also
learned about modeling and deployment.


52
00:04:12,930 --> 00:04:18,660
There's just one last optional section
that you can watch if you want on scoping.


53
00:04:18,660 --> 00:04:22,520
I hope you come with me to watch
the optional scoping videos as well.


54
00:04:22,520 --> 00:04:26,050
We'll talk about how to
select a project to work on.


55
00:04:26,050 --> 00:04:30,840
But either way congrats on finishing
all the required videos of this course.


56
00:04:30,840 --> 00:04:33,831
I hope you learned a lot and
that these ideas will be useful for


57
00:04:33,831 --> 00:04:35,661
all the machine learning projects.
1
00:00:00,440 --> 00:00:04,098
Picking the right project to work
on is one of the most rare and


2
00:00:04,098 --> 00:00:05,990
valuable schools in Ai today.


3
00:00:05,990 --> 00:00:11,356
In the next few videos, I'd like to share
with you some best practices for scoping,


4
00:00:11,356 --> 00:00:16,440
picking what project to work on and also
planning out the scope of the project.


5
00:00:16,440 --> 00:00:21,281
I remember when I was younger, I tended
to just jump into the first project


6
00:00:21,281 --> 00:00:26,440
that I got excited about and sometimes
I was lucky and it worked out okay.


7
00:00:26,440 --> 00:00:29,627
Now that I've had a little
bit more experience,


8
00:00:29,627 --> 00:00:34,366
I find that if you buy yourself or
your team is going to spend a lot of time,


9
00:00:34,366 --> 00:00:39,264
weeks or months or even longer work on
the project is well worth your while to


10
00:00:39,264 --> 00:00:44,086
think through a few options and try to
select the most promising project to


11
00:00:44,086 --> 00:00:47,440
work on before putting so
much effort into it.


12
00:00:47,440 --> 00:00:50,495
So that let's dive into scoping.


13
00:00:50,495 --> 00:00:57,940
Let's use the example of an e commerce
retailer looking to increase sales.


14
00:00:57,940 --> 00:01:02,783
If you were to sit down and brainstorm
what and Nikon company could do,


15
00:01:02,783 --> 00:01:08,366
you might come up with many ideas, such
as maybe a better product recommendation


16
00:01:08,366 --> 00:01:13,046
system or better search so
people can find what they're looking for


17
00:01:13,046 --> 00:01:16,903
or you may find that the catalog
data is missing fields or


18
00:01:16,903 --> 00:01:21,861
is incomplete and this affects
search your recommendations results.


19
00:01:21,861 --> 00:01:25,870
So you might see the project to improve
the quality of the catalog data.


20
00:01:25,870 --> 00:01:30,903
Or you may help them with inventory
management, such as deciding how many


21
00:01:30,903 --> 00:01:36,340
shirts to buy, where to ship the shirts or
what price optimization.


22
00:01:36,340 --> 00:01:41,268
With a quick brainstorming session,
you may be able to come up with dozens of


23
00:01:41,268 --> 00:01:44,351
ideas for
how to help this e commerce retailer.


24
00:01:45,840 --> 00:01:50,720
The questions that we like to answer,
the scoping process are what project or


25
00:01:50,720 --> 00:01:52,532
projects should we work on.


26
00:01:52,532 --> 00:01:57,342
Whether the metrics for success and
whether the resources such as data time,


27
00:01:57,342 --> 00:02:00,340
people needed to execute this project.


28
00:02:00,340 --> 00:02:05,151
What I've seen in a lot of businesses is
that of all the ideas you can work on.


29
00:02:05,151 --> 00:02:07,531
Some are going to be much
more valuable than others.


30
00:02:07,531 --> 00:02:12,390
Maybe two times or five times or 10 times
more valuable than a different idea.


31
00:02:12,390 --> 00:02:17,268
And being able to pick the most valuable
project will significantly increase


32
00:02:17,268 --> 00:02:19,840
the impact of your work.


33
00:02:19,840 --> 00:02:23,205
Machine learning is a general purpose to
there are a lot of things we could do with


34
00:02:23,205 --> 00:02:24,080
machine learning.


35
00:02:24,080 --> 00:02:27,400
How do we take valuable
projects to work on?


36
00:02:27,400 --> 00:02:31,151
Let's dive more deeply into
the scoping process in the next video.
1
00:00:00,440 --> 00:00:03,706
I'd like to share with you a process for
scoping projects,


2
00:00:03,706 --> 00:00:07,060
that hope will be valuable for
how you decide what to work on.


3
00:00:07,060 --> 00:00:11,696
When I'm speaking with a company for
the first time about their AI projects,


4
00:00:11,696 --> 00:00:14,000
this is the process that I use as well.


5
00:00:14,000 --> 00:00:17,850
Let's dive in,
when brainstorming prices work on.


6
00:00:17,850 --> 00:00:23,480
The first thing I do is usually get
together with a business or private owner.


7
00:00:23,480 --> 00:00:27,620
Often not an Ai person, but
someone that understands the business and


8
00:00:27,620 --> 00:00:31,040
application and to brainstorm with them.


9
00:00:31,040 --> 00:00:34,240
What are their business or
application problems?


10
00:00:34,240 --> 00:00:40,440
And at this stage I'm trying to identify
a business problem, not an Ai problem.


11
00:00:40,440 --> 00:00:43,252
So if I'm speaking with
Nikon retail business,


12
00:00:43,252 --> 00:00:45,733
like the example from the previous video.


13
00:00:45,733 --> 00:00:50,318
I might ask, what are the top few things,
top three things you wish were working


14
00:00:50,318 --> 00:00:53,551
better, and
maybe they'll share business problems.


15
00:00:53,551 --> 00:00:58,507
Like they like to increase conversions
number of people that go to the website


16
00:00:58,507 --> 00:01:01,270
and convert to a sale or reduce inventory.


17
00:01:01,270 --> 00:01:05,531
So you don't need as much stuff
sitting around in the warehouse, or


18
00:01:05,531 --> 00:01:09,067
increased margin,
increase the profit per item sold.


19
00:01:09,067 --> 00:01:13,690
At this point in the process,
I'm not trying to identify an AI problem.


20
00:01:13,690 --> 00:01:18,480
In fact, I often tell my partners I don't
want to hear about your AI problems.


21
00:01:18,480 --> 00:01:21,276
I want to hear about your
business problems and


22
00:01:21,276 --> 00:01:25,228
then it's my job to work with you
to see if there is an Ai solution.


23
00:01:25,228 --> 00:01:27,560
And sometimes there isn't and
that's fine too.


24
00:01:27,560 --> 00:01:31,960
Feel free to use the exact same words as
well when brainstorming projects with your


25
00:01:31,960 --> 00:01:32,927
non AI partners.


26
00:01:32,927 --> 00:01:37,396
If you want having identified
a few business problems like


27
00:01:37,396 --> 00:01:39,959
the three examples on the right.


28
00:01:39,959 --> 00:01:46,343
Only then do I see or start to brainstorm
if there are possible AI solutions,


29
00:01:46,343 --> 00:01:50,860
not all problems can be solved by AI and
that's okay.


30
00:01:50,860 --> 00:01:53,707
But hopefully we'll come
up with some ideas for


31
00:01:53,707 --> 00:01:59,440
using machine learning algorithms,
to address some of the business problems.


32
00:01:59,440 --> 00:02:01,360
I find it is hopeful for


33
00:02:01,360 --> 00:02:06,921
this process to separate out
the identification of the problem,


34
00:02:06,921 --> 00:02:11,691
from the identification of
the solution as engineers.


35
00:02:11,691 --> 00:02:18,156
We are pretty good at coming up with
solutions, but having a clear articulation


36
00:02:18,156 --> 00:02:24,540
of what is the problem first often
helps us come up with better solutions.


37
00:02:24,540 --> 00:02:28,005
This type of separation between
problem and solution is


38
00:02:28,005 --> 00:02:33,140
something you might hear about in
the writings on design thinking as well.


39
00:02:33,140 --> 00:02:37,631
After brainstorming a variety
of different solutions,


40
00:02:37,631 --> 00:02:44,010
I would then assess the feasibility and
the value of these different solutions.


41
00:02:44,010 --> 00:02:49,440
Sometimes you hear me use the word
diligence, to refer to this phrase.


42
00:02:49,440 --> 00:02:53,425
Diligence is a term that actually
comes from the legal field, but


43
00:02:53,425 --> 00:02:56,760
it basically means
double-checking if an AI solution


44
00:02:56,760 --> 00:02:59,680
really is technically feasible and
valuable.


45
00:02:59,680 --> 00:03:04,975
Double-checking something that you're
hoping, it's true really is true,


46
00:03:04,975 --> 00:03:08,937
after validating technical feasibility and
value or ROI.


47
00:03:08,937 --> 00:03:13,691
Return on investment if you can project
if it still looks promising right,


48
00:03:13,691 --> 00:03:15,587
if it still looks promising.


49
00:03:15,587 --> 00:03:22,840
We then flesh out the milestones for the
project and finally budget for resources.


50
00:03:22,840 --> 00:03:28,597
Let's take a deeper look at this
process of identifying problems,


51
00:03:28,597 --> 00:03:33,951
and solutions and
we'll use these three examples from Nikon.


52
00:03:35,440 --> 00:03:40,597
So the first one increased conversion,
if the business wants


53
00:03:40,597 --> 00:03:47,040
to increase conversions, you may
have different ideas are doing that.


54
00:03:47,040 --> 00:03:53,945
For example, you may want to improve
the quality of the website search results.


55
00:03:53,945 --> 00:03:57,175
So people find more relevant
products when they search.


56
00:03:57,175 --> 00:04:01,995
Or you might decide to try to offer
a better product recommendations based on


57
00:04:01,995 --> 00:04:04,340
their purchase history.


58
00:04:04,340 --> 00:04:11,978
It is quite common that one problem may
lead to multiple ideas for solutions.


59
00:04:11,978 --> 00:04:15,652
And you may be able to bring
some other ideas as well,


60
00:04:15,652 --> 00:04:20,350
such as maybe a redesign of how
products are displayed on the page.


61
00:04:20,350 --> 00:04:25,007
Or you may find interesting ways to
surface the most relevant product reviews,


62
00:04:25,007 --> 00:04:29,130
to help users understand the product and
does hopefully purchase it.


63
00:04:29,130 --> 00:04:32,350
So there can be many ways
to increase conversions.


64
00:04:32,350 --> 00:04:36,740
Take the next problem from the previous
slide of reducing inventory.


65
00:04:36,740 --> 00:04:41,283
Maybe, you can imagine
a demand prediction project to


66
00:04:41,283 --> 00:04:46,034
better estimate how many
people buy something from you.


67
00:04:46,034 --> 00:04:48,898
So you don't purchase too many or
too few, and


68
00:04:48,898 --> 00:04:52,640
have more accurate inventory
in your warehouses.


69
00:04:52,640 --> 00:04:57,369
Or you may decide to come up with
a marketing campaign to drive sales for


70
00:04:57,369 --> 00:05:01,540
specifically the products
that you bought too many of.


71
00:05:01,540 --> 00:05:05,801
So as to steer more purchases of
stuff sitting in your warehouse.


72
00:05:05,801 --> 00:05:12,091
And that could also reduce inventory,
and there could be many other ideas for


73
00:05:12,091 --> 00:05:16,372
solutions or for
the problem of increasing margin.


74
00:05:16,372 --> 00:05:22,251
You may come up with some ways to use
machine learning to optimize what to sell.


75
00:05:22,251 --> 00:05:25,540
What is worth selling and
what is not worth selling.


76
00:05:25,540 --> 00:05:30,613
And Nikon retail, sometimes this is called
merchandising, just deciding what to sell.


77
00:05:30,613 --> 00:05:34,427
Or you can recommend bundles
where if someone buys a camera.


78
00:05:34,427 --> 00:05:38,091
Maybe you can recommend to them
a protective camera case and


79
00:05:38,091 --> 00:05:40,661
these bundles can also increase margin.


80
00:05:41,840 --> 00:05:46,091
The problem identification is
a step of thinking through


81
00:05:46,091 --> 00:05:49,109
whether the things you want to achieve.


82
00:05:49,109 --> 00:05:53,624
And solution identification is
a process of thinking through


83
00:05:53,624 --> 00:05:56,840
how to achieve those objectives.


84
00:05:56,840 --> 00:06:00,185
One thing I still see too many
teams do today is jump into


85
00:06:00,185 --> 00:06:04,203
the first project that they're
excited about in my experience.


86
00:06:04,203 --> 00:06:07,712
If you have deep domain knowledge
about an application or


87
00:06:07,712 --> 00:06:12,256
industry may be the first thing your
gut gets excited about could be okay.


88
00:06:12,256 --> 00:06:16,342
But even then I find it worthwhile
to first engage in divergent


89
00:06:16,342 --> 00:06:20,045
thinking where you brainstorm
a lot of possibilities.


90
00:06:20,045 --> 00:06:24,333
To be followed by conversion thinking
where you then narrow it down to one or


91
00:06:24,333 --> 00:06:28,140
a small handful of the most
promising projects to focus on.


92
00:06:28,140 --> 00:06:32,910
One thing I hope you might avoid is
working really hard on the project and


93
00:06:32,910 --> 00:06:37,040
creating a certain amount of monetary or
social value.


94
00:06:37,040 --> 00:06:42,177
If for the same amount of work, there's
a different project that could have


95
00:06:42,177 --> 00:06:47,740
created 10 times more, monitoring or
social or other positive types of value.


96
00:06:47,740 --> 00:06:51,861
And I think this type of scoping
process will help you to do that.
1
00:00:00,000 --> 00:00:01,500
In the last video,


2
00:00:01,500 --> 00:00:04,140
you heard about the
step of assessing


3
00:00:04,140 --> 00:00:08,205
a project for technical
feasibility and for value.


4
00:00:08,205 --> 00:00:10,260
Let's take a deeper
look at how you can


5
00:00:10,260 --> 00:00:12,930
carry out this diligence step to


6
00:00:12,930 --> 00:00:15,299
figure out if a project
really is feasible


7
00:00:15,299 --> 00:00:17,925
and also how valuable
it really is.


8
00:00:17,925 --> 00:00:19,865
Let's start with feasibility.


9
00:00:19,865 --> 00:00:23,085
Is this project idea
technically feasible?


10
00:00:23,085 --> 00:00:25,875
Before you've started on the
Machine Learning Project,


11
00:00:25,875 --> 00:00:29,750
how do you know if this
thing can even be built?


12
00:00:29,750 --> 00:00:33,880
One way to get a quick
sense of feasibility is to


13
00:00:33,880 --> 00:00:37,750
use an external benchmark
such as the research,


14
00:00:37,750 --> 00:00:40,540
literature or other
forms of publications,


15
00:00:40,540 --> 00:00:42,970
or if different company


16
00:00:42,970 --> 00:00:45,100
or even a competitor
has managed to


17
00:00:45,100 --> 00:00:49,860
build a certain type of
online search system before,


18
00:00:49,860 --> 00:00:53,130
or recommendation system
or, inventory management.


19
00:00:53,130 --> 00:00:55,614
But if there's some
external benchmark


20
00:00:55,614 --> 00:00:57,250
that might help give you


21
00:00:57,250 --> 00:01:02,120
a sense that this project
may be technically feasible,


22
00:01:02,120 --> 00:01:05,150
because someone else has managed
to do something similar.


23
00:01:05,150 --> 00:01:07,210
Either to complement this type of


24
00:01:07,210 --> 00:01:08,650
external benchmark or in


25
00:01:08,650 --> 00:01:11,244
the absence of this other
external benchmark,


26
00:01:11,244 --> 00:01:14,755
here are some other ways
to assess feasibility.


27
00:01:14,755 --> 00:01:15,970
I'm going to build a two by


28
00:01:15,970 --> 00:01:19,180
two matrix that looks
at different cases,


29
00:01:19,180 --> 00:01:20,470
depending on whether your


30
00:01:20,470 --> 00:01:23,200
problem has
unstructured data like


31
00:01:23,200 --> 00:01:25,150
speech images or


32
00:01:25,150 --> 00:01:28,180
structured data like
transaction records.


33
00:01:28,180 --> 00:01:29,710
On the other axis,


34
00:01:29,710 --> 00:01:32,820
I'm going to put new
versus existing.


35
00:01:32,820 --> 00:01:34,180
Whereby new I mean,


36
00:01:34,180 --> 00:01:35,890
you're trying to
build a system to


37
00:01:35,890 --> 00:01:37,840
do a task for the first time,


38
00:01:37,840 --> 00:01:40,240
such as if you've
never done demand


39
00:01:40,240 --> 00:01:43,285
forecasting before and you're
thinking of building one,


40
00:01:43,285 --> 00:01:46,510
whereas existing refers to


41
00:01:46,510 --> 00:01:50,380
if you already have
some existing system,


42
00:01:50,380 --> 00:01:52,885
maybe a machine learning
one, maybe not,


43
00:01:52,885 --> 00:01:56,230
that is carrying out this
task and you're thinking


44
00:01:56,230 --> 00:02:00,140
of scoping out an improvement
to an existing system.


45
00:02:00,140 --> 00:02:01,960
New means you are delivering


46
00:02:01,960 --> 00:02:04,150
a brand new capability
and the existing


47
00:02:04,150 --> 00:02:06,405
means you're scoping
out the project


48
00:02:06,405 --> 00:02:09,190
to improve on an
existing capability.


49
00:02:09,190 --> 00:02:11,905
In the upper left hand quadrant,


50
00:02:11,905 --> 00:02:15,655
to see if a project is
technically feasible,


51
00:02:15,655 --> 00:02:17,770
I find Human Level Performance,


52
00:02:17,770 --> 00:02:20,440
HLP to be very
useful and give you


53
00:02:20,440 --> 00:02:25,210
an initial sense of whether
a project is doable.


54
00:02:25,210 --> 00:02:28,870
When evaluating HLP, I
would give a human to


55
00:02:28,870 --> 00:02:30,690
same data as would be fed to


56
00:02:30,690 --> 00:02:33,295
a learning algorithm
and just ask,


57
00:02:33,295 --> 00:02:35,410
can a human given the same data,


58
00:02:35,410 --> 00:02:37,420
perform the tasks such as can the


59
00:02:37,420 --> 00:02:40,450
human given a picture of
a scratch smartphone,


60
00:02:40,450 --> 00:02:43,940
perform the task of detecting
scratches reliably?


61
00:02:43,940 --> 00:02:45,700
If a human can do it,


62
00:02:45,700 --> 00:02:48,370
then that significantly
increases the hope


63
00:02:48,370 --> 00:02:51,280
they can also get the
learning algorithm to do it.


64
00:02:51,280 --> 00:02:53,680
For existing projects, I would


65
00:02:53,680 --> 00:02:56,635
use HLP as a reference as well.


66
00:02:56,635 --> 00:03:01,090
Where if you have


67
00:03:01,090 --> 00:03:03,490
a visual [inaudible]
inspection system


68
00:03:03,490 --> 00:03:05,110
and you're hoping
to improve it to


69
00:03:05,110 --> 00:03:06,600
a certain level of performance.


70
00:03:06,600 --> 00:03:09,865
If humans can achieve the
level you're hoping to get to,


71
00:03:09,865 --> 00:03:11,155
then that might give you


72
00:03:11,155 --> 00:03:13,900
more hope that it is
technically feasible.


73
00:03:13,900 --> 00:03:16,300
Whereas if you're
hoping to increase


74
00:03:16,300 --> 00:03:19,210
performance well beyond
human level performance,


75
00:03:19,210 --> 00:03:21,640
then that suggests the project


76
00:03:21,640 --> 00:03:24,530
might be harder or
may not be possible.


77
00:03:24,530 --> 00:03:26,530
In addition to HLP,


78
00:03:26,530 --> 00:03:31,630
I often also use the history of


79
00:03:31,630 --> 00:03:36,705
the project as a predictor
for future progress.


80
00:03:36,705 --> 00:03:39,820
I will say more
about both HLP and


81
00:03:39,820 --> 00:03:43,780
history of project in
the next few slides,


82
00:03:43,780 --> 00:03:48,280
but the previous rate of
progress on the project can be


83
00:03:48,280 --> 00:03:50,890
a reasonable predictor for


84
00:03:50,890 --> 00:03:53,330
the future rate of
progress on a project.


85
00:03:53,330 --> 00:03:56,150
You see more of this
later in this video.


86
00:03:56,150 --> 00:03:58,475
Moving over to the right column.


87
00:03:58,475 --> 00:03:59,710
If you're working on


88
00:03:59,710 --> 00:04:03,080
a brand new project
with structure data,


89
00:04:03,080 --> 00:04:06,460
the question I would ask is,


90
00:04:06,460 --> 00:04:13,610
are predictive
features available?


91
00:04:13,610 --> 00:04:16,980
Do you have reason to think
that the data you have,


92
00:04:16,980 --> 00:04:18,915
the inputs X are


93
00:04:18,915 --> 00:04:21,030
strongly predictive
or sufficiently


94
00:04:21,030 --> 00:04:24,075
predictive of the
target outputs Y?


95
00:04:24,075 --> 00:04:27,180
In this box on the lower right,


96
00:04:27,180 --> 00:04:29,190
for structured data problem,


97
00:04:29,190 --> 00:04:32,080
if you're trying to improve
an existing system,


98
00:04:32,080 --> 00:04:35,580
one thing that will
help a lot is if you


99
00:04:35,580 --> 00:04:38,865
can identify new
predictive features.


100
00:04:38,865 --> 00:04:40,890
Are there features that you


101
00:04:40,890 --> 00:04:43,050
aren't yet using but
you can identify that


102
00:04:43,050 --> 00:04:48,535
could really help predict
Y and also by looking at?


103
00:04:48,535 --> 00:04:52,470
The history, of the project.


104
00:04:52,470 --> 00:04:56,925
On this slide, you heard
about three concepts.


105
00:04:56,925 --> 00:04:59,430
Human-level performance,
the question


106
00:04:59,430 --> 00:05:01,544
of whether predictive
features are available,


107
00:05:01,544 --> 00:05:03,555
and also the history
of a project.


108
00:05:03,555 --> 00:05:06,660
Let's take a deeper look
at these three concepts.


109
00:05:06,660 --> 00:05:13,170
Let's start with using HLP
on unstructured data images.


110
00:05:13,170 --> 00:05:17,325
I use HLP to benchmark
what might be durable for


111
00:05:17,325 --> 00:05:19,980
unstructured data
because people are


112
00:05:19,980 --> 00:05:23,505
very good on
unstructured data tasks.


113
00:05:23,505 --> 00:05:27,990
The key criteria for assessing
project feasibility is,


114
00:05:27,990 --> 00:05:29,790
can a human given


115
00:05:29,790 --> 00:05:32,340
the exact same data as would be


116
00:05:32,340 --> 00:05:35,265
given to a learning
algorithm, perform the task?


117
00:05:35,265 --> 00:05:36,840
Let's look at an example.


118
00:05:36,840 --> 00:05:38,670
Let's say you're building
a self-driving car,


119
00:05:38,670 --> 00:05:41,835
and you want an
algorithm to classify


120
00:05:41,835 --> 00:05:43,890
whether a traffic
light is currently


121
00:05:43,890 --> 00:05:46,260
red, yellow, or green.


122
00:05:46,260 --> 00:05:49,170
I will take pictures from


123
00:05:49,170 --> 00:05:50,880
your self-driving car and


124
00:05:50,880 --> 00:05:52,935
ask a person to look
at an image like this,


125
00:05:52,935 --> 00:05:56,055
and see if the person
looking only at the image


126
00:05:56,055 --> 00:05:59,550
can tell which lamp is
illuminated and in this example,


127
00:05:59,550 --> 00:06:01,440
it's pretty clear it's green.


128
00:06:01,440 --> 00:06:04,335
But if you find that you also
have pictures like these,


129
00:06:04,335 --> 00:06:05,880
then I can't tell


130
00:06:05,880 --> 00:06:08,370
which lamp is illuminated
in this example.


131
00:06:08,370 --> 00:06:10,050
This is why it's important for


132
00:06:10,050 --> 00:06:12,240
this HLP benchmark
to make sure that


133
00:06:12,240 --> 00:06:14,940
human is given only the same data


134
00:06:14,940 --> 00:06:16,695
as your learning algorithm.


135
00:06:16,695 --> 00:06:19,410
It turns out maybe
a human sitting in


136
00:06:19,410 --> 00:06:22,275
the car and seeing the traffic
light with their own eye,


137
00:06:22,275 --> 00:06:24,060
could have told
you which lamp was


138
00:06:24,060 --> 00:06:26,610
illuminated in this
example on the right,


139
00:06:26,610 --> 00:06:29,460
but that's because
the human eye has


140
00:06:29,460 --> 00:06:33,510
superior contrast to
most digital cameras.


141
00:06:33,510 --> 00:06:36,720
But the useful test
is not whether


142
00:06:36,720 --> 00:06:41,865
the human eye can recognize
which lamp is illuminated,


143
00:06:41,865 --> 00:06:45,870
the useful test is
if the person was


144
00:06:45,870 --> 00:06:47,970
sitting back in the
office and they can only


145
00:06:47,970 --> 00:06:50,175
see the image from the camera,


146
00:06:50,175 --> 00:06:51,840
can they still do the task?


147
00:06:51,840 --> 00:06:55,020
That gives you a better
read on feasibility.


148
00:06:55,020 --> 00:06:57,540
Specifically, it helps you make


149
00:06:57,540 --> 00:07:00,030
a better guess at whether
a learning algorithm,


150
00:07:00,030 --> 00:07:02,295
which will only have
access to this image,


151
00:07:02,295 --> 00:07:04,320
can also accurately detect


152
00:07:04,320 --> 00:07:07,250
which lamp in a traffic
light is illuminated.


153
00:07:07,250 --> 00:07:09,095
Making sure that a human sees


154
00:07:09,095 --> 00:07:11,120
only the same data as
a learning algorithm


155
00:07:11,120 --> 00:07:13,130
will see is really important.


156
00:07:13,130 --> 00:07:15,290
I've seen a lot of
projects where for


157
00:07:15,290 --> 00:07:16,640
a long time a team was


158
00:07:16,640 --> 00:07:19,030
working on a computer
vision system, say,


159
00:07:19,030 --> 00:07:22,350
and they thought they could
do it because a human


160
00:07:22,350 --> 00:07:24,600
physically inspecting
the cell phone


161
00:07:24,600 --> 00:07:26,670
or something could
detect the defect.


162
00:07:26,670 --> 00:07:29,070
But it took a long
time to realize that


163
00:07:29,070 --> 00:07:31,260
even a human looking
only at the image,


164
00:07:31,260 --> 00:07:33,180
couldn't figure out
what was going on.


165
00:07:33,180 --> 00:07:35,100
(If you can realize that earlier,)
then you can figure much


166
00:07:35,100 --> 00:07:37,680
earlier that with the
current camera set up,


167
00:07:37,680 --> 00:07:39,540
it just wasn't feasible.


168
00:07:39,540 --> 00:07:43,635
The more efficient thing to
do would have been to invest


169
00:07:43,635 --> 00:07:45,810
early on in a better camera


170
00:07:45,810 --> 00:07:47,985
or better lighting
setup or something,


171
00:07:47,985 --> 00:07:50,685
rather than keep working on
a machine learning algorithm


172
00:07:50,685 --> 00:07:53,340
on the problem that I
think just wasn't durable,


173
00:07:53,340 --> 00:07:55,590
with the imaging setup
available at a time.


174
00:07:55,590 --> 00:07:57,615
Next, for structured
data problems,


175
00:07:57,615 --> 00:07:59,730
one of the key criteria


176
00:07:59,730 --> 00:08:02,265
to assess for technical
feasibility is,


177
00:08:02,265 --> 00:08:05,580
do we have input features
X that seem to be


178
00:08:05,580 --> 00:08:08,910
predictive whenever we're
trying to predict Y.


179
00:08:08,910 --> 00:08:10,710
Let's look at a few examples.


180
00:08:10,710 --> 00:08:13,980
In a Ecom example,


181
00:08:13,980 --> 00:08:17,430
if you have features
that show what are


182
00:08:17,430 --> 00:08:18,750
the past purchases of


183
00:08:18,750 --> 00:08:22,245
a user and you like to
predict future purchases.


184
00:08:22,245 --> 00:08:25,995
That seems possible to
me because most people's


185
00:08:25,995 --> 00:08:30,180
previous purchases are
predictive of future purchases.


186
00:08:30,180 --> 00:08:32,625
If you have past purchase data,


187
00:08:32,625 --> 00:08:35,640
you do have features
that seem predictive of


188
00:08:35,640 --> 00:08:39,120
future purchases and this
project might be worth a try.


189
00:08:39,120 --> 00:08:43,665
Or if you work with
a physical store,


190
00:08:43,665 --> 00:08:46,080
given data on whether if


191
00:08:46,080 --> 00:08:48,720
you want to predict
shopping mall foot traffic.


192
00:08:48,720 --> 00:08:50,640
How many people will
go to the mall?


193
00:08:50,640 --> 00:08:52,245
While we know that,


194
00:08:52,245 --> 00:08:53,835
when it rains a lot,


195
00:08:53,835 --> 00:08:55,740
fewer people leave their house,


196
00:08:55,740 --> 00:08:57,690
so weather is predictive of


197
00:08:57,690 --> 00:09:00,165
foot traffic in
shopping malls and so,


198
00:09:00,165 --> 00:09:03,030
I will say you do have
predictive features.


199
00:09:03,030 --> 00:09:05,100
Let's look at some more examples.


200
00:09:05,100 --> 00:09:07,845
Given DNA of an individual,


201
00:09:07,845 --> 00:09:09,450
let's try to predict if


202
00:09:09,450 --> 00:09:12,360
this individual will
have heart disease.


203
00:09:12,360 --> 00:09:15,120
This one, I don't know,


204
00:09:15,120 --> 00:09:18,480
the mapping from your DNA
to whether or not you


205
00:09:18,480 --> 00:09:21,675
get heart disease is
a very noisy mapping.


206
00:09:21,675 --> 00:09:23,760
In biology, this is referred


207
00:09:23,760 --> 00:09:25,934
to the genotype and phenotype,


208
00:09:25,934 --> 00:09:28,590
but the mapping from
genotype to phenotype or


209
00:09:28,590 --> 00:09:30,690
your genetics to your
health condition


210
00:09:30,690 --> 00:09:33,390
is a very noisy mapping.


211
00:09:33,390 --> 00:09:38,055
I would have mixed feelings
about this project,


212
00:09:38,055 --> 00:09:39,315
because it turns out


213
00:09:39,315 --> 00:09:42,615
your genetic sequence
is only slightly,


214
00:09:42,615 --> 00:09:46,425
maybe mildly predictive of
whether you get heart disease.


215
00:09:46,425 --> 00:09:49,065
I'm going to put a
question mark there.


216
00:09:49,065 --> 00:09:51,805
Given social media chatter,


217
00:09:51,805 --> 00:09:54,960
can you predict demand
for a clothing style?


218
00:09:54,960 --> 00:09:56,900
This is another iffy one.


219
00:09:56,900 --> 00:09:58,570
I think you may be
able to predict


220
00:09:58,570 --> 00:10:01,415
demand for clothing
style right now,


221
00:10:01,415 --> 00:10:04,060
but given social media chatter,


222
00:10:04,060 --> 00:10:06,400
can you predict what
will be the hot,


223
00:10:06,400 --> 00:10:08,395
fashionable trend
six months from now?


224
00:10:08,395 --> 00:10:11,235
That actually seems
very difficult.


225
00:10:11,235 --> 00:10:15,665
One of the ways I've seen
AI projects go poorly,


226
00:10:15,665 --> 00:10:17,240
is if there's an idea


227
00:10:17,240 --> 00:10:19,940
like let's use social
media to figure out what


228
00:10:19,940 --> 00:10:22,515
people are chatting
about in fashion


229
00:10:22,515 --> 00:10:24,080
and then we'll
manufacture the clothing


230
00:10:24,080 --> 00:10:25,390
and sell it in six months.


231
00:10:25,390 --> 00:10:29,555
Sometimes the data just
is not that predictive,


232
00:10:29,555 --> 00:10:32,390
and you end up with a
learning algorithm that


233
00:10:32,390 --> 00:10:35,895
does barely better
than random guessing.


234
00:10:35,895 --> 00:10:38,090
That's why looking at whether you


235
00:10:38,090 --> 00:10:40,534
have features that you
believe are predictive,


236
00:10:40,534 --> 00:10:42,350
is an important step of


237
00:10:42,350 --> 00:10:44,150
diligence for assessing


238
00:10:44,150 --> 00:10:46,305
technical feasibility
of a project.


239
00:10:46,305 --> 00:10:49,400
One last example that
may be even clearer,


240
00:10:49,400 --> 00:10:51,540
which is given a history of


241
00:10:51,540 --> 00:10:54,075
a particular stock
market shares price.


242
00:10:54,075 --> 00:10:56,780
Let's try to predict the
future price of that stock.


243
00:10:56,780 --> 00:10:59,180
All of the evidence I've seen is


244
00:10:59,180 --> 00:11:01,730
that this is not
doable unless you


245
00:11:01,730 --> 00:11:03,860
get some other clever set of


246
00:11:03,860 --> 00:11:06,530
features looking at a single
shares historical price,


247
00:11:06,530 --> 00:11:07,730
to predict the future price of


248
00:11:07,730 --> 00:11:10,935
that stock is
exceedingly difficult.


249
00:11:10,935 --> 00:11:14,270
I would say if those are
the only futures you have,


250
00:11:14,270 --> 00:11:16,275
those features are
not predictive of


251
00:11:16,275 --> 00:11:17,840
the future price of that stock


252
00:11:17,840 --> 00:11:19,685
based on the evidence I've seen.


253
00:11:19,685 --> 00:11:21,830
Even leaving aside
the question of how


254
00:11:21,830 --> 00:11:23,985
much predicting share
prices are trading,


255
00:11:23,985 --> 00:11:25,794
if there's any social value,


256
00:11:25,794 --> 00:11:28,100
I have some questions
about that sometimes.


257
00:11:28,100 --> 00:11:29,550
I think this project is


258
00:11:29,550 --> 00:11:32,150
also just not
technically feasible.


259
00:11:32,150 --> 00:11:35,595
Finally, on this diagram,


260
00:11:35,595 --> 00:11:39,110
one last criteria I
mentioned a couple of


261
00:11:39,110 --> 00:11:42,410
times is the history
of a project.


262
00:11:42,410 --> 00:11:43,965
Let's take a look at that.


263
00:11:43,965 --> 00:11:45,170
When I've worked on


264
00:11:45,170 --> 00:11:49,305
a machine learning
application for many months,


265
00:11:49,305 --> 00:11:53,415
I found that the rates of
previous improvements can be


266
00:11:53,415 --> 00:11:55,430
maybe a surprisingly
good predictor for


267
00:11:55,430 --> 00:11:57,855
the rate of future improvement.


268
00:11:57,855 --> 00:12:00,425
Here's a simple
model you can use.


269
00:12:00,425 --> 00:12:03,559
Let's see a speech
recognition as the example,


270
00:12:03,559 --> 00:12:09,560
and let's say that this is
human level performance.


271
00:12:09,560 --> 00:12:11,915
I'm going to use human
level performance


272
00:12:11,915 --> 00:12:13,400
as our estimate for


273
00:12:13,400 --> 00:12:15,560
B0 or the irreducible level


274
00:12:15,560 --> 00:12:17,205
of error that we hope to get to.


275
00:12:17,205 --> 00:12:19,550
Let's say that when
you started a project,


276
00:12:19,550 --> 00:12:22,995
you'll see in the first
quarter or Q1 of some year,


277
00:12:22,995 --> 00:12:28,050
the system had 10
percent error rate.


278
00:12:28,050 --> 00:12:30,710
Over time, in
subsequent quarters,


279
00:12:30,710 --> 00:12:35,830
the error went down like,


280
00:12:35,830 --> 00:12:41,220
Q2, Q3, Q4 and so on.


281
00:12:41,220 --> 00:12:44,420
It turns out that it's not


282
00:12:44,420 --> 00:12:49,865
a terrible model to
estimate this curve.


283
00:12:49,865 --> 00:12:52,985
If you want to estimate how well


284
00:12:52,985 --> 00:12:56,734
the team could do in the future,


285
00:12:56,734 --> 00:12:58,680
one simple model I've used,


286
00:12:58,680 --> 00:13:00,980
is to estimate the rate of


287
00:13:00,980 --> 00:13:04,805
progress as for every
fixed period of time,


288
00:13:04,805 --> 00:13:09,375
say every quarter,
the team will reduce


289
00:13:09,375 --> 00:13:11,295
the error rate by


290
00:13:11,295 --> 00:13:16,340
some percentage relative to
human level performance.


291
00:13:16,340 --> 00:13:18,695
In this case, it looks like


292
00:13:18,695 --> 00:13:21,800
this gap between the current
level of performance and


293
00:13:21,800 --> 00:13:23,570
human level performance is


294
00:13:23,570 --> 00:13:27,170
shrinking by maybe 30
percent every quarter,


295
00:13:27,170 --> 00:13:29,900
which is why you get
this curve that is


296
00:13:29,900 --> 00:13:33,020
exponentially
decaying to what HRP.


297
00:13:33,020 --> 00:13:35,415
By estimating this
rate of progress,


298
00:13:35,415 --> 00:13:36,710
you may project into


299
00:13:36,710 --> 00:13:39,965
the future that hopefully
in future quarters,


300
00:13:39,965 --> 00:13:41,870
you continue to reduce


301
00:13:41,870 --> 00:13:44,505
the error by 30 percent
relative to HRP.


302
00:13:44,505 --> 00:13:46,280
This will give you a
sense of what might be


303
00:13:46,280 --> 00:13:48,350
reasonable for the future rate


304
00:13:48,350 --> 00:13:49,860
of progress on this project.


305
00:13:49,860 --> 00:13:54,740
This gives you a sense of
what may be feasible for


306
00:13:54,740 --> 00:13:58,190
an existing project for which
you already have this type


307
00:13:58,190 --> 00:14:02,210
of history and can try to
extrapolate into the future.


308
00:14:02,210 --> 00:14:04,370
In this video, you saw how


309
00:14:04,370 --> 00:14:06,755
to use human level performance,


310
00:14:06,755 --> 00:14:10,785
the question of whether you
have predictive features and


311
00:14:10,785 --> 00:14:12,620
the history of a project in


312
00:14:12,620 --> 00:14:15,675
order to assess
technical feasibility.


313
00:14:15,675 --> 00:14:18,485
Next, let's dive more deeply into


314
00:14:18,485 --> 00:14:21,350
assessing the value of a project.


315
00:14:21,350 --> 00:14:24,530
We'll do that in the next video.
1
00:00:00,000 --> 00:00:01,529
How do you estimate


2
00:00:01,529 --> 00:00:04,260
the value of Machine
Learning Project?


3
00:00:04,260 --> 00:00:07,050
This is sometimes not
easy to estimate,


4
00:00:07,050 --> 00:00:09,705
but let me share with you
a few best practices.


5
00:00:09,705 --> 00:00:11,880
Took speech recognition, let's


6
00:00:11,880 --> 00:00:14,040
say you're working on building


7
00:00:14,040 --> 00:00:16,410
a more accurate speech
recognition system


8
00:00:16,410 --> 00:00:17,760
for the purpose of


9
00:00:17,760 --> 00:00:19,830
voice search so that
people speak to


10
00:00:19,830 --> 00:00:22,655
the smartphone app
to do web searches.


11
00:00:22,655 --> 00:00:26,790
It turns out that
in most businesses


12
00:00:26,790 --> 00:00:28,620
there will be some metrics


13
00:00:28,620 --> 00:00:31,020
that machine learning engineers


14
00:00:31,020 --> 00:00:34,845
are used to optimizing
and some metrics that


15
00:00:34,845 --> 00:00:37,380
the owners of the product or


16
00:00:37,380 --> 00:00:40,685
the business will
want to maximize.


17
00:00:40,685 --> 00:00:42,810
There's often a gap between


18
00:00:42,810 --> 00:00:47,250
these two building machine
learning systems to objective


19
00:00:47,250 --> 00:00:50,160
that a learning
algorithm may optimize


20
00:00:50,160 --> 00:00:54,165
might be something right
where level accuracy.


21
00:00:54,165 --> 00:00:58,020
If a user says a certain
number of words,


22
00:00:58,020 --> 00:01:00,930
how many of the words
do we get right?


23
00:01:00,930 --> 00:01:02,970
Maybe the learning algorithm


24
00:01:02,970 --> 00:01:04,650
actually does great in the sense


25
00:01:04,650 --> 00:01:07,735
on log-likelihood or
some other criteria.


26
00:01:07,735 --> 00:01:10,425
But many machine learning
teams would be comfortable


27
00:01:10,425 --> 00:01:13,800
trying to get good
words-level accuracy.


28
00:01:13,800 --> 00:01:16,305
But when using this in
a business context,


29
00:01:16,305 --> 00:01:20,414
one other key metrics
query-level accuracy,


30
00:01:20,414 --> 00:01:22,500
which is how often do you get


31
00:01:22,500 --> 00:01:24,900
all the words in a query right.


32
00:01:24,900 --> 00:01:29,520
For some businesses word-level
accuracy is important,


33
00:01:29,520 --> 00:01:33,615
but query-level accuracy
may be even more important.


34
00:01:33,615 --> 00:01:37,755
We've now taken one step
away from the objective


35
00:01:37,755 --> 00:01:39,450
that the learning algorithm


36
00:01:39,450 --> 00:01:42,675
is almost directly optimizing.


37
00:01:42,675 --> 00:01:45,405
Even after you get
the query right,


38
00:01:45,405 --> 00:01:48,510
which is important for
the user experience,


39
00:01:48,510 --> 00:01:50,670
what users care even more


40
00:01:50,670 --> 00:01:53,580
about is the search
result quality.


41
00:01:53,580 --> 00:01:56,490
The reason the business may want


42
00:01:56,490 --> 00:01:59,280
to ensure search result quality


43
00:01:59,280 --> 00:02:01,200
is that this gives users


44
00:02:01,200 --> 00:02:05,130
a better experience and just
increases user engagement,


45
00:02:05,130 --> 00:02:07,740
so they come back to the
search engine more often.


46
00:02:07,740 --> 00:02:11,640
This is important, but
this is just one step


47
00:02:11,640 --> 00:02:15,960
towards actually driving the
revenue of the business.


48
00:02:15,960 --> 00:02:18,120
One gap I've often seen


49
00:02:18,120 --> 00:02:20,730
between machine learning
teams and business teams


50
00:02:20,730 --> 00:02:25,890
is the entry team will
usually want to work on this,


51
00:02:25,890 --> 00:02:34,150
whereas the business leader
may once promises on that.


52
00:02:34,250 --> 00:02:37,990
In order for a project
to move forward,


53
00:02:37,990 --> 00:02:40,720
I usually try to have
the technical and


54
00:02:40,720 --> 00:02:42,850
the business teams try to agree


55
00:02:42,850 --> 00:02:45,280
on metrics that both
are comfortable with.


56
00:02:45,280 --> 00:02:47,740
This often takes a little bit of


57
00:02:47,740 --> 00:02:51,370
compromise where the machine
learning team might stretch


58
00:02:51,370 --> 00:02:53,800
a little bit further
to the right and


59
00:02:53,800 --> 00:02:55,360
the business teams stretch


60
00:02:55,360 --> 00:02:57,625
a little bit further to the left.


61
00:02:57,625 --> 00:02:59,560
The further we go to the right,


62
00:02:59,560 --> 00:03:01,000
the harder it is for


63
00:03:01,000 --> 00:03:04,560
a machine learning team to
really give a guarantee.


64
00:03:04,560 --> 00:03:08,650
I wish more problems
could be solved by


65
00:03:08,650 --> 00:03:13,615
gradient descent or by
optimizing tested accuracy.


66
00:03:13,615 --> 00:03:16,360
But that's just not to
say of the world today,


67
00:03:16,360 --> 00:03:19,780
a lot of practical
problems require


68
00:03:19,780 --> 00:03:21,310
we do something more than


69
00:03:21,310 --> 00:03:23,780
just optimizing tested accuracy.


70
00:03:23,780 --> 00:03:26,350
Having the technical team and


71
00:03:26,350 --> 00:03:28,540
the business teams both step


72
00:03:28,540 --> 00:03:31,060
a little bit outside their
comfort zone is often


73
00:03:31,060 --> 00:03:34,030
important for reaching
compromise to pick


74
00:03:34,030 --> 00:03:38,020
some set of metrics that the
technical team feels like


75
00:03:38,020 --> 00:03:39,550
could stretch a bit to deliver


76
00:03:39,550 --> 00:03:42,310
on and that the business
team can stretch


77
00:03:42,310 --> 00:03:44,445
a bit to feel comfortable will


78
00:03:44,445 --> 00:03:46,945
create sufficient value
for the business.


79
00:03:46,945 --> 00:03:50,455
One other practice
I've found useful


80
00:03:50,455 --> 00:03:54,730
is that if you can do
even very rough back of


81
00:03:54,730 --> 00:03:57,670
the envelope calculations to


82
00:03:57,670 --> 00:03:59,470
relate what level of


83
00:03:59,470 --> 00:04:02,810
accuracy to some of the
metrics on the right.


84
00:04:02,810 --> 00:04:06,320
If word accuracy
improves by one percent,


85
00:04:06,320 --> 00:04:08,260
if you have any rough guess


86
00:04:08,260 --> 00:04:11,424
for will that improve
query level accuracy,


87
00:04:11,424 --> 00:04:16,010
maybe by 0.7 percent
or 0.8 percent,


88
00:04:16,010 --> 00:04:18,600
and how much will that improve


89
00:04:18,600 --> 00:04:20,920
search result quality and


90
00:04:20,920 --> 00:04:23,570
user engagement
and maybe revenue,


91
00:04:23,570 --> 00:04:25,270
if able to come up with


92
00:04:25,270 --> 00:04:28,990
even a very crude back of
the envelope calculation.


93
00:04:28,990 --> 00:04:32,440
Sometimes these are also
called Fermi estimates.


94
00:04:32,440 --> 00:04:37,410
You can read about
this on Wikipedia.


95
00:04:37,410 --> 00:04:40,850
That can also be a
way to help bridge


96
00:04:40,850 --> 00:04:42,110
the machine learning and gene


97
00:04:42,110 --> 00:04:44,375
metrics and business metrics.


98
00:04:44,375 --> 00:04:47,660
Now it goes without
saying that as


99
00:04:47,660 --> 00:04:51,200
part of estimating the
value of a project,


100
00:04:51,200 --> 00:04:53,715
I would encourage you to give


101
00:04:53,715 --> 00:04:57,290
thought to any ethical
considerations as well,


102
00:04:57,290 --> 00:04:59,570
such as is this project


103
00:04:59,570 --> 00:05:02,360
creating net positive
societal value?


104
00:05:02,360 --> 00:05:05,590
If not, I hope you won't do it.


105
00:05:05,590 --> 00:05:08,120
Or is this project
reasonably fair and


106
00:05:08,120 --> 00:05:10,520
free from bias and have any


107
00:05:10,520 --> 00:05:12,980
ethical or values-based concerns


108
00:05:12,980 --> 00:05:15,680
been openly aired and debated?


109
00:05:15,680 --> 00:05:17,810
I find it issues of


110
00:05:17,810 --> 00:05:20,840
values and ethics is
very domain-dependent,


111
00:05:20,840 --> 00:05:24,110
is very different in
making loans versus


112
00:05:24,110 --> 00:05:29,335
health care versus
recommending products online.


113
00:05:29,335 --> 00:05:31,670
Encourage you to look up


114
00:05:31,670 --> 00:05:34,580
any ethical frameworks that have


115
00:05:34,580 --> 00:05:38,090
been developed for your
industry and your application.


116
00:05:38,090 --> 00:05:40,310
If you have any concerns,


117
00:05:40,310 --> 00:05:42,720
raise it for debate
within your team.


118
00:05:42,720 --> 00:05:44,270
Ultimately, if you don't


119
00:05:44,270 --> 00:05:46,100
think the project
you're working on


120
00:05:46,100 --> 00:05:47,840
will help other people or


121
00:05:47,840 --> 00:05:50,025
will help humanity move forward,


122
00:05:50,025 --> 00:05:53,270
I hope you'll keep
searching for other,


123
00:05:53,270 --> 00:05:55,825
more meaningful
projects to jump into.


124
00:05:55,825 --> 00:05:59,660
In my work, I have faced
difficult choices where


125
00:05:59,660 --> 00:06:01,520
I really wasn't sure if


126
00:06:01,520 --> 00:06:04,245
a particular project was
something I should work on.


127
00:06:04,245 --> 00:06:06,850
Because I really didn't
know if it would make


128
00:06:06,850 --> 00:06:09,190
people net-net better off,


129
00:06:09,190 --> 00:06:10,720
and I found that having


130
00:06:10,720 --> 00:06:14,740
a team debate and
discuss it openly often


131
00:06:14,740 --> 00:06:17,455
helps us get to better answer


132
00:06:17,455 --> 00:06:20,580
and feel more comfortable with
whatever decision we make.


133
00:06:20,580 --> 00:06:22,750
I have called
multiple projects on


134
00:06:22,750 --> 00:06:25,420
ethical considerations
when I felt


135
00:06:25,420 --> 00:06:28,540
the project was
economically sound,


136
00:06:28,540 --> 00:06:30,250
but I didn't think it
would help people,


137
00:06:30,250 --> 00:06:31,540
and I just told the team,


138
00:06:31,540 --> 00:06:33,235
I don't want to do it
and I won't do it.


139
00:06:33,235 --> 00:06:35,080
I hope this gives
you a framework for


140
00:06:35,080 --> 00:06:38,650
evaluating the
value of a project.


141
00:06:38,650 --> 00:06:41,335
On the ethics and value piece,


142
00:06:41,335 --> 00:06:44,080
I think all of us collectively
should only work on


143
00:06:44,080 --> 00:06:47,020
projects that create net
positive social value,


144
00:06:47,020 --> 00:06:50,005
that help other people that
move humanity forward.


145
00:06:50,005 --> 00:06:53,045
I personally cool projects
just on that basis.


146
00:06:53,045 --> 00:06:55,630
There are multiple
projects that I


147
00:06:55,630 --> 00:06:58,810
felt the economic value
was completely sound.


148
00:06:58,810 --> 00:07:00,850
The economic case
is completely fine,


149
00:07:00,850 --> 00:07:02,695
but I look for the
price and I said,


150
00:07:02,695 --> 00:07:05,955
I don't think this actually
helps people and have


151
00:07:05,955 --> 00:07:09,475
cool projects just on that
basis and told my teams,


152
00:07:09,475 --> 00:07:10,870
let's find something else to work


153
00:07:10,870 --> 00:07:12,430
on because I'm not
going to do that.


154
00:07:12,430 --> 00:07:16,810
I hope that you also be able
to focus your efforts just


155
00:07:16,810 --> 00:07:18,880
on projects that help people


156
00:07:18,880 --> 00:07:22,250
and that help move
humanity forward.
1
00:00:00,000 --> 00:00:02,155
Say you've identified
the problem,


2
00:00:02,155 --> 00:00:03,810
found a worthy solution,


3
00:00:03,810 --> 00:00:06,120
finish diligence for
technical feasibility


4
00:00:06,120 --> 00:00:09,030
and value and you think
this is worth doing.


5
00:00:09,030 --> 00:00:11,010
Let's take a look at
the last steps of


6
00:00:11,010 --> 00:00:14,625
the scoping process of
milestones and resourcing.


7
00:00:14,625 --> 00:00:17,760
Determining milestones
and resourcing involves


8
00:00:17,760 --> 00:00:20,940
writing out the key
specifications for your project.


9
00:00:20,940 --> 00:00:23,520
This will include
machine learning metrics


10
00:00:23,520 --> 00:00:26,175
such as accuracy or
precision-recall.


11
00:00:26,175 --> 00:00:28,410
For some applications,
this may also


12
00:00:28,410 --> 00:00:31,680
include fairness
types of metrics.


13
00:00:31,680 --> 00:00:33,930
The specification will
often also include


14
00:00:33,930 --> 00:00:35,940
software metrics regarding


15
00:00:35,940 --> 00:00:38,790
the software system
such as latency,


16
00:00:38,790 --> 00:00:40,470
throughput, queries per second,


17
00:00:40,470 --> 00:00:41,610
and so on given


18
00:00:41,610 --> 00:00:46,200
the computational resources
available, and if possible,


19
00:00:46,200 --> 00:00:49,140
you might also write
out estimates of


20
00:00:49,140 --> 00:00:50,880
the business metrics you hope to


21
00:00:50,880 --> 00:00:52,980
move for the project
you're scoping,


22
00:00:52,980 --> 00:00:55,275
such as how much
incremental revenue


23
00:00:55,275 --> 00:00:57,180
if you have a way
of estimating that?


24
00:00:57,180 --> 00:00:58,680
In addition, writing out


25
00:00:58,680 --> 00:01:01,140
the resources needed,
how much data?


26
00:01:01,140 --> 00:01:03,450
From which teams? Personnel, any


27
00:01:03,450 --> 00:01:05,280
help you need from
cross-functional teams.


28
00:01:05,280 --> 00:01:07,560
What software integrations, data


29
00:01:07,560 --> 00:01:09,030
leaving support or other support


30
00:01:09,030 --> 00:01:10,815
you need from other teams.


31
00:01:10,815 --> 00:01:14,790
Finally, the timeline on


32
00:01:14,790 --> 00:01:16,090
which you hope to achieve


33
00:01:16,090 --> 00:01:18,220
certain milestones
or deliverables.


34
00:01:18,220 --> 00:01:19,690
Now, if you're looking at


35
00:01:19,690 --> 00:01:20,950
a machine learning project


36
00:01:20,950 --> 00:01:22,180
and you find that you're having


37
00:01:22,180 --> 00:01:23,785
a very hard time


38
00:01:23,785 --> 00:01:27,190
writing on some of these
key specifications,


39
00:01:27,190 --> 00:01:31,300
then you might also
consider carrying out


40
00:01:31,300 --> 00:01:33,700
a bench-marking exercise
to compare it to


41
00:01:33,700 --> 00:01:35,860
other similar projects that


42
00:01:35,860 --> 00:01:37,690
others may have worked on before,


43
00:01:37,690 --> 00:01:41,470
or building a proof
of concept first in


44
00:01:41,470 --> 00:01:45,010
order to get a better sense
of what accuracy, precision,


45
00:01:45,010 --> 00:01:48,965
or latency throughput or other
metrics might be feasible,


46
00:01:48,965 --> 00:01:52,765
and only after you've done
that POC, proof of concept,


47
00:01:52,765 --> 00:01:55,390
to then use that
information to more


48
00:01:55,390 --> 00:01:58,570
confidently scope out
the milestones and


49
00:01:58,570 --> 00:02:03,160
resources needed for a
larger scale execution


50
00:02:03,160 --> 00:02:05,285
of the project you have in mind.


51
00:02:05,285 --> 00:02:07,870
That's it. Congratulations
on making it


52
00:02:07,870 --> 00:02:10,290
to the end of this
section on scoping.


53
00:02:10,290 --> 00:02:13,090
I hope that these
ideas will help you to


54
00:02:13,090 --> 00:02:17,240
pick valuable and meaningful
projects to work on.


55
00:02:17,240 --> 00:02:21,475
Thank you also for sticking
with me from deployment,


56
00:02:21,475 --> 00:02:23,245
through modeling,
through training,


57
00:02:23,245 --> 00:02:25,344
all the way back to scoping,


58
00:02:25,344 --> 00:02:26,770
and I hope this framework of


59
00:02:26,770 --> 00:02:29,500
the full cycle of machine
learning project will


60
00:02:29,500 --> 00:02:31,105
also be useful for


61
00:02:31,105 --> 00:02:34,760
all the projects I hope
you will build and deploy.